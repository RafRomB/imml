{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5452f6-5352-49cd-8875-54163830b614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96a4a6-2a62-49d4-ac73-4334322faec7",
   "metadata": {},
   "source": [
    "# Tutorial: Impute incomplete modality and feature-wise multi-modal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b68e8e-d8ed-4955-b50a-10373c31be7d",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783600a-fc9e-4580-b207-f6f477db4d2d",
   "metadata": {},
   "source": [
    "You’ll need the following libraries installed: matplotlib; seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e6818-747c-405c-a3ae-334c8f301698",
   "metadata": {},
   "source": [
    "## Step 1: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13e3c8-610a-4e67-9097-4a96741890b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.impute import MOFAImputer, jNMFImputer, get_observed_view_indicator\n",
    "from imvc.preprocessing import MultiViewTransformer, NormalizerNaN\n",
    "from imvc.ampute import Amputer\n",
    "from imvc.preprocessing import ConcatenateViews\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30da50-f199-452c-8ee8-f2913a8fd522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tueplots import axes, bundles\n",
    "plt.rcParams.update(**bundles.icml2022(), **axes.lines())\n",
    "for key in [\"axes.labelsize\", \"axes.titlesize\", \"font.size\", \"legend.fontsize\", \"xtick.labelsize\", \"ytick.labelsize\"]:\n",
    "    if key == \"legend.fontsize\":\n",
    "        plt.rcParams[key] += 3\n",
    "    else:\n",
    "        plt.rcParams[key] += 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778f559-3bd7-4b53-ab19-e0170d35f7b6",
   "metadata": {},
   "source": [
    "## Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c7859-2fea-45d1-8f8d-99eacb3c8c5e",
   "metadata": {},
   "source": [
    "We'll use the sensIT300 dataset, a multi-view dataset with 300 samples and 2 modalities, to demonstrate how to handle multi-view data. Each view represents a distinct set of features for the same set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987a06d-abd7-4a59-9e37-03fbba284c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs, y = LoadDataset.load_dataset(dataset_name=\"sensIT300\", return_y= True)\n",
    "print(\"Samples:\", len(Xs[0]), \"\\t\", \"Modalities:\", len(Xs), \"\\t\", \"Features:\", [X.shape[1] for X in Xs])\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6aa40-35fb-4d1b-9094-a841ac764f6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 3: Apply missing data mechanism (Amputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa696db3-7302-41f4-b24d-4fcabd30d89c",
   "metadata": {},
   "source": [
    "Using Amputer, we randomly introduce missing data to simulate a scenario where some modalities are missing. Here, 30% of the samples will be incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ecad2-674d-4841-a9d0-59f7bc109d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amputed_Xs = Amputer(p= 0.3, mechanism=\"mcar\", random_state=42).fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab778da-3a34-4f6f-954c-f59f3fbe2ed7",
   "metadata": {},
   "source": [
    "You can visualize which modalities are missing using a binary color map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd67c74-b440-46ae-806f-901ec085bd21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xlabel,ylabel = \"Modality\", \"Samples\"\n",
    "observed_view_indicator = get_observed_view_indicator(amputed_Xs).sort_values(list(range(len(amputed_Xs))))\n",
    "plt.pcolor(observed_view_indicator, cmap=\"binary\")\n",
    "plt.xticks(np.arange(0.5, len(observed_view_indicator.columns), 1), observed_view_indicator.columns)\n",
    "_ = plt.xlabel(xlabel), plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58ff28-0544-4719-ad56-ec67aba07650",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 4: Impute missing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb1979-98c1-4c87-9fb3-c25f486d67b8",
   "metadata": {},
   "source": [
    "We are going to apply a pipeline that consists of the following steps: Standardization of the features; Imputation of missing modalities using MOFAImputer, a method designed for multi-view data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a45c7-2a58-4e80-a309-738382b99bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")),\n",
    "                         MOFAImputer(n_components = 8, random_state=42).set_output(transform=\"pandas\"))\n",
    "imputed_Xs = pipeline.fit_transform(amputed_Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8713bb-39db-4e88-a357-48c4bda8a061",
   "metadata": {},
   "source": [
    "You can again visualize the dataset after imputation to observe the filled modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5a3ed-a4b1-4c1a-bef9-d20c6b192d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observed_view_indicator = get_observed_view_indicator(imputed_Xs).sort_values(list(range(len(imputed_Xs))))\n",
    "plt.pcolor(observed_view_indicator, cmap=\"binary_r\")\n",
    "plt.xticks(np.arange(0.5, len(observed_view_indicator.columns), 1), observed_view_indicator.columns)\n",
    "_ = plt.xlabel(xlabel), plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9344e5-792e-4ff0-963d-960be5338def",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the imputation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82624e4-8003-419f-ab65-ebf8dc9c4ea1",
   "metadata": {},
   "source": [
    "We will calculate the Mean Squared Error (MSE) between the true values (before amputation) and the imputed values, restricted to the places where data were missing. The MSE helps quantify how well the imputation was performed. In addition to the MOFA-based pipeline, we introduce a baseline imputation method that uses SimpleImputer to fill in missing values with the average.\n",
    "\n",
    "Define a range of missingness proportions (ps), and vary the number of components for MOFAImputer (n_components_list). We will generate both block- and feature-wise missing data, and perform multiple runs for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a93718-a849-49e1-a82b-df22feae1ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ps = np.arange(0.1, 1, 0.2)\n",
    "n_components_list = [1, 2, 4, 8, 16, 32]\n",
    "mechanisms = [\"um\", \"pm\", \"mcar\", \"mnar\"]\n",
    "n_times = 25\n",
    "algorithms = [\"MOFAImputer\", \"jNMFImputer\", \"MeanImputer\"]\n",
    "all_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01481b3b-ffa8-42eb-8b59-0b3c64f9bc7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for algorithm in tqdm(algorithms):\n",
    "#     all_metrics[algorithm] = {}\n",
    "#     for method in tqdm(methods):\n",
    "#         all_metrics[algorithm][method] = {}\n",
    "#         for mechanism in tqdm(mechanisms):\n",
    "#             all_metrics[algorithm][method][mechanism] = {}\n",
    "#             for p in ps:\n",
    "#                 missing_percentage = int(p*100)\n",
    "#                 all_metrics[algorithm][method][mechanism][missing_percentage] = {}\n",
    "#                 for n_components in n_components_list:\n",
    "#                     all_metrics[algorithm][method][mechanism][missing_percentage][n_components] = {}\n",
    "#                     for i in range(n_times):\n",
    "#                         all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i] = {}\n",
    "#                         alg = eval(algorithm)\n",
    "#                         if algorithm == \"MOFAImputer\":\n",
    "#                             normalizer = StandardScaler()\n",
    "#                         elif algorithm == \"jNMFImputer\":\n",
    "#                             normalizer = MinMaxScaler()\n",
    "#                         pipeline = make_pipeline(\n",
    "#                             MultiViewTransformer(normalizer.set_output(transform=\"pandas\")),\n",
    "#                             alg(n_components = n_components, random_state=i))\n",
    "#                         if method == \"Baseline\":\n",
    "#                             pipeline = make_pipeline(\n",
    "#                                 MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")),\n",
    "#                                 *pipeline)\n",
    "#                         elif method == \"Filling\":\n",
    "#                             pipeline = make_pipeline(\n",
    "#                                 MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")),\n",
    "#                                 pipeline[0])\n",
    "#                         amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "#                         for X in amputed_Xs:\n",
    "#                             X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "#                         masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "#                         try:\n",
    "#                             imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "#                             transformed_Xs = pipeline[:-1].transform(Xs)\n",
    "#                             metric = np.mean([mean_squared_error(transformed_X.values[mask], imputed_X.values[mask])\n",
    "#                                               for transformed_X,imputed_X,mask in zip(transformed_Xs, imputed_Xs, masks)])\n",
    "#                             all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i][\"Mean Squared Error\"] = metric\n",
    "#                             all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i][\"Comments\"] = \"\"\n",
    "#                         except Exception as ex:\n",
    "#                             all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i][\"Comments\"] = ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc5765-9780-4ee1-a5ef-57f50af16873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for algorithm in tqdm(algorithms):\n",
    "#     all_metrics[algorithm] = {}\n",
    "#     for method in tqdm(methods):\n",
    "#         all_metrics[algorithm][method] = {}\n",
    "#         for mechanism in tqdm(mechanisms):\n",
    "#             all_metrics[algorithm][method][mechanism] = {}\n",
    "#             for p in ps:\n",
    "#                 missing_percentage = int(p*100)\n",
    "#                 all_metrics[algorithm][method][mechanism][missing_percentage] = {}\n",
    "#                 for n_components in n_components_list:\n",
    "#                     all_metrics[algorithm][method][mechanism][missing_percentage][n_components] = {}\n",
    "#                     for i in range(n_times):\n",
    "#                         all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i] = {}\n",
    "#                         alg = eval(algorithm)\n",
    "#                         if algorithm == \"MOFAImputer\":\n",
    "#                             normalizer = StandardScaler()\n",
    "#                         elif algorithm == \"jNMFImputer\":\n",
    "#                             normalizer = MinMaxScaler()\n",
    "#                         pipeline = make_pipeline(\n",
    "#                             MultiViewTransformer(normalizer.set_output(transform=\"pandas\")),\n",
    "#                             alg(n_components = n_components, random_state=i))\n",
    "#                         if method == \"Baseline\":\n",
    "#                             pipeline = make_pipeline(\n",
    "#                                 MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")),\n",
    "#                                 *pipeline)\n",
    "#                         elif method == \"Filling\":\n",
    "#                             pipeline = make_pipeline(\n",
    "#                                 MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")),\n",
    "#                                 pipeline[0])\n",
    "#                         amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "#                         for X in amputed_Xs:\n",
    "#                             X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "#                         masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "#                         try:\n",
    "#                             imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "#                             if method == \"Filling\":\n",
    "#                                 transformer_list = pipeline[-1].transformer_list_\n",
    "#                             else:\n",
    "#                                 transformer_list = pipeline[-2].transformer_list_\n",
    "#                             imputed_Xs = [pd.DataFrame(transformer.inverse_transform(X), index=X.index, columns=X.columns)\n",
    "#                                           for X, transformer in zip(imputed_Xs, transformer_list)]\n",
    "#                             metric = np.mean([mean_absolute_error(transformed_X.values[mask], imputed_X.values[mask])\n",
    "#                                               for transformed_X,imputed_X,mask in zip(Xs, imputed_Xs, masks)])\n",
    "#                             all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i][\"Mean Squared Error\"] = metric\n",
    "#                             all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i][\"Comments\"] = \"\"\n",
    "#                         except Exception as ex:\n",
    "#                             all_metrics[algorithm][method][mechanism][missing_percentage][n_components][i][\"Comments\"] = ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcbf81-bf56-4bc0-a227-6f91ba65639e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for algorithm in tqdm(algorithms):\n",
    "    all_metrics[algorithm] = {}\n",
    "    for mechanism in tqdm(mechanisms):\n",
    "        all_metrics[algorithm][mechanism] = {}\n",
    "        for p in ps:\n",
    "            missing_percentage = int(p*100)\n",
    "            all_metrics[algorithm][mechanism][missing_percentage] = {}\n",
    "            for n_components in n_components_list:\n",
    "                if (algorithm == \"MeanImputer\") and (n_components != n_components_list[0]):\n",
    "                    all_metrics[algorithm][mechanism][missing_percentage][n_components] = all_metrics[algorithm][mechanism][missing_percentage][n_components_list[0]]\n",
    "                    continue\n",
    "                all_metrics[algorithm][mechanism][missing_percentage][n_components] = {}\n",
    "                for i in range(n_times):\n",
    "                    all_metrics[algorithm][mechanism][missing_percentage][n_components][i] = {}\n",
    "                    if algorithm == \"MeanImputer\":\n",
    "                        pipeline = make_pipeline(\n",
    "                            MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")))\n",
    "                    else:\n",
    "                        if algorithm == \"MOFAImputer\":\n",
    "                            normalizer = StandardScaler()\n",
    "                        elif algorithm == \"jNMFImputer\":\n",
    "                            normalizer = MinMaxScaler()\n",
    "                        alg = eval(algorithm)\n",
    "                        pipeline = make_pipeline(\n",
    "                            MultiViewTransformer(normalizer.set_output(transform=\"pandas\")),\n",
    "                            alg(n_components = n_components, random_state=i))\n",
    "                    amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "                    for X in amputed_Xs:\n",
    "                        X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "                    masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "                    try:\n",
    "                        imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "                        if algorithm == \"MeanImputer\":\n",
    "                            transformer_list = pipeline[-1].transformer_list_\n",
    "\n",
    "                        else:\n",
    "                            transformer_list = pipeline[-2].transformer_list_\n",
    "                            imputed_Xs = [pd.DataFrame(transformer.inverse_transform(X), index=X.index, columns=X.columns)\n",
    "                                          for X, transformer in zip(imputed_Xs, transformer_list)]\n",
    "                        metric = np.mean([mean_absolute_error(transformed_X.values[mask], imputed_X.values[mask])\n",
    "                                          for transformed_X,imputed_X,mask in zip(Xs, imputed_Xs, masks)])\n",
    "                        all_metrics[algorithm][mechanism][missing_percentage][n_components][i][\"Mean Squared Error\"] = metric\n",
    "                        pipeline = make_pipeline(ConcatenateViews(), StandardScaler(), LogisticRegression(random_state=i))\n",
    "                        pipeline.fit(imputed_Xs, y)\n",
    "                        pred = pipeline.predict(imputed_Xs)\n",
    "                        metric = accuracy_score(y_true=y, y_pred=pred)\n",
    "                        all_metrics[algorithm][mechanism][missing_percentage][n_components][i][\"Accuracy\"] = metric\n",
    "                        pipeline = make_pipeline(ConcatenateViews(), StandardScaler(), SVC(random_state=i))\n",
    "                        pipeline.fit(imputed_Xs, y)\n",
    "                        pred = pipeline.predict(imputed_Xs)\n",
    "                        metric = accuracy_score(y_true=y, y_pred=pred)\n",
    "                        all_metrics[algorithm][mechanism][missing_percentage][n_components][i][\"Accuracy_SVM\"] = metric\n",
    "                        all_metrics[algorithm][mechanism][missing_percentage][n_components][i][\"Comments\"] = \"\"\n",
    "                    except Exception as ex:\n",
    "                        all_metrics[algorithm][mechanism][missing_percentage][n_components][i][\"Comments\"] = ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f878616-abdc-4c83-af7a-2802baa912fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_data = [\n",
    "    {\n",
    "        'Algorithm': algorithm,\n",
    "        'Mechanism': mechanism,\n",
    "        'Missing rate (\\%)': p,\n",
    "        'Components': n_components,\n",
    "        'Iteration': i,\n",
    "        **iter_dict\n",
    "    }\n",
    "    for algorithm, algorithm_dict in all_metrics.items()\n",
    "    for mechanism, mechanism_dict in algorithm_dict.items()\n",
    "    for p, p_dict in mechanism_dict.items()\n",
    "    for n_components, n_components_dict in p_dict.items()\n",
    "    for i, iter_dict in n_components_dict.items()\n",
    "]\n",
    "df = pd.DataFrame(flattened_data)\n",
    "df = df.sort_values([\"Algorithm\", \"Mechanism\", \"Missing rate (\\%)\", \"Components\", \"Iteration\"], ascending=[True, False, True, True, True])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd258e-410c-4d34-9ab7-ef0de1fe2281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby([\"Algorithm\", \"Missing rate (\\%)\"])[\"Accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e9949-eb11-457f-a4a9-5b795849f6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = df[df[\"Comments\"] != \"\"]\n",
    "print(\"errors\", errors.shape)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eae84b-1934-4af3-874e-376542890128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mechanism_names = {\"um\": \"Unpaired missing\",\n",
    "             \"pm\": \"Partial missing\",\n",
    "             \"mcar\": \"Missing completely at random\",\n",
    "             \"mnar\": \"Missing not at random\",\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1622d20-e1d9-4b32-9da7-792332bd14cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df, col=\"Mechanism\", legend_out=True, sharey=False,\n",
    "                  despine= False).map_dataframe(sns.pointplot, x=\"Missing rate (\\%)\",\n",
    "                                                y=\"Mean Squared Error\", hue=\"Algorithm\",\n",
    "                                                capsize= 0.05, seed= 42,\n",
    "                                                palette= sns.color_palette(\"colorblind\"),\n",
    "                                                linestyles= [\"-\", \"--\", \":\"],\n",
    "                                                dodge= True)\n",
    "g.add_legend(loc=\"upper left\")\n",
    "\n",
    "for i, mechanism in enumerate(df[\"Mechanism\"].unique()):\n",
    "    g.axes[0][i].set_title(mechanism_names[mechanism])\n",
    "    # g.axes.flatten()[i].set_xlim((-0.2, 4.2))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3a936-cc06-4da4-a130-8cbcc5bb53f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df, col=\"Mechanism\", row=\"Components\", legend_out=False, sharey=False,\n",
    "                  despine= False).map_dataframe(sns.pointplot, x=\"Missing rate (\\%)\",\n",
    "                                                y=\"Mean Squared Error\", hue=\"Algorithm\",\n",
    "                                                capsize= 0.05, seed= 42,\n",
    "                                                palette= sns.color_palette(\"colorblind\"),\n",
    "                                                linestyles= [\"-\", \"--\", \":\"],\n",
    "                                                dodge= True)\n",
    "\n",
    "g.add_legend(loc=\"upper left\")\n",
    "for i, mechanism in enumerate(df[\"Mechanism\"].unique()):\n",
    "    g.axes[0][i].set_title(mechanism_names[mechanism])\n",
    "    # g.axes.flatten()[i].set_xlim((-0.2, 4.2))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cce06a-cabd-48c3-86be-89a81e4208e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df, col=\"Mechanism\", row=\"Components\", legend_out=False, sharey=False,\n",
    "                  despine= False, ylim = (0.1, 1.05)).map_dataframe(sns.pointplot, x=\"Missing rate (\\%)\",\n",
    "                                                                    y=\"Accuracy\", hue=\"Algorithm\", \n",
    "                                                                    capsize= 0.05, seed= 42,\n",
    "                                                palette= sns.color_palette(\"colorblind\"),\n",
    "                                                linestyles= [\"-\", \"--\", \":\"],\n",
    "                                                                    dodge=True)\n",
    "\n",
    "g.add_legend(loc=\"upper left\")\n",
    "for i, mechanism in enumerate(df[\"Mechanism\"].unique()):\n",
    "    g.axes[0][i].set_title(mechanism_names[mechanism])\n",
    "    # g.axes.flatten()[i].set_xlim((-0.2, 4.2))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3c477-46ed-4ed7-8ddb-380784fea5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df, col=\"Mechanism\", row=\"Components\", legend_out=False, sharey=False,\n",
    "                  despine= False, ylim = (0.1, 1.05)).map_dataframe(sns.pointplot, x=\"Missing rate (\\%)\",\n",
    "                                                                    y=\"Accuracy_SVM\", hue=\"Algorithm\", \n",
    "                                                                    capsize= 0.05, seed= 42,\n",
    "                                                palette= sns.color_palette(\"colorblind\"),\n",
    "                                                linestyles= [\"-\", \"--\", \":\"],\n",
    "                                                                    dodge=True)\n",
    "\n",
    "g.add_legend(loc=\"upper left\")\n",
    "for i, mechanism in enumerate(df[\"Mechanism\"].unique()):\n",
    "    g.axes[0][i].set_title(mechanism_names[mechanism])\n",
    "    # g.axes.flatten()[i].set_xlim((-0.2, 4.2))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d0cd8-6edf-4e4a-ba13-944e7b14df26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imvc.impute import MOFAImputer, get_observed_view_indicator, jNMFImputer\n",
    "from imvc.preprocessing import NormalizerNaN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "key = \"jNMFImputer\"\n",
    "all_metrics[key] = {}\n",
    "for p in tqdm(ps):\n",
    "    missing_percentage = int(p*100)\n",
    "    all_metrics[key][missing_percentage] = {}\n",
    "    for n_components in n_components_list:\n",
    "        all_metrics[key][missing_percentage][n_components] = {}\n",
    "        for i in range(n_times):\n",
    "            pipeline = make_pipeline(MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n",
    "                                     jNMFImputer(n_components = n_components, random_state=i))\n",
    "            amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "            for X in amputed_Xs:\n",
    "                X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "            masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "            imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "            transformed_Xs = pipeline[0].transform(Xs)\n",
    "            metric = np.mean([mean_absolute_error(transformed_X.values[mask], imputed_X[mask])\n",
    "                              for transformed_X,imputed_X,mask in zip(transformed_Xs, imputed_Xs, masks)])\n",
    "            all_metrics[key][missing_percentage][n_components][i] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251cac0-5af2-4c47-90a3-f23f285de311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = \"Baseline\"\n",
    "all_metrics[key] = {}\n",
    "for p in tqdm(ps):\n",
    "    missing_percentage = int(p*100)\n",
    "    all_metrics[key][missing_percentage] = {}\n",
    "    for n_components in n_components_list:\n",
    "        pipeline = make_pipeline(MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n",
    "                                 MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")))\n",
    "        all_metrics[key][missing_percentage][n_components] = {}\n",
    "        for i in range(n_times):\n",
    "            amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "            for X in amputed_Xs:\n",
    "                X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "            masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "            imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "            transformed_Xs = pipeline[0].transform(Xs)\n",
    "            metric = np.mean([mean_absolute_error(transformed_X.values[mask], imputed_X.values[mask])\n",
    "                              for transformed_X,imputed_X,mask in zip(transformed_Xs, imputed_Xs, masks)])\n",
    "            all_metrics[key][missing_percentage][n_components][i] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749e16b-81ee-4569-b0c1-ff65c0b1438c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = \"MOFA\"\n",
    "all_metrics[key] = {}\n",
    "for p in ps:\n",
    "    missing_percentage = int(p*100)\n",
    "    all_metrics[key][missing_percentage] = {}\n",
    "    for n_components in n_components_list:\n",
    "        all_metrics[key][missing_percentage][n_components] = {}\n",
    "        for i in range(n_times):\n",
    "            pipeline = make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")),\n",
    "                                     MOFAImputer(n_components = n_components, random_state=i).set_output(transform=\"pandas\"))\n",
    "            amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "            for X in amputed_Xs:\n",
    "                X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "            masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "            imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "            transformed_Xs = pipeline[0].transform(Xs)\n",
    "            metric = np.mean([mean_squared_error(transformed_X.values[mask], imputed_X.values[mask]) for transformed_X,imputed_X,mask in zip(transformed_Xs, imputed_Xs, masks)])\n",
    "            all_metrics[key][missing_percentage][n_components][i] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6ff41-6c22-4ff6-b31d-b1c4c04d126e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = \"Baseline\"\n",
    "all_metrics[key] = {}\n",
    "for p in ps:\n",
    "    missing_percentage = int(p*100)\n",
    "    all_metrics[key][missing_percentage] = {}\n",
    "    for n_components in n_components_list:\n",
    "        pipeline = make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")),\n",
    "                                 MultiViewTransformer(SimpleImputer().set_output(transform=\"pandas\")))\n",
    "        all_metrics[key][missing_percentage][n_components] = {}\n",
    "        for i in range(n_times):\n",
    "            amputed_Xs = Amputer(p= p, mechanism=mechanism, random_state=i).fit_transform(Xs)\n",
    "            for X in amputed_Xs:\n",
    "                X.iloc[np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)] = np.nan\n",
    "            masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n",
    "            imputed_Xs = pipeline.fit_transform(amputed_Xs)\n",
    "            transformed_Xs = pipeline[0].transform(Xs)\n",
    "            metric = np.mean([mean_squared_error(transformed_X.values[mask], imputed_X.values[mask]) for transformed_X,imputed_X,mask in zip(transformed_Xs, imputed_Xs, masks)])\n",
    "            all_metrics[key][missing_percentage][n_components][i] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b6440-75d3-419c-8aa0-79369f3bb7ef",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2ff64-f098-410f-a54e-eff319c32f58",
   "metadata": {},
   "source": [
    "After collecting the results from both the MOFA and Baseline imputation methods, we flatten the data into a structured format for easy comparison using visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8145c-8878-4e5e-8cf9-f39e800a3858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_data = [\n",
    "    {\n",
    "        'Method': method,\n",
    "        'Missing rate': p,\n",
    "        'Components': n_comp,\n",
    "        **n_dict\n",
    "    }\n",
    "    for method, method_dict in all_metrics.items()\n",
    "    for p, p_dict in method_dict.items()\n",
    "    for n_comp, n_dict in p_dict.items()\n",
    "]\n",
    "df = pd.DataFrame(flattened_data)\n",
    "df = df.melt(id_vars=['Method', 'Missing rate', \"Components\"], var_name='Iteration', value_name='Imputation error (MSE)')\n",
    "df = df.sort_values([\"Missing rate\", \"Method\", \"Components\", \"Iteration\"], ascending=[True, False, True, True])\n",
    "# df.to_csv(\"tutorials/impute_results.csv\", index= None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d6049-1aea-49c9-bec7-5db94e00b195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby([\"Method\", \"Missing rate\", \"Components\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d8ea2-52ae-4abe-90b5-2b9ced128ab0",
   "metadata": {},
   "source": [
    "We’ll use Seaborn to create point plots that show the imputation error for different levels of missing data and varying MOFA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694e6ed-86e4-42e3-bb8d-b0d8271a16d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tutorials/impute_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8fc9f-9901-4913-a3f9-795088075da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, len(n_components_list), figsize=(20, 3))\n",
    "fig.supylabel(\"Imputation error (MSE)\", fontsize=plt.rcParams[\"axes.labelsize\"])\n",
    "\n",
    "for idx, components in enumerate(n_components_list):\n",
    "    ax = axs[0, idx]\n",
    "    ax2 = axs[1, idx]\n",
    "    ax.set_title(f'Components = {components}', fontsize=plt.rcParams[\"axes.labelsize\"])\n",
    "    \n",
    "    sns.pointplot(data=df, x=\"Missing rate (\\%)\", y=\"Imputation error (MSE)\", hue=\"Method\", markers=[\"o\", \"X\"],\n",
    "                  linestyles=[\"-\", \"--\"], seed= 42, capsize= 0.05, palette= \"tab10\", ax=ax)\n",
    "    sns.pointplot(data=df, x=\"Missing rate (\\%)\", y=\"Imputation error (MSE)\", hue=\"Method\", markers=[\"o\", \"X\"],\n",
    "                  linestyles=[\"-\", \"--\"], seed= 42, capsize= 0.05, palette= \"tab10\", ax=ax2)\n",
    "\n",
    "    \n",
    "    ax.set_ylim(4, 6)\n",
    "    ax2.set_ylim(1, 1.8)\n",
    "\n",
    "    ax.spines.bottom.set_visible(False)\n",
    "    ax2.spines.top.set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    if components == min(n_components_list):\n",
    "        ax.get_legend().set_title(None)\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set(ylabel=None)\n",
    "        ax2.set(ylabel=None)\n",
    "    else:\n",
    "        ax.get_legend().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax2.get_yaxis().set_visible(False)\n",
    "    ax2.get_legend().set_visible(False)\n",
    "\n",
    "    \n",
    "    d = .02\n",
    "    kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax.plot((-d, +d), (-d, +d), **kwargs)\n",
    "    ax.plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "\n",
    "    kwargs.update(transform=ax2.transAxes)\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "plt.savefig(\"paper_figures/imputation_a.pdf\")\n",
    "plt.savefig(\"paper_figures/imputation_a.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12763ac8-942f-4b0d-bd4b-af2177b007bf",
   "metadata": {},
   "source": [
    "This plot shows the imputation error for both methods (MOFA and baseline) across different levels of missing data. The x-axis represents the percentage of missing rate (ranging from 10% to 90%), and the y-axis represents the imputation error (MSE). Each subplot corresponds to a different number of components, with values of 1, 2, 4, 8, 16, and 32 from left to right. The blue solid line represents the MOFA method, while the orange dashed line represents the baseline method. Error bars are present for both methods, indicating variability in performance (95% confidence interval).\n",
    "\n",
    "For both methods, the imputation error tends to increase as the missing rate increases. MOFA consistently outperforms the baseline method. However, with extreme values of % missing rate, MOFA is not able to achieve better values than the baseline, indicating a collapse due to the lack of information.\n",
    "\n",
    "Next, we focus on how the imputation error changes as we increase the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb995e1-f3d3-42a3-bc09-85ed1e9a0fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplot_mosaic('ABCDE;FGHIE', figsize=(20, 3))\n",
    "keys = list(axs.keys())\n",
    "fig.supylabel(\"Imputation error (MSE)\", fontsize=plt.rcParams[\"axes.labelsize\"])\n",
    "\n",
    "for idx, p in enumerate(ps):\n",
    "    missing_percentage = int(p*100)\n",
    "    if p == max(ps):\n",
    "        min_key = keys[idx]\n",
    "        max_key = keys[idx]\n",
    "    else:\n",
    "        min_key = keys[idx]\n",
    "        max_key = keys[idx + len(ps)]\n",
    "    ax = axs[min_key]\n",
    "    ax2 = axs[max_key]\n",
    "    ax.set_title(f'Missing rate = {missing_percentage}', fontsize=plt.rcParams[\"axes.titlesize\"])\n",
    "\n",
    "    sns.pointplot(data=df[df[\"Missing rate (\\%)\"] == missing_percentage], x=\"Components\", y=\"Imputation error (MSE)\", \n",
    "                  hue=\"Method\", markers=[\"o\", \"X\"], linestyles=[\"-\", \"--\"],\n",
    "                  seed= 42, capsize= 0.05, palette= \"tab10\", ax=ax, errorbar=None)\n",
    "    sns.pointplot(data=df[df[\"Missing rate (\\%)\"] == missing_percentage], x=\"Components\", y=\"Imputation error (MSE)\",\n",
    "                  hue=\"Method\", markers=[\"o\", \"X\"], linestyles=[\"-\", \"--\"],\n",
    "                  seed= 42, capsize= 0.05, palette= \"tab10\", ax=ax2, errorbar=None)\n",
    "    \n",
    "    if p == max(ps):\n",
    "        ax.get_legend().set_visible(False)\n",
    "        ax.set(ylabel=None)\n",
    "        ax2.set(ylabel=None)\n",
    "    else:\n",
    "        ylims = ax.lines[1].get_ydata()\n",
    "        average = np.mean(ylims)\n",
    "        ax.set_ylim(average - 0.018, average + 0.018)\n",
    "        ylims = ax.lines[0].get_ydata()\n",
    "        average = np.mean(ylims)\n",
    "        ax2.set_ylim(average - 0.018, average + 0.018)\n",
    "\n",
    "        ax.spines.bottom.set_visible(False)\n",
    "        ax2.spines.top.set_visible(False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "\n",
    "        d = .02\n",
    "        if p == min(ps):\n",
    "            ax.get_legend().set_title(None)\n",
    "            ax.legend(loc='best')\n",
    "            ax.set(ylabel=None)\n",
    "            ax2.set(ylabel=None)\n",
    "        else:\n",
    "            ax.get_legend().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            ax2.get_yaxis().set_visible(False)\n",
    "        ax2.get_legend().set_visible(False)\n",
    "\n",
    "\n",
    "        d = .02\n",
    "        kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "        ax.plot((-d, +d), (-d, +d), **kwargs)\n",
    "        ax.plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "\n",
    "        kwargs.update(transform=ax2.transAxes)\n",
    "        ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "        ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "plt.savefig(\"paper_figures/imputation_b.pdf\")\n",
    "plt.savefig(\"paper_figures/imputation_b.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b0a5d-e45b-4135-b1ca-5ad42393b910",
   "metadata": {},
   "source": [
    "As the percentage of missing rate increases, the imputation error decreases more dramatically with higher values of C, especially from C=8 to C=32. This suggests that using more components improves the quality of imputation, particularly when dealing with datasets that have a high proportion of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763eec6-cb0a-40a5-85b5-72024e34beb0",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e0629-d628-4147-b0ca-c04de3248348",
   "metadata": {},
   "source": [
    "MOFA is generally superior to the Baseline method in terms of imputation error. Increasing the number of components (C) significantly improves imputation accuracy, especially for highly incomplete datasets, making it a key factor in reducing error. While the Baseline method can narrow the gap with MOFA at higher values of C, MOFA still consistently yields better performance, making it the preferable choice for multi-view imputation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090898e2-e896-4265-aece-6341cd45f500",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32747e-c6a5-4cea-86df-127c37686034",
   "metadata": {},
   "source": [
    "This comparison highlights the strength of MOFAImputer in handling multi-view datasets. In summary, MOFA stands out as the more robust method, particularly for datasets with moderate to high proportions of missing data and when the number of components (C) is sufficiently large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
