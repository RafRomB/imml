{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64840a7392653ffb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils.getresult import GetResult\n",
    "\n",
    "\n",
    "folder_results = \"results\"\n",
    "folder_subresults = \"subresults\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_results, filelame)\n",
    "subresults_path = os.path.join(folder_results, folder_subresults)\n",
    "logs_file = os.path.join(folder_results, 'logs.txt')\n",
    "error_file = os.path.join(folder_results, 'errors.txt')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "args = lambda: None\n",
    "args.continue_benchmarking, args.n_jobs = True, 2\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-continue_benchmarking', default= False, action='store_true')\n",
    "# parser.add_argument('-n_jobs', default= 1, type= int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.n_jobs > 1:\n",
    "    pandarallel.initialize(nb_workers= args.n_jobs)\n",
    "\n",
    "datasets = [\n",
    "    \"simulated_gm\",\n",
    "    \"simulated_InterSIM\",\n",
    "    \"simulated_netMUG\",\n",
    "    \"nutrimouse_genotype\",\n",
    "    \"nutrimouse_diet\",\n",
    "    \"bbcsport\",\n",
    "    \"buaa\",\n",
    "    \"metabric\",\n",
    "    \"digits\",\n",
    "    \"bdgp\",\n",
    "    \"tcga\",\n",
    "    \"caltech101\",\n",
    "    \"nuswide\",\n",
    "]\n",
    "two_view_datasets = [\"simulated_gm\", \"nutrimouse_genotype\", \"nutrimouse_diet\", \"metabric\", \"bdgp\",\n",
    "                     \"buaa\", \"simulated_netMUG\"]\n",
    "amputation_mechanisms = [\"EDM\", 'MCAR', 'MAR', 'MNAR']\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(25)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(),\n",
    "                                   MultiViewTransformer(FunctionTransformer(pd.DataFrame)), ConcatenateViews(),\n",
    "                                   StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"IntNMF\": {\"alg\": MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"COCA\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()), \"missing_percentage\": probs,\n",
    "                   \"amputation_mechanism\": amputation_mechanisms, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "indexes_names = list(indexes_results.keys())\n",
    "results = GetResult.create_results_table(datasets=datasets, indexes_results=indexes_results,\n",
    "                                         indexes_names=indexes_names, amputation_mechanisms=amputation_mechanisms,\n",
    "                                         two_view_datasets=two_view_datasets)\n",
    "\n",
    "if not args.continue_benchmarking:\n",
    "    if not eval(input(\"Are you sure you want to start benchmarking and delete previous results? (True/False)\")):\n",
    "        raise Exception\n",
    "    results.to_csv(file_path)\n",
    "\n",
    "    shutil.rmtree(subresults_path, ignore_errors=True)\n",
    "    os.mkdir(subresults_path)\n",
    "\n",
    "    os.remove(logs_file) if os.path.exists(logs_file) else None\n",
    "    os.remove(error_file) if os.path.exists(error_file) else None\n",
    "    open(logs_file, 'w').close()\n",
    "    open(error_file, 'w').close()\n",
    "else:\n",
    "    finished_results = pd.read_csv(file_path, index_col= indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "    finished_results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                    indexes_names=indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "\n",
    "results = results.sort_index(level= \"missing_percentage\", sort_remaining= False)\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    names = dataset_name.split(\"_\")\n",
    "    if \"simulated\" in names:\n",
    "        names = [\"_\".join(names)]\n",
    "    x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "    y = y[y_name]\n",
    "    n_clusters = y.nunique()\n",
    "    unfinished_results_dataset = unfinished_results.loc[[dataset_name]]\n",
    "\n",
    "    if args.n_jobs == 1:\n",
    "        iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "        iterator.apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y, n_clusters=n_clusters,\n",
    "                                                         algorithms=algorithms, random_state=random_state,\n",
    "                                                         subresults_path=subresults_path, logs_file=logs_file,\n",
    "                                                         error_file=error_file), axis= 1)\n",
    "    else:\n",
    "        if 0 in unfinished_results_dataset.index.get_level_values(\"missing_percentage\"):\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.xs(0, level=\"missing_percentage\",\n",
    "                                                                           drop_level=False).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns= indexes_names)\n",
    "            iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                      n_clusters=n_clusters,\n",
    "                                                                      algorithms=algorithms,\n",
    "                                                                      random_state=random_state,\n",
    "                                                                      subresults_path=subresults_path,\n",
    "                                                                      logs_file=logs_file,\n",
    "                                                                      error_file=error_file), axis= 1)\n",
    "            results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                   indexes_names=indexes_names)\n",
    "            results.to_csv(file_path)\n",
    "\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.drop(unfinished_results_dataset_idx).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns=indexes_names)\n",
    "        else:\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "\n",
    "        iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                  n_clusters=n_clusters,\n",
    "                                                                  algorithms=algorithms,\n",
    "                                                                  random_state=random_state,\n",
    "                                                                  subresults_path=subresults_path,\n",
    "                                                                  logs_file=logs_file,\n",
    "                                                                  error_file=error_file), axis= 1)\n",
    "        results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                               indexes_names=indexes_names)\n",
    "        results.to_csv(file_path)\n",
    "        GetResult.remove_subresults(results=results, subresults_path=subresults_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c287a9c0-f82c-4b75-82c8-34e3e6c7eb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, Normalizer\n",
    "from sklearn import metrics\n",
    "from reval.utils import kuhn_munkres_algorithm\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from collections import defaultdict\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews, Amputer, FillIncompleteSamples\n",
    "from imvc.utils import DatasetUtils\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.algorithms import NMFC\n",
    "from imvc.cluster import NEMO, DAIMC\n",
    "\n",
    "from models import Model\n",
    "from utils import Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b607781f2af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils.getresult import GetResult\n",
    "\n",
    "\n",
    "folder_results = \"results\"\n",
    "folder_subresults = \"subresults\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_results, filelame)\n",
    "subresults_path = os.path.join(folder_results, folder_subresults)\n",
    "logs_file = os.path.join(folder_results, 'logs.txt')\n",
    "error_file = os.path.join(folder_results, 'errors.txt')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "args = lambda: None\n",
    "args.continue_benchmarking, args.n_jobs = True, 2\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-continue_benchmarking', default= False, action='store_true')\n",
    "# parser.add_argument('-n_jobs', default= 1, type= int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.n_jobs > 1:\n",
    "    pandarallel.initialize(nb_workers= args.n_jobs)\n",
    "\n",
    "datasets = [\n",
    "    \"simulated_gm\",\n",
    "    \"simulated_InterSIM\",\n",
    "    \"simulated_netMUG\",\n",
    "    \"nutrimouse_genotype\",\n",
    "    \"nutrimouse_diet\",\n",
    "    \"bbcsport\",\n",
    "    \"buaa\",\n",
    "    \"metabric\",\n",
    "    \"digits\",\n",
    "    \"bdgp\",\n",
    "    \"tcga\",\n",
    "    \"caltech101\",\n",
    "    \"nuswide\",\n",
    "]\n",
    "two_view_datasets = [\"simulated_gm\", \"nutrimouse_genotype\", \"nutrimouse_diet\", \"metabric\", \"bdgp\",\n",
    "                     \"buaa\", \"simulated_netMUG\"]\n",
    "amputation_mechanisms = [\"EDM\", 'MCAR', 'MAR', 'MNAR']\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(25)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(),\n",
    "                                   MultiViewTransformer(FunctionTransformer(pd.DataFrame)), ConcatenateViews(),\n",
    "                                   StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"IntNMF\": {\"alg\": MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"COCA\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()), \"missing_percentage\": probs,\n",
    "                   \"amputation_mechanism\": amputation_mechanisms, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "indexes_names = list(indexes_results.keys())\n",
    "results = GetResult.create_results_table(datasets=datasets, indexes_results=indexes_results,\n",
    "                                         indexes_names=indexes_names, amputation_mechanisms=amputation_mechanisms,\n",
    "                                         two_view_datasets=two_view_datasets)\n",
    "\n",
    "if not args.continue_benchmarking:\n",
    "    if not eval(input(\"Are you sure you want to start benchmarking and delete previous results? (True/False)\")):\n",
    "        raise Exception\n",
    "    results.to_csv(file_path)\n",
    "\n",
    "    shutil.rmtree(subresults_path, ignore_errors=True)\n",
    "    os.mkdir(subresults_path)\n",
    "\n",
    "    os.remove(logs_file) if os.path.exists(logs_file) else None\n",
    "    os.remove(error_file) if os.path.exists(error_file) else None\n",
    "    open(logs_file, 'w').close()\n",
    "    open(error_file, 'w').close()\n",
    "else:\n",
    "    finished_results = pd.read_csv(file_path, index_col= indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "    finished_results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                    indexes_names=indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "\n",
    "results = results.sort_index(level= \"missing_percentage\", sort_remaining= False)\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    names = dataset_name.split(\"_\")\n",
    "    if \"simulated\" in names:\n",
    "        names = [\"_\".join(names)]\n",
    "    x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "    y = y[y_name]\n",
    "    n_clusters = y.nunique()\n",
    "    unfinished_results_dataset = unfinished_results.loc[[dataset_name]]\n",
    "\n",
    "    if args.n_jobs == 1:\n",
    "        iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "        iterator.apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y, n_clusters=n_clusters,\n",
    "                                                         algorithms=algorithms, random_state=random_state,\n",
    "                                                         subresults_path=subresults_path, logs_file=logs_file,\n",
    "                                                         error_file=error_file), axis= 1)\n",
    "    else:\n",
    "        if 0 in unfinished_results_dataset.index.get_level_values(\"missing_percentage\"):\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.xs(0, level=\"missing_percentage\",\n",
    "                                                                           drop_level=False).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns= indexes_names)\n",
    "            iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                      n_clusters=n_clusters,\n",
    "                                                                      algorithms=algorithms,\n",
    "                                                                      random_state=random_state,\n",
    "                                                                      subresults_path=subresults_path,\n",
    "                                                                      logs_file=logs_file,\n",
    "                                                                      error_file=error_file), axis= 1)\n",
    "            results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                   indexes_names=indexes_names)\n",
    "            results.to_csv(file_path)\n",
    "\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.drop(unfinished_results_dataset_idx).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns=indexes_names)\n",
    "        else:\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "\n",
    "        iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                  n_clusters=n_clusters,\n",
    "                                                                  algorithms=algorithms,\n",
    "                                                                  random_state=random_state,\n",
    "                                                                  subresults_path=subresults_path,\n",
    "                                                                  logs_file=logs_file,\n",
    "                                                                  error_file=error_file), axis= 1)\n",
    "        results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                               indexes_names=indexes_names)\n",
    "        results.to_csv(file_path)\n",
    "        GetResult.remove_subresults(results=results, subresults_path=subresults_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f243d7cad0925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils.getresult import GetResult\n",
    "\n",
    "\n",
    "folder_results = \"results\"\n",
    "folder_subresults = \"subresults\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_results, filelame)\n",
    "subresults_path = os.path.join(folder_results, folder_subresults)\n",
    "logs_file = os.path.join(folder_results, 'logs.txt')\n",
    "error_file = os.path.join(folder_results, 'errors.txt')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "args = lambda: None\n",
    "args.continue_benchmarking, args.n_jobs = True, 2\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-continue_benchmarking', default= False, action='store_true')\n",
    "# parser.add_argument('-n_jobs', default= 1, type= int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.n_jobs > 1:\n",
    "    pandarallel.initialize(nb_workers= args.n_jobs)\n",
    "\n",
    "datasets = [\n",
    "    \"simulated_gm\",\n",
    "    \"simulated_InterSIM\",\n",
    "    \"simulated_netMUG\",\n",
    "    \"nutrimouse_genotype\",\n",
    "    \"nutrimouse_diet\",\n",
    "    \"bbcsport\",\n",
    "    \"buaa\",\n",
    "    \"metabric\",\n",
    "    \"digits\",\n",
    "    \"bdgp\",\n",
    "    \"tcga\",\n",
    "    \"caltech101\",\n",
    "    \"nuswide\",\n",
    "]\n",
    "two_view_datasets = [\"simulated_gm\", \"nutrimouse_genotype\", \"nutrimouse_diet\", \"metabric\", \"bdgp\",\n",
    "                     \"buaa\", \"simulated_netMUG\"]\n",
    "amputation_mechanisms = [\"EDM\", 'MCAR', 'MAR', 'MNAR']\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(25)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(),\n",
    "                                   MultiViewTransformer(FunctionTransformer(pd.DataFrame)), ConcatenateViews(),\n",
    "                                   StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"IntNMF\": {\"alg\": MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"COCA\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()), \"missing_percentage\": probs,\n",
    "                   \"amputation_mechanism\": amputation_mechanisms, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "indexes_names = list(indexes_results.keys())\n",
    "results = GetResult.create_results_table(datasets=datasets, indexes_results=indexes_results,\n",
    "                                         indexes_names=indexes_names, amputation_mechanisms=amputation_mechanisms,\n",
    "                                         two_view_datasets=two_view_datasets)\n",
    "\n",
    "if not args.continue_benchmarking:\n",
    "    if not eval(input(\"Are you sure you want to start benchmarking and delete previous results? (True/False)\")):\n",
    "        raise Exception\n",
    "    results.to_csv(file_path)\n",
    "\n",
    "    shutil.rmtree(subresults_path, ignore_errors=True)\n",
    "    os.mkdir(subresults_path)\n",
    "\n",
    "    os.remove(logs_file) if os.path.exists(logs_file) else None\n",
    "    os.remove(error_file) if os.path.exists(error_file) else None\n",
    "    open(logs_file, 'w').close()\n",
    "    open(error_file, 'w').close()\n",
    "else:\n",
    "    finished_results = pd.read_csv(file_path, index_col= indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "    finished_results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                    indexes_names=indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "\n",
    "results = results.sort_index(level= \"missing_percentage\", sort_remaining= False)\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    names = dataset_name.split(\"_\")\n",
    "    if \"simulated\" in names:\n",
    "        names = [\"_\".join(names)]\n",
    "    x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "    y = y[y_name]\n",
    "    n_clusters = y.nunique()\n",
    "    unfinished_results_dataset = unfinished_results.loc[[dataset_name]]\n",
    "\n",
    "    if args.n_jobs == 1:\n",
    "        iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "        iterator.apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y, n_clusters=n_clusters,\n",
    "                                                         algorithms=algorithms, random_state=random_state,\n",
    "                                                         subresults_path=subresults_path, logs_file=logs_file,\n",
    "                                                         error_file=error_file), axis= 1)\n",
    "    else:\n",
    "        if 0 in unfinished_results_dataset.index.get_level_values(\"missing_percentage\"):\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.xs(0, level=\"missing_percentage\",\n",
    "                                                                           drop_level=False).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns= indexes_names)\n",
    "            iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                      n_clusters=n_clusters,\n",
    "                                                                      algorithms=algorithms,\n",
    "                                                                      random_state=random_state,\n",
    "                                                                      subresults_path=subresults_path,\n",
    "                                                                      logs_file=logs_file,\n",
    "                                                                      error_file=error_file), axis= 1)\n",
    "            results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                   indexes_names=indexes_names)\n",
    "            results.to_csv(file_path)\n",
    "\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.drop(unfinished_results_dataset_idx).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns=indexes_names)\n",
    "        else:\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "\n",
    "        iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                  n_clusters=n_clusters,\n",
    "                                                                  algorithms=algorithms,\n",
    "                                                                  random_state=random_state,\n",
    "                                                                  subresults_path=subresults_path,\n",
    "                                                                  logs_file=logs_file,\n",
    "                                                                  error_file=error_file), axis= 1)\n",
    "        results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                               indexes_names=indexes_names)\n",
    "        results.to_csv(file_path)\n",
    "        GetResult.remove_subresults(results=results, subresults_path=subresults_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4eb4b696dcd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils.getresult import GetResult\n",
    "\n",
    "\n",
    "folder_results = \"results\"\n",
    "folder_subresults = \"subresults\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_results, filelame)\n",
    "subresults_path = os.path.join(folder_results, folder_subresults)\n",
    "logs_file = os.path.join(folder_results, 'logs.txt')\n",
    "error_file = os.path.join(folder_results, 'errors.txt')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "args = lambda: None\n",
    "args.continue_benchmarking, args.n_jobs = True, 2\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-continue_benchmarking', default= False, action='store_true')\n",
    "# parser.add_argument('-n_jobs', default= 1, type= int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.n_jobs > 1:\n",
    "    pandarallel.initialize(nb_workers= args.n_jobs)\n",
    "\n",
    "datasets = [\n",
    "    \"simulated_gm\",\n",
    "    \"simulated_InterSIM\",\n",
    "    \"simulated_netMUG\",\n",
    "    \"nutrimouse_genotype\",\n",
    "    \"nutrimouse_diet\",\n",
    "    \"bbcsport\",\n",
    "    \"buaa\",\n",
    "    \"metabric\",\n",
    "    \"digits\",\n",
    "    \"bdgp\",\n",
    "    \"tcga\",\n",
    "    \"caltech101\",\n",
    "    \"nuswide\",\n",
    "]\n",
    "two_view_datasets = [\"simulated_gm\", \"nutrimouse_genotype\", \"nutrimouse_diet\", \"metabric\", \"bdgp\",\n",
    "                     \"buaa\", \"simulated_netMUG\"]\n",
    "amputation_mechanisms = [\"EDM\", 'MCAR', 'MAR', 'MNAR']\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(25)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(),\n",
    "                                   MultiViewTransformer(FunctionTransformer(pd.DataFrame)), ConcatenateViews(),\n",
    "                                   StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"IntNMF\": {\"alg\": MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"COCA\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()), \"missing_percentage\": probs,\n",
    "                   \"amputation_mechanism\": amputation_mechanisms, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "indexes_names = list(indexes_results.keys())\n",
    "results = GetResult.create_results_table(datasets=datasets, indexes_results=indexes_results,\n",
    "                                         indexes_names=indexes_names, amputation_mechanisms=amputation_mechanisms,\n",
    "                                         two_view_datasets=two_view_datasets)\n",
    "\n",
    "if not args.continue_benchmarking:\n",
    "    if not eval(input(\"Are you sure you want to start benchmarking and delete previous results? (True/False)\")):\n",
    "        raise Exception\n",
    "    results.to_csv(file_path)\n",
    "\n",
    "    shutil.rmtree(subresults_path, ignore_errors=True)\n",
    "    os.mkdir(subresults_path)\n",
    "\n",
    "    os.remove(logs_file) if os.path.exists(logs_file) else None\n",
    "    os.remove(error_file) if os.path.exists(error_file) else None\n",
    "    open(logs_file, 'w').close()\n",
    "    open(error_file, 'w').close()\n",
    "else:\n",
    "    finished_results = pd.read_csv(file_path, index_col= indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "    finished_results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                    indexes_names=indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "\n",
    "results = results.sort_index(level= \"missing_percentage\", sort_remaining= False)\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    names = dataset_name.split(\"_\")\n",
    "    if \"simulated\" in names:\n",
    "        names = [\"_\".join(names)]\n",
    "    x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "    y = y[y_name]\n",
    "    n_clusters = y.nunique()\n",
    "    unfinished_results_dataset = unfinished_results.loc[[dataset_name]]\n",
    "\n",
    "    if args.n_jobs == 1:\n",
    "        iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "        iterator.apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y, n_clusters=n_clusters,\n",
    "                                                         algorithms=algorithms, random_state=random_state,\n",
    "                                                         subresults_path=subresults_path, logs_file=logs_file,\n",
    "                                                         error_file=error_file), axis= 1)\n",
    "    else:\n",
    "        if 0 in unfinished_results_dataset.index.get_level_values(\"missing_percentage\"):\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.xs(0, level=\"missing_percentage\",\n",
    "                                                                           drop_level=False).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns= indexes_names)\n",
    "            iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                      n_clusters=n_clusters,\n",
    "                                                                      algorithms=algorithms,\n",
    "                                                                      random_state=random_state,\n",
    "                                                                      subresults_path=subresults_path,\n",
    "                                                                      logs_file=logs_file,\n",
    "                                                                      error_file=error_file), axis= 1)\n",
    "            results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                   indexes_names=indexes_names)\n",
    "            results.to_csv(file_path)\n",
    "\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.drop(unfinished_results_dataset_idx).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns=indexes_names)\n",
    "        else:\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "\n",
    "        iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                  n_clusters=n_clusters,\n",
    "                                                                  algorithms=algorithms,\n",
    "                                                                  random_state=random_state,\n",
    "                                                                  subresults_path=subresults_path,\n",
    "                                                                  logs_file=logs_file,\n",
    "                                                                  error_file=error_file), axis= 1)\n",
    "        results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                               indexes_names=indexes_names)\n",
    "        results.to_csv(file_path)\n",
    "        GetResult.remove_subresults(results=results, subresults_path=subresults_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feda9365-8bde-471e-9b80-35032e409922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils.getresult import GetResult\n",
    "\n",
    "\n",
    "folder_results = \"results\"\n",
    "folder_subresults = \"subresults\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_results, filelame)\n",
    "subresults_path = os.path.join(folder_results, folder_subresults)\n",
    "logs_file = os.path.join(folder_results, 'logs.txt')\n",
    "error_file = os.path.join(folder_results, 'errors.txt')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "args = lambda: None\n",
    "args.continue_benchmarking, args.n_jobs = True, 2\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-continue_benchmarking', default= False, action='store_true')\n",
    "# parser.add_argument('-n_jobs', default= 1, type= int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if args.n_jobs > 1:\n",
    "    pandarallel.initialize(nb_workers= args.n_jobs)\n",
    "\n",
    "datasets = [\n",
    "    \"simulated_gm\",\n",
    "    \"simulated_InterSIM\",\n",
    "    \"simulated_netMUG\",\n",
    "    \"nutrimouse_genotype\",\n",
    "    \"nutrimouse_diet\",\n",
    "    \"bbcsport\",\n",
    "    \"buaa\",\n",
    "    \"metabric\",\n",
    "    \"digits\",\n",
    "    \"bdgp\",\n",
    "    \"tcga\",\n",
    "    \"caltech101\",\n",
    "    \"nuswide\",\n",
    "]\n",
    "two_view_datasets = [\"simulated_gm\", \"nutrimouse_genotype\", \"nutrimouse_diet\", \"metabric\", \"bdgp\",\n",
    "                     \"buaa\", \"simulated_netMUG\"]\n",
    "amputation_mechanisms = [\"EDM\", 'MCAR', 'MAR', 'MNAR']\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(25)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(),\n",
    "                                   MultiViewTransformer(FunctionTransformer(pd.DataFrame)), ConcatenateViews(),\n",
    "                                   StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"IntNMF\": {\"alg\": MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"COCA\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()), \"missing_percentage\": probs,\n",
    "                   \"amputation_mechanism\": amputation_mechanisms, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "indexes_names = list(indexes_results.keys())\n",
    "results = GetResult.create_results_table(datasets=datasets, indexes_results=indexes_results,\n",
    "                                         indexes_names=indexes_names, amputation_mechanisms=amputation_mechanisms,\n",
    "                                         two_view_datasets=two_view_datasets)\n",
    "\n",
    "if not args.continue_benchmarking:\n",
    "    if not eval(input(\"Are you sure you want to start benchmarking and delete previous results? (True/False)\")):\n",
    "        raise Exception\n",
    "    results.to_csv(file_path)\n",
    "\n",
    "    shutil.rmtree(subresults_path, ignore_errors=True)\n",
    "    os.mkdir(subresults_path)\n",
    "\n",
    "    os.remove(logs_file) if os.path.exists(logs_file) else None\n",
    "    os.remove(error_file) if os.path.exists(error_file) else None\n",
    "    open(logs_file, 'w').close()\n",
    "    open(error_file, 'w').close()\n",
    "else:\n",
    "    finished_results = pd.read_csv(file_path, index_col= indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "    finished_results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                    indexes_names=indexes_names)\n",
    "    results.loc[finished_results.index, finished_results.columns] = finished_results\n",
    "\n",
    "results = results.sort_index(level= \"missing_percentage\", sort_remaining= False)\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    names = dataset_name.split(\"_\")\n",
    "    if \"simulated\" in names:\n",
    "        names = [\"_\".join(names)]\n",
    "    x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "    y = y[y_name]\n",
    "    n_clusters = y.nunique()\n",
    "    unfinished_results_dataset = unfinished_results.loc[[dataset_name]]\n",
    "\n",
    "    if args.n_jobs == 1:\n",
    "        iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "        iterator.apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y, n_clusters=n_clusters,\n",
    "                                                         algorithms=algorithms, random_state=random_state,\n",
    "                                                         subresults_path=subresults_path, logs_file=logs_file,\n",
    "                                                         error_file=error_file), axis= 1)\n",
    "    else:\n",
    "        if 0 in unfinished_results_dataset.index.get_level_values(\"missing_percentage\"):\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.xs(0, level=\"missing_percentage\",\n",
    "                                                                           drop_level=False).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns= indexes_names)\n",
    "            iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                      n_clusters=n_clusters,\n",
    "                                                                      algorithms=algorithms,\n",
    "                                                                      random_state=random_state,\n",
    "                                                                      subresults_path=subresults_path,\n",
    "                                                                      logs_file=logs_file,\n",
    "                                                                      error_file=error_file), axis= 1)\n",
    "            results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                                   indexes_names=indexes_names)\n",
    "            results.to_csv(file_path)\n",
    "\n",
    "            unfinished_results_dataset_idx = unfinished_results_dataset.drop(unfinished_results_dataset_idx).index\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset_idx.to_list(), columns=indexes_names)\n",
    "        else:\n",
    "            iterator = pd.DataFrame(unfinished_results_dataset.index.to_list(), columns=indexes_names)\n",
    "\n",
    "        iterator.parallel_apply(lambda x: GetResult.run_iteration(idx= x, results= results, Xs=Xs, y=y,\n",
    "                                                                  n_clusters=n_clusters,\n",
    "                                                                  algorithms=algorithms,\n",
    "                                                                  random_state=random_state,\n",
    "                                                                  subresults_path=subresults_path,\n",
    "                                                                  logs_file=logs_file,\n",
    "                                                                  error_file=error_file), axis= 1)\n",
    "        results = GetResult.collect_subresults(results=results.copy(), subresults_path=subresults_path,\n",
    "                                               indexes_names=indexes_names)\n",
    "        results.to_csv(file_path)\n",
    "        GetResult.remove_subresults(results=results, subresults_path=subresults_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7478410-990e-4abf-ae6e-68c0b44dbbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(),\n",
    "                                   MultiViewTransformer(FunctionTransformer(pd.DataFrame)), ConcatenateViews(),\n",
    "                                   StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"IntNMF\": {\"alg\": MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"COCA\": {\"alg\": MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\")), \"params\": {}},\n",
    "    \"NEMO\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                  NEMO()),\n",
    "                                  \"params\": {}},\n",
    "    \"DAIMC\": {\"alg\": make_pipeline(MultiViewTransformer(FunctionTransformer(lambda x: x.divide(x.pow(2).sum(axis=1).pow(1/2), axis= 0))),\n",
    "        DAIMC()),\n",
    "        \"params\": {}},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0564a45d-c336-4dca-b78a-f67a129b92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"simulated_gm\"\n",
    "amputation_mechanism = \"MCAR\"\n",
    "random_state = 42\n",
    "run_n = 0\n",
    "p = 0.6\n",
    "impute = False\n",
    "alg_name = \"NEMO\"\n",
    "incomplete_algorithms = True\n",
    "alg = algorithms[alg_name]\n",
    "errors_dict = defaultdict(int)\n",
    "names = dataset_name.split(\"_\")\n",
    "if \"simulated\" in names:\n",
    "    names = [\"_\".join(names)]\n",
    "x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "y = y[y_name]\n",
    "n_clusters = y.nunique()\n",
    "train_Xs = DatasetUtils.shuffle_imvd(Xs=Xs, random_state=random_state + run_n)\n",
    "y_train = y.loc[train_Xs[0].index]\n",
    "strat = False\n",
    "if p != 0:\n",
    "    if amputation_mechanism == \"EDM\":\n",
    "        try:\n",
    "            assert n_clusters < len(train_Xs[0]) * (1-p)\n",
    "        except AssertionError as exception:\n",
    "            raise AssertionError(f\"{exception}; n_clusters < len(train_Xs[0]) * (1-p)\")\n",
    "        amp = Amputer(p=round(p, 2), mechanism=amputation_mechanism, random_state=random_state + run_n,\n",
    "                     assess_percentage=True, stratify=y_train)\n",
    "        try:\n",
    "            train_Xs = amp.fit_transform(train_Xs)\n",
    "            strat = True\n",
    "        except ValueError:\n",
    "            amp.set_params(**{\"stratify\": None})\n",
    "            train_Xs = amp.fit_transform(train_Xs)\n",
    "    else:\n",
    "        amp = Amputer(p=round(p, 2), mechanism=amputation_mechanism, random_state=random_state + run_n)\n",
    "        train_Xs = amp.fit_transform(train_Xs)\n",
    "if impute:\n",
    "    train_Xs = MultiViewTransformer(SimpleImputer(strategy=\"mean\").set_output(\n",
    "        transform=\"pandas\")).fit_transform(train_Xs)\n",
    "elif not incomplete_algorithms:\n",
    "    train_Xs = DatasetUtils.select_complete_samples(Xs=train_Xs)\n",
    "    y_train = y_train.loc[train_Xs[0].index]\n",
    "DatasetUtils.get_n_complete_samples(Xs=train_Xs), DatasetUtils.get_n_incomplete_samples(Xs=train_Xs), len(train_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3b7a6e-889e-497a-8a32-4f307a675735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.default_rng(random_state).random(10) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a6e29a8-db58-4fb1-8dc5-a4986578e7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.random.default_rng(random_state).normal(size=(len(train_Xs[0]), len(train_Xs)))\n",
    "mask = np.random.default_rng(random_state).random(X.shape) < p\n",
    "views_to_fix = np.random.default_rng(random_state).integers(low=0, high=X.shape[1], size=len(X))\n",
    "samples_to_fix = (mask == False).all(1)\n",
    "for view_idx in np.unique(views_to_fix):\n",
    "    mask[samples_to_fix, view_idx] = True\n",
    "mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c5da82-c82b-48a3-a035-10e03233d2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82175732-294b-405d-b24c-e3472c5a0712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_view_profile = DatasetUtils.get_missing_view_profile(Xs=train_Xs)\n",
    "missing_view_profile.sum(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1189991-a1b0-46d8-9eef-f0686c0b4b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.default_rng(random_state).random(missing_view_profile.shape) < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e9c1be6a-1ea2-4923-8a5d-7fe5e188a9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "views_to_fix = np.random.default_rng(random_state).integers(low=0, high= missing_view_profile.shape[1], size= len(missing_view_profile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef1346f6-81e7-41db-bcde-709f1ee531bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples_to_fix = (missing_view_profile == 0).all(1)\n",
    "for view_idx in np.unique(views_to_fix):\n",
    "    missing_view_profile.loc[samples_to_fix, view_idx] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa07801b-6d8d-4636-af8e-3da1ea54c999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_view_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3c78ab52-427a-4095-97f4-bb000bfb5030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_view_profile.sum(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106702ba-5270-4117-b95e-309e0c5bec6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Model(alg_name=alg_name, alg=alg)\n",
    "clusters, model = model.method(train_Xs=train_Xs, n_clusters=n_clusters, random_state=random_state, run_n=run_n)\n",
    "clusters = pd.Series(clusters, index=y_train.index)\n",
    "metrics.matthews_corrcoef(y_true= y_train, y_pred= kuhn_munkres_algorithm(true_lab=y_train, pred_lab=clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1edc51-4963-4a6a-a1e3-8dfa0edd8f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dataset_name = \"bbcsport\"\n",
    "amputation_mechanism = \"MNAR\"\n",
    "random_state = 42\n",
    "run_n = 1\n",
    "p = 0.2\n",
    "impute = False\n",
    "alg_name = \"COCA\"\n",
    "alg = algorithms[alg_name]\n",
    "errors_dict = defaultdict(int)\n",
    "names = dataset_name.split(\"_\")\n",
    "if \"simulated\" in names:\n",
    "    names = [\"_\".join(names)]\n",
    "x_name,y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "y = y[y_name]\n",
    "n_clusters = y.nunique()\n",
    "train_Xs = DatasetUtils.shuffle_imvd(Xs=Xs, random_state=random_state + run_n)\n",
    "y_train = y.loc[train_Xs[0].index]\n",
    "strat = False\n",
    "if p != 0:\n",
    "    if amputation_mechanism == \"EDM\":\n",
    "        try:\n",
    "            assert n_clusters < len(train_Xs[0]) * (1-p)\n",
    "        except AssertionError as exception:\n",
    "            raise AssertionError(f\"{exception}; n_clusters < len(train_Xs[0]) * (1-p)\")\n",
    "        amp = Amputer(p=round(p, 2), mechanism=amputation_mechanism, random_state=random_state + run_n,\n",
    "                     assess_percentage=True, stratify=y_train)\n",
    "        try:\n",
    "            train_Xs = amp.fit_transform(train_Xs)\n",
    "            strat = True\n",
    "        except ValueError:\n",
    "            amp.set_params(**{\"stratify\": None})\n",
    "            train_Xs = amp.fit_transform(train_Xs)\n",
    "    else:\n",
    "        amp = Amputer(p=round(p, 2), mechanism=amputation_mechanism, random_state=random_state + run_n)\n",
    "        train_Xs = amp.fit_transform(train_Xs)\n",
    "sns.heatmap(DatasetUtils.get_missing_view_profile(Xs=train_Xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b392ef80-4cf5-45e6-b0f4-0cf4740b0543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.cluster import DAIMC\n",
    "from imvc.transformers import MultiViewTransformer\n",
    "    \n",
    "Xs = LoadDataset.load_dataset(dataset_name=\"bbcsport\")\n",
    "pipeline = MultiViewTransformer(make_pipeline(Normalizer().set_output(transform=\"pandas\"),\n",
    "                    SimpleImputer(strategy=\"constant\", fill_value=0).set_output(transform=\"pandas\")))\n",
    "Xs = pipeline.fit_transform(Xs)\n",
    "estimator = DAIMC(n_clusters = 2)\n",
    "labels = estimator.fit_predict(Xs)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4a47b-1fdc-4ddc-be1d-2f045b52a315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import oct2py\n",
    "\n",
    "oc = oct2py.Oct2Py(temp_dir = \"imvc/cluster/_daimc/\")\n",
    "transformed_train_Xs = MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")).fit_transform(train_Xs)\n",
    "missing_view_profile = DatasetUtils.get_missing_view_profile(transformed_train_Xs)\n",
    "transformed_train_Xs = [X.T for X in transformed_train_Xs]\n",
    "transformed_train_Xs = tuple(transformed_train_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f474b-8500-4084-ac06-09de61c97e62",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(\"imvc\", \"cluster\", \"_daimc\", \"newinit.m\")) as f:\n",
    "    oc.eval(f.read())\n",
    "with open(os.path.join(\"imvc\", \"cluster\", \"_daimc\", \"litekmeans.m\")) as f:\n",
    "    oc.eval(f.read())\n",
    "with open(os.path.join(\"imvc\", \"cluster\", \"_daimc\", \"DAIMC.m\")) as f:\n",
    "    oc.eval(f.read())\n",
    "with open(os.path.join(\"imvc\", \"cluster\", \"_daimc\", \"UpdateV_DAIMC.m\")) as f:\n",
    "    oc.eval(f.read())\n",
    "oc.eval(\"pkg load statistics\")\n",
    "oc.eval(\"pkg load control\")\n",
    "oc.warning(\"off\", \"Octave:possible-matlab-short-circuit-operator\")\n",
    "\n",
    "options = {\"afa\": 0.0001, \"beta\": 10000}\n",
    "w = tuple([oc.diag(missing_view) for _,missing_view in missing_view_profile.items()])\n",
    "u_0, v_0, b_0 = oc.newinit(transformed_train_Xs, w, n_clusters, len(transformed_train_Xs), nout=3)\n",
    "u, v, b, f, p, n = oc.DAIMC(transformed_train_Xs, w, u_0, v_0, b_0, None, n_clusters, len(transformed_train_Xs), options, nout=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea843b-f1a1-4863-8f16-562d3fc50809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0683a-5aef-45f2-8715-5575484fc31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import oct2py\n",
    "\n",
    "# x = scipy.io.loadmat(f'imvc/datasets/data/bbcsport/original/bbcsport4vbigRnSp.mat')\n",
    "oc = oct2py.Oct2Py(temp_dir = \"imvc/temp\")\n",
    "oc.eval(\"Dataname = 'bbcsport4vbigRnSp'\")\n",
    "oc.eval(\"Datafold = ['imvc/datasets/data/bbcsport/original/bbcsport4vbigRnSp.mat']\")\n",
    "oc.load(Dataname);\n",
    "oc.load(Datafold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd45902-f938-4931-8076-e1ae308df6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Dataname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047daba-98de-446a-9a58-350999e7ce05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_Xs = make_pipeline(MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\"))).fit_transform(train_Xs)\n",
    "mask = [X.notnull().astype(int) for X in train_Xs]\n",
    "mask = Utils.convert_df_to_r_object(mask)\n",
    "train_Xs = Utils.convert_df_to_r_object(train_Xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe366a8-d917-42ac-b453-113b24686475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnTensor = importr(\"nnTensor\")\n",
    "clusters = nnTensor.jNMF(train_Xs, M= mask, J= n_clusters)\n",
    "clusters = np.array(clusters[0]).argmax(axis=1)\n",
    "clusters = pd.Series(clusters, index=y_train.index)\n",
    "metrics.matthews_corrcoef(y_true= y_train, y_pred= kuhn_munkres_algorithm(true_lab=y_train, pred_lab=clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d20bf-5628-4770-957c-dd919eae1044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs, y, met = LoadDataset.load_dataset(dataset_name=\"tcga\", return_y=True, shuffle= False, return_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f33829-cb86-4bcc-925b-1aa77afbc799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78457e-92f6-4414-9789-5d9546c4e569",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages(\"nnTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803d2b9-60d9-474d-880f-a030ce4842f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(\"nnTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977cba0-cad3-4259-a07f-6e1470f11b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17800f7-efe6-4e4f-8d9a-eadaefe59e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meth = pd.read_csv(\"imvc/datasets/data/tcga/original/DNAmethylationClusteringMatrix.csv\", index_col= 0).T\n",
    "meth.index = meth.index.str[:12]\n",
    "print(meth.shape)\n",
    "meth = meth.loc[:, (meth.isna().sum() < meth.shape[1]*0.01).values]\n",
    "print(meth.shape)\n",
    "meth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab43f0d-f938-4a01-bbef-aff30c4a6419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mirna = pd.read_csv(\"imvc/datasets/data/tcga/original/PanCan.miRNAseq.RPM.215-MIMATs-most-variant-25pc.4229-samples.NMF-input.BCGSC.20140603.csv\", index_col= 0).T\n",
    "mirna.index = mirna.index.str.split(\"_\").str[1].str[:12]\n",
    "mirna.columns.name = None\n",
    "print(mirna.shape)\n",
    "mirna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7089a30-2370-49d3-8588-5f79cd806656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prot = pd.read_csv(\"imvc/datasets/data/tcga/original/PanCan11_RBN_RPPA_without_Duplicates_20130325.csv\", index_col= 0)\n",
    "prot = prot.drop(columns= [\"Set\",\"Sample_Source\",\"Sample_description\",\"UUID\"])\n",
    "prot.index.name = None\n",
    "print(prot.shape)\n",
    "prot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a228db9-7a0a-4447-89bd-e915749e640e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = pd.read_csv(\"imvc/datasets/data/tcga/original/PanCan12.3602-corrected-v3.txt\", sep= \"\\t\", index_col= 0, skiprows=1).T\n",
    "exp.index = exp.index.str[:12]\n",
    "print(exp.shape)\n",
    "exp = exp.loc[:, (exp.isna().sum() < exp.shape[1]*0.01).values]\n",
    "print(exp.shape)\n",
    "exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3d4ee-ad4d-4ecd-b81c-7c507cb33334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idxs = meth.index.intersection(prot.index).intersection(mirna.index).intersection(exp.index)\n",
    "meth, prot = meth.reset_index().drop_duplicates(subset=\"index\").set_index(\"index\"), prot.reset_index().drop_duplicates(subset=\"index\").set_index(\"index\")\n",
    "mirna, exp = mirna.reset_index().drop_duplicates(subset=\"index\").set_index(\"index\"), exp.reset_index().drop_duplicates(subset=\"index\").set_index(\"index\")\n",
    "prot = prot.loc[idxs]\n",
    "meth = meth.loc[idxs]\n",
    "mirna = mirna.loc[idxs]\n",
    "exp = exp.loc[idxs, (exp - exp.median(axis=0)).abs().median().sort_values(ascending= False).iloc[:2000].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ede7d-7bec-4a9d-a585-ab42628a6e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder(dtype= int).set_output(transform= \"pandas\")\n",
    "tumor = oe.fit_transform(prot[[\"Tumor\"]]).squeeze()\n",
    "prot = prot.drop(columns= \"Tumor\")\n",
    "tumor.name = None\n",
    "tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c61ee-9ec2-4a09-b7f4-9bb79f87e639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "met = {\n",
    "    \"modality\": {0: \"miRNA\", 1: \"mRNA\", 2: \"RPPA\", 3: \"methyl\"},\n",
    "    \"labels\": {i:j for i,j in enumerate(oe.categories_[0])},\n",
    "    \"samples\": {i:j for i,j in enumerate(tumor.index)},\n",
    "}\n",
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746df86b-bd3b-4527-b8e4-d19323eea9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(meth.shape)\n",
    "print(prot.shape)\n",
    "print(mirna.shape)\n",
    "print(exp.shape)\n",
    "print(tumor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbaafd1-eee8-40cd-abe3-396f3ac53e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn = KNNImputer().set_output(transform=\"pandas\")\n",
    "meth = knn.fit_transform(meth)\n",
    "meth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156b834-46c1-415a-b011-cb87e28f3578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = knn.fit_transform(exp)\n",
    "exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ceb968-e931-4c54-a9ff-103af165029f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meth.to_csv(\"imvc/datasets/data/tcga/tcga_3.csv\", index= False)\n",
    "prot.to_csv(\"imvc/datasets/data/tcga/tcga_2.csv\", index= False)\n",
    "mirna.to_csv(\"imvc/datasets/data/tcga/tcga_0.csv\", index= False)\n",
    "exp.to_csv(\"imvc/datasets/data/tcga/tcga_1.csv\", index= False)\n",
    "tumor.to_csv(\"imvc/datasets/data/tcga/tcga_y.csv\", index= False)\n",
    "with open('imvc/datasets/data/tcga/metadata.json', 'w') as fp:\n",
    "    json.dump(met, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c19eb-23f0-4cc6-a406-458b7f03d00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mvlearn.datasets import make_gaussian_mixture\n",
    "import numpy as np\n",
    "n_samples = 100\n",
    "centers = [[0,1], [0,-1]]\n",
    "covariances = [np.eye(2), np.eye(2)]\n",
    "Xs, y = make_gaussian_mixture(n_samples, centers, covariances,\n",
    "                              shuffle=True, shuffle_random_state=42)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f805e9-6bd3-475d-bdf7-8789b7e050d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(y).astype(int).to_csv(f\"imvc/datasets/data/simulated_gm/simulated_gm_y.csv\",  index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e17f08-f3b3-4aa2-b25c-944cb800e3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[pd.DataFrame(X).to_csv(f\"imvc/datasets/data/simulated_gm/simulated_gm_{i}.csv\", index= False) for i,X in enumerate(Xs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566eeb53-a952-483f-a296-49d0c56aefc3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(\"coca\")\n",
    "\n",
    "clusters = coca(arrays, K = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079b8ef-b263-4a03-8081-17eec1d0488c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Model(alg_name = \"IntNMF\", alg = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2e495-33df-4f77-8297-4bd25144a069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in os.listdir(\"imvc/datasets/data/buaa\"):\n",
    "    if \"y.csv\" in i:\n",
    "        continue\n",
    "    if \".csv\" in i:\n",
    "        pd.read_csv(os.path.join(\"imvc/datasets/data/buaa\", i), index_col= 0).iloc[:90].to_csv(os.path.join(\"imvc/datasets/data/buaa\", i), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06f365-6661-426e-ae63-8f67de874bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "for i in os.listdir(\"imvc/datasets/data/\"):\n",
    "    print(i)\n",
    "    for j in os.listdir(os.path.join(\"imvc/datasets/data/\", i)):\n",
    "        if os.path.isfile(os.path.join(\"imvc/datasets/data/\", i, j)):\n",
    "            a = pd.read_csv(os.path.join(\"imvc/datasets/data/\", i, j))\n",
    "            print(j, a.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644e25a-8bdd-4592-b661-f578a1527387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "for i in os.listdir(\"imvc/datasets/data/\"):\n",
    "    print(i)\n",
    "    for j in os.listdir(os.path.join(\"imvc/datasets/data/\", i)):\n",
    "        if os.path.isfile(os.path.join(\"imvc/datasets/data/\", i, j)):\n",
    "            a = pd.read_csv(os.path.join(\"imvc/datasets/data/\", i, j))\n",
    "            print(j, a.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68327e9-ab81-4dd7-bd48-496655af9430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "metadata = {\"modality\": {0: \"morphological features\", 1: \"Karhunen-Love coefficients\", 2: \"profile correlations\", 3: \"Zernike moments\", 4: \"Fourier coefficients of the character shapes\", 5: \"pixel averages of the images from 2x3 windows\"}}\n",
    "import json\n",
    "\n",
    "with open(os.path.join(\"imvc/datasets/data/digits\", 'metadata.json'), 'w') as fp:\n",
    "    json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfdcee-4a3d-4b4a-b526-0ae8748e14ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bb[[\"Patient ID\", \"Pam50 + Claudin-low subtype\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359b8ba-995a-4600-b5f6-9c4098f861e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate([\"X\", \"Ya\"]):\n",
    "    x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/{name}.mat')[name]\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/bdgp/bdgp_{i}.csv', index= False)\n",
    "x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/Yc.mat')[\"Yc\"]\n",
    "pd.DataFrame(x.argmax(1)).to_csv(f'imvc/datasets/data/bdgp/bdgp_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fcf41-846f-470b-902b-a03f295660c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,name in enumerate([\"X\", \"Ya\"]):\n",
    "    x = scipy.io.loadmat(f'imvc/datasets/data/sensIT300/original/sensIT300.mat')[name]\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/bdgp/bdgp_{i}.csv', index= False)\n",
    "x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/Yc.mat')[\"Yc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bbe8b9a-15dd-41bb-962f-9f539083a9be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 50)\n",
      "(300, 50)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "for i,name in enumerate([\"fe1\", \"fe2\"]):\n",
    "    x = scipy.io.loadmat('imvc/datasets/data/sensIT300/original/sensIT300.mat')[name]\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/sensIT300/sensIT300_{i}.csv', index= False)\n",
    "x = scipy.io.loadmat('imvc/datasets/data/sensIT300/original/sensIT300.mat')[\"Label300\"] -1\n",
    "pd.DataFrame(x).to_csv(f'imvc/datasets/data/sensIT300/sensIT300_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84c0072f-859d-4877-9eab-f554b78d06bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2310, 9)\n",
      "(2310, 10)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "dataset_name = \"statlog\"\n",
    "for i,name in enumerate([\"fe1\", \"fe2\"]):\n",
    "    x = scipy.io.loadmat(f'imvc/datasets/data/{dataset_name}/original/mul_image_segmentation.mat')[\"data\"][i][0]\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/{dataset_name}/{dataset_name}_{i}.csv', index= False)\n",
    "x = scipy.io.loadmat(f'imvc/datasets/data/{dataset_name}/original/mul_image_segmentation.mat')[\"labels\"] -1\n",
    "pd.DataFrame(x).to_csv(f'imvc/datasets/data/{dataset_name}/{dataset_name}_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b37cd77e-5823-4008-b43f-27fe0639f353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1051, 3000)\n",
      "(1051, 1840)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"webkb\"\n",
    "pd.Series(scipy.io.loadmat(f'imvc/datasets/data/{dataset_name}/original/PAGE.mat')[\"Y\"].flatten()).value_counts()\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "dataset_name = \"webkb\"\n",
    "for i,name in enumerate([\"PAGE\", \"LINK\"]):\n",
    "    x = scipy.io.loadmat(f'imvc/datasets/data/{dataset_name}/original/{name}.mat')[\"X\"]\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/{dataset_name}/{dataset_name}_{i}.csv', index= False)\n",
    "x = scipy.io.loadmat(f'imvc/datasets/data/{dataset_name}/original/{name}.mat')[\"Y\"]\n",
    "pd.DataFrame(pd.factorize(x.flatten())[0]).to_csv(f'imvc/datasets/data/{dataset_name}/{dataset_name}_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e19b0bd8-90e5-4f93-9dad-fba1b5ce080e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 265)\n",
      "(265, 1703)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "dataset_name = \"wisconsin\"\n",
    "for i,name in enumerate([\"cites\", \"content\"]):\n",
    "    x = scipy.io.mmread(f'imvc/datasets/data/{dataset_name}/original/{dataset_name}_{name}.mtx').toarray()\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/{dataset_name}/{dataset_name}_{i}.csv', index= False)\n",
    "x = pd.read_csv(\"imvc/datasets/data/wisconsin/original/wisconsin_act.txt\", header=None)-1\n",
    "x.to_csv(f'imvc/datasets/data/{dataset_name}/{dataset_name}_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "86c4c40c-23de-4b69-b98d-8bd84794958f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcsport\n",
      "Samples: 116; Views: 4; Features: [1991, 2063, 2113, 2158]; Clusters: 5.\n",
      "\n",
      "bdgp\n",
      "Samples: 2500; Views: 2; Features: [1750, 79]; Clusters: 5.\n",
      "\n",
      "buaa\n",
      "Samples: 90; Views: 2; Features: [100, 100]; Clusters: 10.\n",
      "\n",
      "caltech101\n",
      "Samples: 9144; Views: 6; Features: [48, 40, 254, 1984, 512, 928]; Clusters: 102.\n",
      "\n",
      "digits\n",
      "Samples: 2000; Views: 6; Features: [6, 64, 216, 47, 76, 240]; Clusters: 10.\n",
      "\n",
      "metabric\n",
      "Samples: 1904; Views: 2; Features: [2000, 2000]; Clusters: 7.\n",
      "\n",
      "nuswide\n",
      "Samples: 30000; Views: 5; Features: [64, 225, 144, 73, 128]; Clusters: 31.\n",
      "\n",
      "sensIT300\n",
      "Samples: 300; Views: 2; Features: [50, 50]; Clusters: 3.\n",
      "\n",
      "simulated_InterSIM\n",
      "Samples: 500; Views: 3; Features: [367, 131, 160]; Clusters: 3.\n",
      "\n",
      "simulated_gm\n",
      "Samples: 100; Views: 2; Features: [2, 2]; Clusters: 2.\n",
      "\n",
      "simulated_netMUG\n",
      "Samples: 1000; Views: 2; Features: [2000, 2000]; Clusters: 3.\n",
      "\n",
      "statlog\n",
      "Samples: 2310; Views: 2; Features: [9, 10]; Clusters: 7.\n",
      "\n",
      "tcga\n",
      "Samples: 2437; Views: 4; Features: [215, 2000, 131, 1739]; Clusters: 10.\n",
      "\n",
      "webkb\n",
      "Samples: 1051; Views: 2; Features: [3000, 1840]; Clusters: 2.\n",
      "\n",
      "wisconsin\n",
      "Samples: 265; Views: 2; Features: [265, 1703]; Clusters: 5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from imvc.datasets import LoadDataset\n",
    "\n",
    "datasets = sorted([f for f in os.listdir(\"imvc/datasets/data\") if not f.startswith('.')])\n",
    "for dataset_name in datasets:\n",
    "    names = dataset_name.split(\"_\")\n",
    "    if \"simulated\" in names:\n",
    "        names = [\"_\".join(names)]\n",
    "    x_name, y_name = names if len(names) > 1 else (names[0], \"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True)\n",
    "    try:\n",
    "        y = y[y_name]\n",
    "    except:\n",
    "        continue\n",
    "    n_clusters = y.nunique()\n",
    "    print(dataset_name)\n",
    "    print(f\"Samples: {len(Xs[0])}; Views: {len(Xs)}; Features: {[X.shape[1] for X in Xs]}; Clusters: {n_clusters}.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c82d6-e3b7-4f35-8380-fedca782ef31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/tcga\"\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "for i,x in enumerate([\"exp\", \"methy\", \"mirna\"]):\n",
    "    target = []\n",
    "    files_x = [os.path.join(path, file) for file in files if file.startswith(x)]\n",
    "    ds = []\n",
    "    for file_x in files_x:\n",
    "        d_x = pd.read_csv(file_x, index_col= 0).T\n",
    "        print(file_x, d_x.shape)\n",
    "        target.extend([file_x.split(\"_\")[-1]]* d_x.shape[0])\n",
    "        ds.append(d_x)\n",
    "    d = pd.concat(ds)\n",
    "    print(x, d.shape)\n",
    "    d = d.dropna(axis= 1)\n",
    "    print(x, d.shape)\n",
    "    d.to_csv(os.path.join(path, f'tcga_{i}.csv'))\n",
    "pd.Series(target).to_csv(os.path.join(path, 'tcga_y.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1fa46-5710-40f3-a6a4-13c7f9c96006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/tcga\"\n",
    "files = [os.path.join(path, f) for f in os.listdir(path) if f.startswith(\"tcga_\")]\n",
    "d = pd.concat([pd.read_csv(file) for file in files], axis= 1)\n",
    "for i,file in enumerate(files):\n",
    "    d_x = pd.read_csv(file)\n",
    "    print(file, d_x.shape, d_x.loc[d.index])\n",
    "    d_x.loc[d.index].to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7d760-b3cb-43f4-afa4-16f588a1a44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/tcga\"\n",
    "for i,x in enumerate([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]):\n",
    "    with ZipFile(os.path.join(path, x)) as zf:\n",
    "        for file in zf.namelist():\n",
    "            with zf.open(file) as f2:\n",
    "                d = pd.read_csv(f2, sep= \" \")\n",
    "                print(file, d.shape)\n",
    "                d.to_csv(f\"{os.path.join(path, file)}_{x.split('.')[0]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7b2c8-bc2b-4237-8b04-967f054a8cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/digits\"\n",
    "for i,x in enumerate([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]):\n",
    "    d = pd.read_csv(os.path.join(path, x))\n",
    "    print(d.shape)\n",
    "    d.iloc[:, :-1].to_csv(os.path.join(path, f\"digits_{i}.csv\"), index= False)\n",
    "d.iloc[:, -1].to_csv(os.path.join(path, f\"digits_y.csv\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff058a6d-ddb2-4499-8c57-6d6b19b99b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/Yc.mat')[\"Yc\"]\n",
    "x.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53749fa-f7af-4e14-b192-a326e883c5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afef5b-8792-4244-9e96-fa35b00079c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6a5d2-ba01-4d6f-a4fb-7cf5d097fce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"X\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cccf4b-89df-4d0c-a95c-be3977097bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"X\"][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1306beb-4250-49c7-bb64-7986e6c238d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abfe0c-ebd8-43b8-ba13-7fc7f4c21ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = pyreadr.read_r('imvc/datasets/data/metabric/METABRIC_discovery')\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5aeed-9351-4151-b3f2-e9d344c9f43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"mydatCNV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556223fb-cd7d-4a21-b6e0-1bbe3a119356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea130b-e706-4025-afd0-a5e827aa1afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mat[\"Y\"]).squeeze().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9954cb-dc76-45d9-8ae7-219cffe4d972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('imvc/datasets/data/bdgp/X.mat')\n",
    "mat[\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d74d47-3223-4d7e-aaeb-a2de0873505c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('imvc/datasets/data/bdgp/Yc.mat')\n",
    "mat[\"Yc\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22dba5-2b9a-46cf-a9c2-37cf78244a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"Yc\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
