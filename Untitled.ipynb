{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda9365-8bde-471e-9b80-35032e409922",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from snf import compute\n",
    "from bignmf.models.jnmf.integrative import IntegrativeJnmf\n",
    "from bignmf.models.jnmf.standard import StandardJnmf\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.utils import DatasetUtils\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils import save_record, run_iteration\n",
    "\n",
    "folder_name = \"results\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_name, filelame)\n",
    "logs_file = os.path.join(folder_name, 'logs.txt')\n",
    "error_file = os.path.join(folder_name, 'error.txt')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('start_benchmarking', default= False, type= bool)\n",
    "args = parser.parse_args()\n",
    "\n",
    "datasets = [\"nutrimouse_genotype\", \"nutrimouse_diet\", \"bbcsport\", \"bdgp\", \"caltech101\", \"digits\", \"tcga_tissue\", \"tcga_survival\", \"nuswide\", \"metabric\"]\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(10)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(), MultiViewTransformer(FunctionTransformer(pd.DataFrame)),\n",
    "                                   ConcatenateViews(), StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {},\n",
    "    \"intNMF\": {},\n",
    "    \"jNMF\": {},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()),\n",
    "                   \"missing_percentage\": probs, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "\n",
    "\n",
    "if args.start_benchmarking:\n",
    "    results = pd.DataFrame(datasets, columns= [\"dataset\"])\n",
    "    for k,v in {k:v for k,v in indexes_results.items() if k != \"dataset\"}.items():\n",
    "        results = results.merge(pd.Series(v, name= k), how= \"cross\")\n",
    "    results = results.set_index(list(indexes_results.keys()))\n",
    "    results[[\"finished\", \"completed\"]] = False\n",
    "else:\n",
    "    results = pd.read_csv(file_path, index_col= list(indexes_results.keys()))\n",
    "    results_ = results.select_dtypes(object).drop(columns= [\"comments\", \"stratified\"]).replace(np.nan, \"np.nan\")\n",
    "    for col in results_.columns:\n",
    "        results[col] = results_[col].apply(eval)\n",
    "        \n",
    "    open(logs_file, 'w').close()\n",
    "    open(error_file, 'w').close()\n",
    "\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    names = dataset_name.split(\"_\")\n",
    "    x_name,y_name = names if len(names) >1 else (names[0],\"0\")\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=x_name, return_y=True, shuffle= False)\n",
    "    y = y[y_name]\n",
    "    n_clusters = y.nunique()\n",
    "\n",
    "    iterator = pd.Series(unfinished_results.loc[unfinished_results.index.get_level_values(\"dataset\") == dataset_name].index.to_list())\n",
    "    iterator.apply(lambda x: run_iteration(idx= x, results= results, Xs=Xs, y=y, n_clusters=n_clusters,\n",
    "                                           algorithms=algorithms, random_state=random_state, file_path=file_path,\n",
    "                                           logs_file=logs_file, error_file=error_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ba1a4-9005-4882-acec-edd62f153b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5268465-57d3-4d0a-8bdd-476e2865b5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf134aca-e6b8-459e-b8ab-9168082cb934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in iterator:\n",
    "    row = results.loc[[idx]]\n",
    "    row_index = row.index\n",
    "    alg_name, impute, p, run_n = (\n",
    "        row_index.get_level_values(\"algorithm\")[0],\n",
    "        row_index.get_level_values(\"imputation\")[0],\n",
    "        row_index.get_level_values(\"missing_percentage\")[0] / 100,\n",
    "        row_index.get_level_values(\"run_n\")[0])\n",
    "\n",
    "    if p == run_n and impute:\n",
    "        print(row.drop(columns=row.columns).reset_index().to_dict(orient=\"records\")[0], \"\\t\", datetime.now())\n",
    "    alg = algorithms[alg_name]\n",
    "    train_Xs = DatasetUtils.shuffle_imvd(Xs=Xs, random_state=random_state + run_n)\n",
    "    y_train = y.loc[train_Xs[0].index]\n",
    "    errors_dict = defaultdict(int)\n",
    "    if p != 0:\n",
    "        try:\n",
    "            assert n_clusters < len(train_Xs[0]) * (1-p)\n",
    "        except AssertionError as exception:\n",
    "            errors_dict[f\"{type(exception).__name__}: {exception}; n_clusters < len(train_Xs[0]) * (1-p)\"] += 1\n",
    "            # print(errors_dict, row.drop(columns=row.columns).reset_index().to_dict(orient=\"records\")[0])\n",
    "            results.loc[idx, [\"finished\", \"comments\"]] = True, errors_dict\n",
    "            results.to_csv(file_path)\n",
    "            # return results.loc[idx]\n",
    "        try:\n",
    "            train_Xs = DatasetUtils.add_random_noise_to_views(Xs=train_Xs, p=round(p, 2),\n",
    "                                                              random_state=random_state + run_n,\n",
    "                                                              assess_percentage=True, stratify=y_train)\n",
    "            strat = True\n",
    "        except ValueError:\n",
    "            try:\n",
    "                train_Xs = DatasetUtils.add_random_noise_to_views(Xs=train_Xs, p=round(p, 2),\n",
    "                                                                  random_state=random_state + run_n,\n",
    "                                                                  assess_percentage=True)\n",
    "                strat = False\n",
    "            except Exception as exception:\n",
    "                errors_dict[f\"{type(exception).__name__}: {exception}\"] += 1\n",
    "                # print(errors_dict, row.drop(columns=row.columns).reset_index().to_dict(orient=\"records\")[0])\n",
    "                results.loc[idx, [\"finished\", \"comments\"]] = True, errors_dict\n",
    "                results.to_csv(file_path)\n",
    "                # return results.loc[idx]\n",
    "    else:\n",
    "        strat = False\n",
    "\n",
    "    if impute:\n",
    "        train_Xs = MultiViewTransformer(SimpleImputer(strategy=\"mean\").set_output(transform=\"pandas\")).fit_transform(\n",
    "            train_Xs)\n",
    "    else:\n",
    "        train_Xs = DatasetUtils.select_complete_samples(Xs=train_Xs)\n",
    "        y_train = y_train.loc[train_Xs[0].index]\n",
    "\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        if alg_name == \"SNF\":\n",
    "            preprocessing_step = MultiViewTransformer(StandardScaler().set_output(transform=\"pandas\"))\n",
    "            train_Xs = preprocessing_step.fit_transform(train_Xs)\n",
    "            affinities = compute.make_affinity(train_Xs, normalize=False)\n",
    "            fused = compute.snf(affinities)\n",
    "            clusters = spectral_clustering(fused, n_clusters=n_clusters, random_state=random_state + run_n)\n",
    "        elif alg_name == \"intNMF\":\n",
    "            preprocessing_step = MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\"))\n",
    "            train_Xs = preprocessing_step.fit_transform(train_Xs)\n",
    "            model = IntegrativeJnmf({k: v for k, v in enumerate(train_Xs)}, k=n_clusters, lamb=0.1)\n",
    "            model.run(trials=50, iterations=100, verbose=False)\n",
    "            model.cluster_data()\n",
    "            clusters = np.argmax(model.w_cluster, axis=1)\n",
    "        elif alg_name == \"jNMF\":\n",
    "            preprocessing_step = make_pipeline(MultiViewTransformer(MinMaxScaler().set_output(transform=\"pandas\")))\n",
    "            train_Xs = preprocessing_step.fit_transform(train_Xs)\n",
    "            model = StandardJnmf({k: v for k, v in enumerate(train_Xs)}, k=n_clusters)\n",
    "            model.run(trials=50, iterations=100, verbose=False)\n",
    "            model.cluster_data()\n",
    "            clusters = np.argmax(model.w_cluster, axis=1)\n",
    "        else:\n",
    "            model, params = alg[\"alg\"], alg[\"params\"]\n",
    "            if alg_name == \"GroupPCA\":\n",
    "                model[1].set_params(n_components=n_clusters, random_state=random_state + run_n, multiview_output=False)\n",
    "            elif alg_name == \"AJIVE\":\n",
    "                model[1].set_params(random_state=random_state + run_n)\n",
    "            if alg_name == \"NMFC\":\n",
    "                model[-1].set_params(n_components=n_clusters, random_state=random_state + run_n)\n",
    "            else:\n",
    "                model[-1].set_params(n_clusters=n_clusters, random_state=random_state + run_n)\n",
    "            clusters = model.fit_predict(train_Xs)\n",
    "    except ValueError as exception:\n",
    "        if alg_name == \"AJIVE\" and len(y_train) < 5:\n",
    "            errors_dict[f\"{type(exception).__name__}: {exception}\"] += 1\n",
    "            # print(errors_dict, row.drop(columns=row.columns).reset_index().to_dict(orient=\"records\")[0])\n",
    "            results.loc[idx, [\"finished\", \"comments\"]] = True, errors_dict\n",
    "            results.to_csv(file_path)\n",
    "            # return results.loc[idx]\n",
    "        if alg_name == \"SNF\" and len(y_train) < 17:\n",
    "            errors_dict[f\"{type(exception).__name__}: {exception}\"] += 1\n",
    "            # print(errors_dict, row.drop(columns=row.columns).reset_index().to_dict(orient=\"records\")[0])\n",
    "            results.loc[idx, [\"finished\", \"comments\"]] = True, errors_dict\n",
    "            results.to_csv(file_path)\n",
    "            # return results.loc[idx]\n",
    "        if alg_name == \"intNMF\" and len(y_train) < 5:\n",
    "            errors_dict[f\"{type(exception).__name__}: {exception}\"] += 1\n",
    "            # print(errors_dict, row.drop(columns=row.columns).reset_index().to_dict(orient=\"records\")[0])\n",
    "            results.loc[idx, [\"finished\", \"comments\"]] = True, errors_dict\n",
    "            results.to_csv(file_path)\n",
    "            # return results.loc[idx]\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    clusters = pd.Series(clusters, index=y_train.index)\n",
    "\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    if alg_name in [\"NMFC\"]:\n",
    "        train_X = model.transform(train_Xs)\n",
    "    elif alg_name in [\"SNF\"]:\n",
    "        train_X = preprocessing_step.transform(train_Xs)\n",
    "    elif alg_name in [\"intNMF\", \"jNMF\"]:\n",
    "        train_X = model.w\n",
    "    else:\n",
    "        train_X = model[:-1].transform(train_Xs)\n",
    "    if isinstance(train_X, list):\n",
    "        train_X = ConcatenateViews().fit_transform(train_X)\n",
    "    if not isinstance(train_X, pd.DataFrame):\n",
    "        train_X = pd.DataFrame(train_X, index=y_train.index)\n",
    "\n",
    "    assert train_X.index.equals(y_train.index)\n",
    "    assert train_Xs[0].index.equals(y_train.index)\n",
    "\n",
    "    if p > 0:\n",
    "        best_solution = pd.MultiIndex.from_arrays(\n",
    "            [[row_index.get_level_values(level=level)[0]] if level != \"missing_percentage\" else [0]\n",
    "             for level in row_index.names], names=row_index.names)\n",
    "        best_solution = results.loc[best_solution].iloc[0]\n",
    "        y_train_total = pd.Series(best_solution[\"y_true\"], index=best_solution[\"y_pred_idx\"])\n",
    "        best_solution = pd.Series(best_solution[\"y_pred\"], index=best_solution[\"y_pred_idx\"])\n",
    "    else:\n",
    "        best_solution = None\n",
    "        y_train_total = None\n",
    "\n",
    "    dict_results = save_record(train_Xs=train_Xs, train_X=train_X, clusters=clusters, y=y_train, p=p, y_true_total=y_train_total,\n",
    "                               best_solution=best_solution, elapsed_time=elapsed_time, strat=strat,\n",
    "                               random_state=random_state, errors_dict=errors_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399f5b0-0a87-43b5-a3e5-ed889a11b75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_Xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa768e-afb9-4bce-ab54-4153b3f0442d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87ca60-bfc6-4c0d-9cf6-e1b42baf1111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.initialize_wh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54d908-f93f-479a-9379-83b8ce314ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.update_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7339d-456b-425b-a479-ff15c0ba0ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2baf9-1a51-4754-947e-151652f2ae87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce9066-04a3-4918-9ada-114769c85f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.select_dtypes(object).drop(columns= [\"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fda0f-66ad-4779-9f6f-c039aebdd1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4916b-eafd-45ec-ad10-00c528d3dafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fun(x: int):\n",
    "    \"4\" / 3\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1334c-de0a-42c6-a419-fb17337deccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fun()\n",
    "except Exception as exception:\n",
    "    print(type(exception).__name__, \":\", exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546920d8-3b0c-4475-b03a-6da27f6b601a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "defaultdict(defaultdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315124ea-e781-4315-9e11-549bbcc56b2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans, spectral_clustering\n",
    "from mvlearn.decomposition import AJIVE, GroupPCA\n",
    "from mvlearn.cluster import MultiviewSpectralClustering, MultiviewCoRegSpectralClustering\n",
    "from snf import compute\n",
    "from bignmf.models.jnmf.integrative import IntegrativeJnmf\n",
    "from bignmf.models.jnmf.standard import StandardJnmf\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.utils import DatasetUtils\n",
    "from imvc.transformers import MultiViewTransformer, ConcatenateViews\n",
    "from imvc.algorithms import NMFC\n",
    "\n",
    "from utils import save_record\n",
    "# from utils import GroupPCA\n",
    "\n",
    "folder_name = \"results\"\n",
    "filelame = \"complete_algorithms_evaluation.csv\"\n",
    "file_path = os.path.join(folder_name, filelame)\n",
    "\n",
    "random_state = 42\n",
    "START_BENCHMARKING = False\n",
    "\n",
    "datasets = [\"nutrimouse_genotype\", \"nutrimouse_diet\", \"bbcsport\", \"bdgp\", \"caltech101\", \"digits\", \"tcga_tissue\", \"tcga_survival\", \"nuswide\", \"metabric\"]\n",
    "probs = np.arange(100, step= 10)\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(10)\n",
    "algorithms = {\n",
    "    \"Concat\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                    StandardScaler().set_output(transform='pandas'),\n",
    "                                    KMeans()), \"params\": {}},\n",
    "    \"NMFC\": {\"alg\": make_pipeline(ConcatenateViews(),\n",
    "                                  MinMaxScaler().set_output(transform='pandas'),\n",
    "                                  NMFC().set_output(transform='pandas')), \"params\": {}},\n",
    "    \"MVSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                  MultiviewSpectralClustering()),\n",
    "                             \"params\": {}},\n",
    "    \"MVCoRegSpectralClustering\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\")),\n",
    "                                                       MultiviewCoRegSpectralClustering()),\n",
    "                                  \"params\": {}},\n",
    "    \"GroupPCA\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), GroupPCA(), StandardScaler(), KMeans()),\n",
    "                 \"params\": {}},\n",
    "    \"AJIVE\": {\"alg\": make_pipeline(MultiViewTransformer(StandardScaler()), AJIVE(), MultiViewTransformer(FunctionTransformer(pd.DataFrame)),\n",
    "                                   ConcatenateViews(), StandardScaler(), KMeans()),\n",
    "              \"params\": {}},\n",
    "    \"SNF\": {},\n",
    "    \"intNMF\": {},\n",
    "    \"jNMF\": {},\n",
    "}\n",
    "indexes_results = {\"dataset\": datasets, \"algorithm\": list(algorithms.keys()),\n",
    "                   \"missing_percentage\": probs, \"imputation\": imputation, \"run_n\": runs_per_alg}\n",
    "\n",
    "\n",
    "if START_BENCHMARKING:\n",
    "    results = pd.DataFrame(datasets, columns= [\"dataset\"])\n",
    "    for k,v in {k:v for k,v in indexes_results.items() if k != \"dataset\"}.items():\n",
    "        results = results.merge(pd.Series(v, name= k), how= \"cross\")\n",
    "    results = results.set_index(list(indexes_results.keys()))\n",
    "    results[[\"finished\", \"completed\"]] = False\n",
    "else:\n",
    "    results = pd.read_csv(file_path, index_col= list(indexes_results.keys()))\n",
    "    results_ = results.select_dtypes(object).drop(columns= \"comments\").replace(np.nan, \"np.nan\")\n",
    "    for col in results_.columns:\n",
    "        results[col] = results_[col].apply(eval)\n",
    "\n",
    "unfinished_results = results.loc[~results[\"finished\"]]\n",
    "\n",
    "for dataset_name in unfinished_results.index.get_level_values(\"dataset\").unique():\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name=dataset_name.split(\"_\")[0], return_y=True, shuffle= False)\n",
    "    y = pd.DataFrame(y)\n",
    "    for target in y.columns:\n",
    "        y_series = y[target].squeeze()\n",
    "        n_clusters = y_series.nunique()\n",
    "\n",
    "        for idx_iterator in unfinished_results.loc[unfinished_results.index.get_level_values(\"dataset\") == dataset_name].itertuples():\n",
    "            idx = idx_iterator[0]\n",
    "            row = results.loc[[idx]]\n",
    "            row_index = row.index\n",
    "            print(row.drop(columns= row.columns).reset_index().to_dict(orient=\"records\")[0])\n",
    "            alg_name, impute, p, run_n = (\n",
    "                row_index.get_level_values(\"algorithm\")[0],\n",
    "                row_index.get_level_values(\"imputation\")[0],\n",
    "                row_index.get_level_values(\"missing_percentage\")[0]/100,\n",
    "                row_index.get_level_values(\"run_n\")[0])\n",
    "\n",
    "            alg = algorithms[alg_name]\n",
    "            train_Xs = DatasetUtils.shuffle_imvd(Xs=Xs, random_state= random_state + run_n)\n",
    "            y_train = y_series.loc[train_Xs[0].index]\n",
    "            errors_dict = defaultdict(int)\n",
    "            if p != 0:\n",
    "                # if n_clusters > len(train_Xs[0])*p:\n",
    "                #     continue\n",
    "                try:\n",
    "                    train_Xs = DatasetUtils.add_random_noise_to_views(Xs=train_Xs, p= round(p, 2),\n",
    "                                                                      random_state =random_state + run_n, \n",
    "                                                                      assess_percentage = True, stratify = y_train)\n",
    "                except Exception as exception:\n",
    "                    errors_dict[type(exception).__name__] += 1\n",
    "                    print(errors_dict)\n",
    "                    results.loc[idx, \"finished\"] = True\n",
    "                    results.to_csv(file_path)\n",
    "                    continue\n",
    "\n",
    "            if impute:\n",
    "                train_Xs = MultiViewTransformer(SimpleImputer(strategy=\"mean\").set_output(transform= \"pandas\")).fit_transform(train_Xs)\n",
    "            else:\n",
    "                train_Xs = DatasetUtils.select_complete_samples(Xs = train_Xs)\n",
    "                y_train = y_train.loc[train_Xs[0].index]\n",
    "\n",
    "            try:\n",
    "                start_time = time.perf_counter()\n",
    "                if alg_name == \"SNF\":\n",
    "                    preprocessing_step = MultiViewTransformer(StandardScaler().set_output(transform= \"pandas\"))\n",
    "                    train_Xs = preprocessing_step.fit_transform(train_Xs)\n",
    "                    affinities = compute.make_affinity(train_Xs, normalize= False)\n",
    "                    fused = compute.snf(affinities)\n",
    "                    clusters = spectral_clustering(fused, n_clusters=n_clusters, random_state=random_state + run_n)\n",
    "                elif alg_name == \"intNMF\":\n",
    "                    preprocessing_step = MultiViewTransformer(MinMaxScaler().set_output(transform= \"pandas\"))\n",
    "                    model = IntegrativeJnmf({k:v for k,v in enumerate(train_Xs)}, k= n_clusters, lamb = 0.1)\n",
    "                    raise\n",
    "                    model.run(trials = 50, iterations = 100, verbose=False)\n",
    "                    model.cluster_data()\n",
    "                    clusters = np.argmax(model.w_cluster, axis= 1)\n",
    "                elif alg_name == \"jNMF\":\n",
    "                    pipeline = make_pipeline(MultiViewTransformer(MinMaxScaler().set_output(transform= \"pandas\")))\n",
    "                    model = StandardJnmf({k:v for k,v in enumerate(train_Xs)}, k= n_clusters)\n",
    "                    model.run(trials = 50, iterations = 100, verbose=False)\n",
    "                    model.cluster_data()\n",
    "                    clusters = np.argmax(model.w_cluster, axis= 1)\n",
    "                else:\n",
    "                    model, params = alg[\"alg\"], alg[\"params\"]\n",
    "                    if alg_name == \"GroupPCA\":\n",
    "                        model[1].set_params(n_components=n_clusters, random_state=random_state + run_n, multiview_output=False)\n",
    "                    elif alg_name == \"AJIVE\":\n",
    "                        model[1].set_params(random_state=random_state + run_n)\n",
    "                    if alg_name == \"NMFC\":\n",
    "                        model[-1].set_params(n_components=n_clusters, random_state=random_state + run_n)\n",
    "                    else:\n",
    "                        model[-1].set_params(n_clusters=n_clusters, random_state=random_state + run_n)\n",
    "                    clusters = model.fit_predict(train_Xs)\n",
    "            except ValueError as exception:\n",
    "                if alg_name == \"AJIVE\" and len(y_train) < 5:\n",
    "                    errors_dict[type(exception).__name__] += 1\n",
    "                    print(errors_dict)\n",
    "                    results.loc[idx, \"finished\"] = True\n",
    "                    results.to_csv(file_path)\n",
    "                    continue\n",
    "                if alg_name == \"SNF\" and len(y_train) < 17:\n",
    "                    errors_dict[type(exception).__name__] += 1\n",
    "                    print(errors_dict)\n",
    "                    results.loc[idx, \"finished\"] = True\n",
    "                    results.to_csv(file_path)\n",
    "                    continue\n",
    "                if alg_name == \"intNMF\" and len(y_train) < 5:\n",
    "                    errors_dict[type(exception).__name__] += 1\n",
    "                    print(errors_dict)\n",
    "                    results.loc[idx, \"finished\"] = True\n",
    "                    results.to_csv(file_path)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            clusters = pd.Series(clusters, index= y_train.index)\n",
    "\n",
    "            elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "            if alg_name in [\"NMFC\"]:\n",
    "                train_X = model.transform(train_Xs)\n",
    "            elif alg_name in [\"SNF\"]:\n",
    "                train_X = preprocessing_step.transform(train_Xs)\n",
    "            elif alg_name in [\"intNMF\", \"jNMF\"]:\n",
    "                train_X = model.w\n",
    "            else:\n",
    "                train_X = model[:-1].transform(train_Xs)\n",
    "            if isinstance(train_X, list):\n",
    "                train_X = ConcatenateViews().fit_transform(train_X)\n",
    "            if not isinstance(train_X, pd.DataFrame):\n",
    "                train_X = pd.DataFrame(train_X, index= y_train.index)\n",
    "                \n",
    "            assert train_X.index.equals(y_train.index)\n",
    "            assert train_Xs[0].index.equals(y_train.index)\n",
    "\n",
    "            if p > 0:\n",
    "                best_solution = pd.MultiIndex.from_arrays([[row_index.get_level_values(level= level)[0]] if level != \"missing_percentage\" else [0]\n",
    "                                                           for level in row_index.names], names= row_index.names)\n",
    "                best_solution = results.loc[best_solution].iloc[0]\n",
    "                best_solution = pd.Series(best_solution[\"y_pred\"], index= best_solution[\"y_pred_idx\"])\n",
    "                best_solution = best_solution.loc[train_X.index]\n",
    "            else:\n",
    "                best_solution = None\n",
    "\n",
    "            dict_results = save_record(train_Xs=train_Xs, train_X=train_X, clusters=clusters, y=y_train, p= p,\n",
    "                              best_solution = best_solution, elapsed_time=elapsed_time,\n",
    "                              random_state=random_state, errors_dict=errors_dict)\n",
    "            dict_results = pd.DataFrame(pd.Series(dict_results), columns= row_index).T\n",
    "            results.loc[[idx], dict_results.columns] = dict_results\n",
    "            results.loc[idx, \"finished\"] = True\n",
    "            results.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aad2d-963c-42f8-87e5-32262db2cc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06d8c3-bcdc-466e-b6b0-5f8994392c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.rand(list(model.x.values())[0].shape[0], model.k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dbed8-c780-49cb-9db9-1b0118b7203a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self.v = {}\n",
    "self.h = {}\n",
    "for key in self.x:\n",
    "    self.h[key] = np.random.rand(self.k, self.x[key].shape[1])\n",
    "    self.v[key] = np.random.rand(number_of_samples, self.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f87c27-4496-47b0-9c23-c8787ff7bff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.silhouette_score(train_X, clusters, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b0bf1-9e90-4382-b083-b97727a04aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    try:\n",
    "        model.run(trials = 50, iterations = 100, verbose=0)\n",
    "        # clusters = model.fit_predict(train_Xs)\n",
    "        print(i)\n",
    "    except Exception as ex:\n",
    "        print(i, ex)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99616f4f-b844-4f11-80be-7848cf692894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[results[\"finished\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07b0ac-5137-4100-ba85-59ab9c7d1b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_solution.loc[train_X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fce7a-d10d-4a29-aa9a-ccb1a906d7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv(file_path, index_col= list(indexes_results.keys()))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba40a8d-6eb3-4b61-87bc-c42399cf6ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[\"label_sizes\"].apply(str).str.replace(\"nan\", \"np.nan\").apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9065d1-e624-413e-a468-ae30311d9d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = a.select_dtypes(object).replace(np.nan, \"np.nan\").drop(columns= \"comments\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f329524-f844-4980-b49a-c8c0916627b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in b.columns:\n",
    "    print(i)\n",
    "    b[i].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a007f6c-58dd-4eec-804e-8c43fa0dbb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.apply(eval, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02da898-ce78-45cc-b096-945a0c7f270e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(datasets, columns= [\"dataset\"])\n",
    "for k,v in {k:v for k,v in indexes_results.items() if k != \"dataset\"}.items():\n",
    "    a = a.merge(pd.Series(v, name= k), how= \"cross\")\n",
    "a = a.set_index(list(indexes_results.keys()))\n",
    "a[\"finished\"] = False\n",
    "for i in a.itertuples():\n",
    "    print(len(i))\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f7de4-c34c-4dda-8965-d6d2b025782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in unfinished_results.iterrows():\n",
    "    print(len(i))\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf794a-e432-4563-a6a7-516983310f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(best_solution.loc[train_X.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a282ec7-3caf-458e-9611-ef70839e349c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bb522-ad3a-4225-8130-61915f8a81c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from imvc.datasets import LoadDataset\n",
    "from imvc.utils import DatasetUtils\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4a119-be24-4784-af50-3ad95e265dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs = DatasetUtils.add_random_noise_to_views(Xs=Xs, p= 0.95, assess_percentage= True, random_state= 42, stratify=y.iloc[:, 1])\n",
    "Xs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a341a3b-04a7-432e-a26c-eb9b4b1750fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = [\"nutrimouse\", \"bbcsport\", \"bdgp\", \"caltech101\", \"digits\", \"tcga\", \"nuswide\", \"metabric\"]\n",
    "probs = np.arange(0., 1., step= 0.1).round(1) * 100\n",
    "imputation = [True, False]\n",
    "runs_per_alg = np.arange(10)\n",
    "\n",
    "results = pd.DataFrame(datasets, columns= [\"dataset\"]).merge(\n",
    "    pd.Series(probs, name= \"missing_percentage\"), how= \"cross\").merge(\n",
    "    pd.Series(imputation, name= \"imputation\"), how= \"cross\").merge(\n",
    "    pd.Series(runs_per_alg, name= \"run_n\"), how= \"cross\")\n",
    "results = results.set_index(['dataset', 'missing_percentage',\"imputation\", 'run_n'])\n",
    "results[\"finished\"] = False\n",
    "results.to_csv(\"pr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b3af9-d507-4a19-a7f8-ffe5de6062e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"pr.csv\", index_col= ['dataset', 'missing_percentage',\"imputation\", 'run_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba373c-9a81-4c16-89ac-9a668865f85c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c644b-4520-441c-86e8-d8753e22cb0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2dcf3-8b7c-41ea-b85d-55270b34c233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DatasetUtils.convert_mvd_into_imvd(Xs, p= 0.1, assess_percentage= True, random_state= random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72887a96-8cfa-4d8d-a03a-87fa747cf713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_into_groups(num_elements, num_groups):\n",
    "    # Calculate the base number of elements per group\n",
    "    base_count = num_elements // num_groups\n",
    "    remaining = num_elements % num_groups\n",
    "    \n",
    "    # Initialize the list to store the number of elements in each group\n",
    "    groups_count = [base_count] * num_groups\n",
    "    \n",
    "    # Distribute the remaining elements equally among the groups\n",
    "    for i in range(remaining):\n",
    "        groups_count[i] += 1\n",
    "    \n",
    "    return groups_count\n",
    "\n",
    "# Total number of elements\n",
    "total_elements = 103\n",
    "# Number of groups to split into\n",
    "num_of_groups = 4\n",
    "\n",
    "# Splitting elements into groups\n",
    "result = split_into_groups(total_elements, num_of_groups)\n",
    "\n",
    "# Displaying the number of elements in each group\n",
    "for i, count in enumerate(result):\n",
    "    print(f\"Group {i + 1}: {count} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb9198-3e8c-487c-808d-018b84f56754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "103 % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0357a0b-14ea-4794-b9a0-1eae31b046ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_count = 103 // 4\n",
    "remaining = 103 % 4\n",
    "\n",
    "# Initialize the list to store the number of elements in each group\n",
    "groups_count = [base_count] * num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df5032-fcc0-4db9-beb9-7735f147f5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset_name in [\n",
    "    # \"nutrimouse\",\n",
    "    \"tcga\"\n",
    "]:\n",
    "    Xs, y = LoadDataset.load_dataset(dataset_name = dataset_name, return_y = True, p= 0.8, assess_percentage=True)\n",
    "    # Xs = GetCompleteSamples().fit_transform(Xs)\n",
    "    # aaaaaaaaaaaaaa\n",
    "    # y.to_csv(os.path.join(\"imvc/datasets/data/\", dataset_name, f\"{dataset_name}_y.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c59f3-100e-4c50-a06b-654377a74eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "survs = []\n",
    "for dataset_name in os.listdir(\"imvc/datasets/data/tcga/original/\"):\n",
    "    if \"survival\" in dataset_name:\n",
    "        print(dataset_name)\n",
    "        surv = pd.read_csv(os.path.join(\"imvc/datasets/data/tcga/original/\", dataset_name), sep= '\"\"', index_col= 0, engine='python')\n",
    "        if surv.shape[1] <2:\n",
    "            surv = pd.read_csv(os.path.join(\"imvc/datasets/data/tcga/original/\", \"survival_lung.csv\"), sep= \"\\t\", index_col= 0)\n",
    "        surv.index = surv.index.str.replace(\".\", \"-\").str.upper()\n",
    "        surv.index = surv.index.str.extract(pat=r\"(TCGA.{8})\", expand=False)\n",
    "        surv.index.name = None\n",
    "        survs.append(surv[[\"Survival\", \"Death\"]])\n",
    "survs = pd.concat(survs).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0ece1-0d40-4116-8e7a-8ddbb44e21bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.index.intersection(survs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cce785-b1a5-4444-8d83-83facc6084c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_,met = LoadDataset.load_dataset(dataset_name = \"tcga\", return_metadata= True, p= 0.8, assess_percentage=True)\n",
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ac9f3-9e7e-4511-bc12-66f62561e5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y[y == 0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cde78-20a9-435e-959d-738ea8160013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern = r\"(TCGA.{8})\"\n",
    "matches = surv.index.str.extract(pat=pattern, expand=False)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dad0f6-6944-44c7-b6f1-49f8e1d76586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset_name in [i for i in sorted(os.listdir(\"imvc/datasets/data/\")[1:]) if i != \"metabric\"]:\n",
    "    Xs = LoadDataset.load_dataset(dataset_name = dataset_name, return_y = False)\n",
    "    for p in np.arange(0., 1., step= 0.1).round(1):\n",
    "        imvd = DatasetUtils.convert_mvd_into_imvd(Xs, p= p, random_state = 42, assess_percentage = True)\n",
    "        complete_Xs = GetCompleteSamples().fit_transform(imvd)\n",
    "        print(dataset_name, \"\\t\", p, \"\\t\", len(Xs), \"\\t\", len(DatasetUtils.get_sample_names(imvd)), \"\\t\", len(DatasetUtils.get_sample_names(complete_Xs)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac67995-e7d0-46e6-984a-e685e57dbbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = np.arange(0., 1., step= 0.1).round(1)\n",
    "algorithms = [\"Concat\", \"MOFA\", \"NMFC\", \"MONET\", \"MSNE\", \"SUMO\", \"NEMO\"]\n",
    "runs_per_alg = np.arange(10).tolist()\n",
    "results = pd.DataFrame(os.listdir(\"imvc/datasets/data/\"), columns= [\"dataset\"]).merge(\n",
    "    pd.Series(algorithms, name= \"algorithm\"), how= \"cross\").merge(\n",
    "    pd.Series(probs, name= \"missing_percentage\"), how= \"cross\").merge(\n",
    "    pd.Series(runs_per_alg, name= \"run_n\"), how= \"cross\")\n",
    "results = results.set_index(['dataset', 'algorithm', 'missing_percentage', 'run_n'])\n",
    "results[\"finished\"] = False\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc5692-a912-4e91-85d6-f07c2537f780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.index.get_level_values(\"dataset\") == \".ipynb_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ddef3-94e1-4f27-94bb-6f20c97a7803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in a:\n",
    "    print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d35ca-f365-447a-934f-cf298b962466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc7d5b-30e7-4f03-abcd-a4063499743e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GetCompleteSamples().fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e17408-7168-45e8-a362-cb262c1192aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "metadata = {\"modality\": {0: \"gene\", 1: \"lipid\"}, \"labels\": {\"genotype\": pd.Series(a[\"genotype\"].unique()).to_dict(), \"diet\": pd.Series(a[\"diet\"].unique()).to_dict()}}\n",
    "\n",
    "with open(os.path.join(\"imvc/datasets/data/nutrimouse\", 'metadata.json'), 'w') as fp:\n",
    "    json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313007c2-1662-4397-9218-f6688383f325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = np.arange(0., 1., step= 0.1).round(1)\n",
    "runs_per_alg = np.arange(10).tolist()\n",
    "pd.DataFrame(probs).merge(pd.DataFrame(runs_per_alg), how= \"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea88c5-641c-439c-8257-1a427f4d574d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame([\"alg\", \"pad\"])\n",
    "b = pd.DataFrame([\"alg1\", \"pad2\"])\n",
    "a.merge(b, how= \"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68327e9-ab81-4dd7-bd48-496655af9430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "metadata = {\"modality\": {0: \"morphological features\", 1: \"Karhunen-Love coefficients\", 2: \"profile correlations\", 3: \"Zernike moments\", 4: \"Fourier coefficients of the character shapes\", 5: \"pixel averages of the images from 2x3 windows\"}}\n",
    "import json\n",
    "\n",
    "with open(os.path.join(\"imvc/datasets/data/digits\", 'metadata.json'), 'w') as fp:\n",
    "    json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf57b4-f042-498f-a25d-0d023c9cf9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('imvc/datasets/data/caltech101/Caltech101-all.mat')\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c66242-c9b9-465f-aef7-d0caf5fc9bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('imvc/datasets/data/caltech101/Caltech101-all.mat')\n",
    "for i,x in enumerate(mat[\"X\"][0]):\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/caltech101/caltech101_{i}.csv', index= False)\n",
    "pd.DataFrame(mat[\"Y\"]).to_csv(f'imvc/datasets/data/caltech101/caltech101_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b89bc-1500-4143-8c30-0009f7dbc03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"imvc/datasets/data/tcga/PanCan.miRNAseq.RPM.215-MIMATs-most-variant-25pc.4229-samples.NMF-input.BCGSC.20140603.csv\", index_col= 0)\n",
    "x.index.name= None\n",
    "x.columns = x.columns.to_series().apply(lambda x: x.split(\"_\")[1]).str[:12]\n",
    "x = x.T\n",
    "x = x[~x.index.duplicated(keep='first')]\n",
    "print(x.shape)\n",
    "x.to_csv(os.path.join(path, \"tcga_0.csv\"))\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce4184-4d41-4fa7-9a12-4f6fe8ced380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"imvc/datasets/data/tcga/PanCan12.3602-corrected-v3.txt\", sep= \"\\t\", index_col= 0, header= [0,1])\n",
    "x.index.name= None\n",
    "x.columns = x.columns.droplevel(0).str[:12]\n",
    "x = x.T\n",
    "x = x[~x.index.duplicated(keep='first')]\n",
    "print(x.shape)\n",
    "x.to_csv(os.path.join(path, \"tcga_1.csv\"))\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba8199-3c46-4aeb-a515-c4a1ebe85af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "metadata = {\"modality\": {0: \"visual\", 1: \"mRNA\", 2: \"text\", 3: \"methyl\"}, \"labels\": classes}\n",
    "import json\n",
    "\n",
    "with open(os.path.join(path, 'metadata.json'), 'w') as fp:\n",
    "    json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f7401-b36d-4cd2-b949-0847c54db423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"imvc/datasets/data/tcga/PanCan11_RBN_RPPA_without_Duplicates_20130325.csv\", index_col= 0)\n",
    "x.index.name= None\n",
    "x = x[x.columns[5:]]\n",
    "x = x[~x.index.duplicated(keep='first')]\n",
    "print(x.shape)\n",
    "x.to_csv(os.path.join(path, \"tcga_2.csv\"))\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60c039-bf61-49dc-82c3-f5aaafb6226d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"imvc/datasets/data/tcga/DNAmethylationClusteringMatrix.csv\", index_col= 0)\n",
    "x.columns = x.columns.str[:12]\n",
    "x = x.T\n",
    "x = x[~x.index.duplicated(keep='first')]\n",
    "print(x.shape)\n",
    "x.to_csv(os.path.join(path, \"tcga_3.csv\"))\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403c144-5c85-40e4-897c-07da8fddcadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = pd.read_csv(os.path.join(path, f\"tcga_y.csv\"), index_col= 0)\n",
    "classes = pd.Series(b.iloc[:, 0].unique()).to_dict()\n",
    "b.iloc[:, 0].replace({v:k for k,v in classes.items()}).to_csv(os.path.join(path, \"tcga_y.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7b08a-f8a5-456b-8e22-08992eaf804f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd080b-f7eb-425b-85aa-8e86695f3c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "a = pd.read_csv(os.path.join(path, f\"tcga_{i}.csv\"), index_col= 0).index\n",
    "print(len(a))\n",
    "for i in range(4):\n",
    "    a = a.intersection(pd.read_csv(os.path.join(path, f\"tcga_{i}.csv\"), index_col= 0).index)\n",
    "    print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1289e7-6f55-4711-aebe-dacedff0116b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    b = pd.read_csv(os.path.join(path, f\"tcga_{i}.csv\"), index_col= 0)\n",
    "    aa = a.intersection(b.index)\n",
    "    print(i, \"\\t\", b.shape, \"\\t\", b.drop_duplicates().shape, \"\\t\", b.loc[aa].shape, \"\\t\", b.loc[aa].drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac7683-9f9b-42fd-9bad-e537e56f6969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = b.loc[\"TCGA-13-0791\"]\n",
    "c[c.columns[c.iloc[0] != c.iloc[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7b617-55bd-48ea-abcb-0ab950b14b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.loc[a].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c78aeb-47b2-45f2-a5a3-656ddab94d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.loc[a].index.difference(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a67a2-27bd-4203-a394-5c5c9df8f344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.drop_duplicates().loc[a.drop_duplicates()].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c32ac-d783-4eeb-918b-a57b27bb6153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.index.intersection(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42004a47-56ac-4e52-8f00-574eac035648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a.intersection(b.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1c8f4-8c06-4ca2-95bc-469a2751567b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"imvc/datasets/data/tcga/mrna_y.csv\", index_col= 0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc643b-8c86-482f-935d-09788a3f58ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = pd.read_csv(\"imvc/datasets/data/tcga/mrna_y.csv\", index_col =0)\n",
    "x2 = pd.read_csv(\"imvc/datasets/data/tcga/mirna_y.csv\", index_col =0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e204fb-eebc-4f07-8bc1-3401822b5b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69c77d-d9dd-404a-aca2-781e43efa19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ad9c5-2f85-4424-8f80-676fa09a492f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1.index.intersection(x2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c831f4-439e-4510-b179-e28897928ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.T.to_csv(os.path.join(path, \"mrna.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ba525-192b-40c5-88b9-622f6dc0cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns.droplevel(1).to_series().to_csv(os.path.join(path, \"mirna_y.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b48d9a-10e4-4392-89f0-eb6b67a51a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = x.columns.to_frame().set_index(1)\n",
    "# a.columns = [''] * len(a.columns)\n",
    "a.index.name= None\n",
    "a.index = a.index.str[:12]\n",
    "a.to_csv(os.path.join(path, \"mrna_y.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6eba92-57ac-4fcc-92f8-4f5db4a9ebb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"imvc/datasets/data/tcga/PanCan.miRNAseq.RPM.215-MIMATs-most-variant-25pc.4229-samples.NMF-input.BCGSC.20140603.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164af31-fb27-4a9d-a190-343b4fea1ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"imvc/datasets/data/tcga/PanCan.miRNAseq.RPM.215-MIMATs-most-variant-25pc.4229-samples.NMF-input.BCGSC.20140603.csv\").columns.to_series().apply(lambda x: x.split(\"_\")[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359b8ba-995a-4600-b5f6-9c4098f861e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate([\"X\", \"Ya\"]):\n",
    "    x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/{name}.mat')[name]\n",
    "    print(x.shape)\n",
    "    pd.DataFrame(x).to_csv(f'imvc/datasets/data/bdgp/bdgp_{i}.csv', index= False)\n",
    "x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/Yc.mat')[\"Yc\"]\n",
    "pd.DataFrame(x.argmax(1)).to_csv(f'imvc/datasets/data/bdgp/bdgp_y.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c82d6-e3b7-4f35-8380-fedca782ef31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/tcga\"\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "for i,x in enumerate([\"exp\", \"methy\", \"mirna\"]):\n",
    "    target = []\n",
    "    files_x = [os.path.join(path, file) for file in files if file.startswith(x)]\n",
    "    ds = []\n",
    "    for file_x in files_x:\n",
    "        d_x = pd.read_csv(file_x, index_col= 0).T\n",
    "        print(file_x, d_x.shape)\n",
    "        target.extend([file_x.split(\"_\")[-1]]* d_x.shape[0])\n",
    "        ds.append(d_x)\n",
    "    d = pd.concat(ds)\n",
    "    print(x, d.shape)\n",
    "    d = d.dropna(axis= 1)\n",
    "    print(x, d.shape)\n",
    "    d.to_csv(os.path.join(path, f'tcga_{i}.csv'))\n",
    "pd.Series(target).to_csv(os.path.join(path, 'tcga_y.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1fa46-5710-40f3-a6a4-13c7f9c96006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/tcga\"\n",
    "files = [os.path.join(path, f) for f in os.listdir(path) if f.startswith(\"tcga_\")]\n",
    "d = pd.concat([pd.read_csv(file) for file in files], axis= 1)\n",
    "for i,file in enumerate(files):\n",
    "    d_x = pd.read_csv(file)\n",
    "    print(file, d_x.shape, d_x.loc[d.index])\n",
    "    d_x.loc[d.index].to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7d760-b3cb-43f4-afa4-16f588a1a44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/tcga\"\n",
    "for i,x in enumerate([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]):\n",
    "    with ZipFile(os.path.join(path, x)) as zf:\n",
    "        for file in zf.namelist():\n",
    "            with zf.open(file) as f2:\n",
    "                d = pd.read_csv(f2, sep= \" \")\n",
    "                print(file, d.shape)\n",
    "                d.to_csv(f\"{os.path.join(path, file)}_{x.split('.')[0]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7b2c8-bc2b-4237-8b04-967f054a8cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"imvc/datasets/data/digits\"\n",
    "for i,x in enumerate([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]):\n",
    "    d = pd.read_csv(os.path.join(path, x))\n",
    "    print(d.shape)\n",
    "    d.iloc[:, :-1].to_csv(os.path.join(path, f\"digits_{i}.csv\"), index= False)\n",
    "d.iloc[:, -1].to_csv(os.path.join(path, f\"digits_y.csv\"), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff058a6d-ddb2-4499-8c57-6d6b19b99b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = scipy.io.loadmat(f'imvc/datasets/data/bdgp/Yc.mat')[\"Yc\"]\n",
    "x.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53749fa-f7af-4e14-b192-a326e883c5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afef5b-8792-4244-9e96-fa35b00079c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6a5d2-ba01-4d6f-a4fb-7cf5d097fce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"X\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cccf4b-89df-4d0c-a95c-be3977097bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"X\"][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1306beb-4250-49c7-bb64-7986e6c238d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abfe0c-ebd8-43b8-ba13-7fc7f4c21ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = pyreadr.read_r('imvc/datasets/data/metabric/METABRIC_discovery')\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5aeed-9351-4151-b3f2-e9d344c9f43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"mydatCNV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556223fb-cd7d-4a21-b6e0-1bbe3a119356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea130b-e706-4025-afd0-a5e827aa1afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mat[\"Y\"]).squeeze().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9954cb-dc76-45d9-8ae7-219cffe4d972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('imvc/datasets/data/bdgp/X.mat')\n",
    "mat[\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d74d47-3223-4d7e-aaeb-a2de0873505c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('imvc/datasets/data/bdgp/Yc.mat')\n",
    "mat[\"Yc\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22dba5-2b9a-46cf-a9c2-37cf78244a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat[\"Yc\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
