{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Impute incomplete modality- and feature-wise multi-modal data\n\nWhen the learning algorithms cannot directly handle missing data, imputation methods become essential to allow\ntheir application. Thus, `iMML` has a module designed for filling missing data, which can be particularly useful\nwhen using external methods that are unable to handle missing values directly.\n\nIn this tutorial, we will explore how to use `iMML` to impute an incomplete multi-modal dataset and how to\nbenchmark imputation quality against a simple baseline.\n\nWhat you will learn:\n\n- How to represent your dataset as Xs (a list of per\u2011modality matrices).\n- How to simulate block\u2011wise and feature\u2011wise missingness with ``Amputer`` and simple masks.\n- How to build an imputation pipeline with StandardScaler + ``MOFAImputer``.\n- How to compare ``MOFAImputer`` to a baseline mean imputer using Mean Absolute Error (MAE).\n- How to visualize missingness before and after imputation.\n\nThis tutorial is fully reproducible and uses a small synthetic dataset. You can easily\nreplace the data\u2011loading section with your own data following the same structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 3\n\n# License: BSD 3-Clause License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import required libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np\nimport pandas as pd\n\nfrom imml.impute import MOFAImputer\nfrom imml.preprocessing import MultiModTransformer\nfrom imml.ampute import Amputer\nfrom imml.visualize import plot_missing_modality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load the dataset\nFor reproducibility, we generate a small synthetic classification dataset and split the features into two\nmodalities (Xs[0], Xs[1]).\n\nUsing your own data:\n\n- Represent your dataset as a Python list Xs, one entry per modality.\n- Each Xs[i] should be a 2D array-like (pandas DataFrame or NumPy array) of shape (n_samples, n_features_i).\n- All modalities must refer to the same samples and be aligned by row.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_state = 42\nX, y = make_classification(n_samples=50, random_state=random_state, n_clusters_per_class=1, n_classes=3)\nX, y = pd.DataFrame(X), pd.Series(y)\nX.columns = X.columns.astype(str)\n# Two modalities: first 10 features and last 10 features\nXs = [X.iloc[:, :10], X.iloc[:, 10:]]\nnames= [\"Modality A\", \"Modality B\"]\nprint(\"Samples:\", len(Xs[0]), \"\\t\", \"Modalities:\", len(Xs), \"\\t\", \"Features:\", [X.shape[1] for X in Xs])\nn_clusters = len(np.unique(y))\ny.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Impute missing data\nWe build an imputation pipeline with two stages:\n1) Standardize features per modality (helps MOFA training and makes features comparable).\n2) Impute missing modalities with ``MOFAImputer``, which learns shared latent factors across modalities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "amputed_Xs = Amputer(p= 0.3, mechanism=\"mcar\", random_state=random_state).fit_transform(Xs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe how missing modalities look:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_missing_modality(Xs=amputed_Xs, sort=False)\n\nn_components = 4\npipeline = make_pipeline(\n    MultiModTransformer(StandardScaler().set_output(transform=\"pandas\")),\n    MOFAImputer(n_components=n_components, random_state=random_state)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe how all modalities are now filled:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "imputed_Xs = pipeline.fit_transform(amputed_Xs)\n_ = plot_missing_modality(Xs=imputed_Xs, sort=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Benchmark imputation accuracy\nWe now compare ``MOFAImputer`` with a simple baseline (feature\u2011wise mean imputation).\nDesign:\n\n- We introduce both modality\u2011wise (block) and feature\u2011wise missingness.\n- For each missingness rate p, we repeat the procedure 5 times with different seeds.\n- We report Mean Absolute Error (MAE) only on entries that were truly missing.\n- For ``MOFAImputer``, we standardize before fitting and then invert the scaling to compute MAE in the original space.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ps = np.arange(0.1, 0.8, 0.2)\nn_times = 5\nmethods = [\"MOFAImputer\", \"MeanImputer\"]\nall_metrics = []\n\nfor algorithm in methods:\n    for p in ps:\n        missing_percentage = int(p*100)\n        for i in range(n_times):\n            ampute = True\n            while ampute: # avoid those iterations where a sample has no available data\n                amputed_Xs = Amputer(p=p, random_state=i).fit_transform(Xs)\n                for X in amputed_Xs:\n                    mask = np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)\n                    X.iloc[mask] = np.nan\n                if pd.concat(amputed_Xs, axis=1).isna().all(axis=1).any():\n                    i += n_times\n                else:\n                    ampute = False\n            if algorithm == \"MeanImputer\":\n                pipeline = make_pipeline(\n                    MultiModTransformer(SimpleImputer().set_output(transform=\"pandas\"))\n                )\n            else:\n                normalizer = StandardScaler()\n                pipeline = make_pipeline(\n                    MultiModTransformer(StandardScaler().set_output(transform=\"pandas\")),\n                    MOFAImputer(n_components = n_components, random_state=i))\n            masks = [np.isnan(amputed_X) for amputed_X in amputed_Xs]\n            imputed_Xs = pipeline.fit_transform(amputed_Xs)\n            transformer_list = pipeline[0].transformer_list_\n            if algorithm != \"MeanImputer\":\n                imputed_Xs = [pd.DataFrame(transformer.inverse_transform(X), index=X.index, columns=X.columns)\n                              for X, transformer in zip(imputed_Xs, transformer_list)]\n            metric = np.mean([mean_absolute_error(transformed_X.values[mask], imputed_X.values[mask])\n                              for transformed_X,imputed_X,mask in zip(Xs, imputed_Xs, masks)])\n            result = {\n                \"Method\": algorithm,\n                'Missing rate (%)': int(p*100),\n                \"Iteration\": i,\n                \"Mean Absolute Error\": metric,\n            }\n            all_metrics.append(result)\n\ndf = pd.DataFrame(all_metrics)\ndf = df.sort_values([\"Method\", \"Missing rate (%)\", \"Iteration\"], ascending=[True, True, True])\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now visualize the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = df.groupby([\"Method\", \"Missing rate (%)\"])[\"Mean Absolute Error\"]\nstats = g.agg(mean=\"mean\", sem=lambda x: x.std(ddof=1) / np.sqrt(len(x))).reset_index()\nmean_wide = stats.pivot(index=\"Missing rate (%)\", columns=\"Method\", values=\"mean\")\nsem_wide  = stats.pivot(index=\"Missing rate (%)\", columns=\"Method\", values=\"sem\")\nax = mean_wide.plot(yerr=sem_wide, marker=\"o\", capsize=3, ylabel=\"Mean Absolute Error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of results\nAcross runs and missingness levels, ``MOFAImputer`` generally achieves lower MAE than the mean\u2011imputation baseline\nat low\u2011to\u2011moderate missing rates, reflecting its ability to infer shared latent structure across modalities.\nAs the missing rate becomes very high, both methods degrade and the gap narrows because little signal remains.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nMany multi\u2011modal learning algorithms expect fully observed inputs, making imputation a practical necessity in\nreal\u2011world workflows. ``MOFAImputer`` offers a principled, cross\u2011modal approach that tends to outperform simple\nbaselines when missingness is not extreme. Thus, `\u00ecMML` can be used for applying less robuts algorithms to\nreal-world applications.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}