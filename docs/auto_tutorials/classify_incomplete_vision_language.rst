
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/classify_incomplete_vision_language.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_classify_incomplete_vision_language.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_classify_incomplete_vision_language.py:


===========================================================================================================
Classify an incomplete vision–language dataset (Oxford‑IIIT Pets) with deep learning
===========================================================================================================

This tutorial demonstrates how to classify samples from an incomplete vision–language dataset using the `iMML`
library. `iMML` supports robust classification even when some modalities (e.g., text or image) are missing, making it
suitable for real‑world multi‑modal data where missingness is common.

We will use the ``RAGPT`` algorithm from the `iMML` classify module on the `Oxford‑IIIT Pets
<https://huggingface.co/datasets/visual-layer/oxford-iiit-pet-vl-enriched?library=datasets>`__  dataset
and evaluate its performance.

What you will learn:

- How to load a public vision–language dataset
  (`Oxford‑IIIT Pets <https://huggingface.co/datasets/visual-layer/oxford-iiit-pet-vl-enriched?library=datasets>`_
  via `Hugging Face Datasets <https://huggingface.co/datasets>`__).
- How to adapt this workflow to your own vision–language data.
- How to build a retrieval‑augmented memory bank and prompts with ``MCR``.
- How to train the ``RAGPT`` classifier when image or text may be missing.
- How to track metrics during training and evaluate with MCC and a confusion matrix.

.. GENERATED FROM PYTHON SOURCE LINES 24-29

.. code-block:: Python


    # sphinx_gallery_thumbnail_number = 1

    # License: BSD 3-Clause License








.. GENERATED FROM PYTHON SOURCE LINES 30-36

Step 0: Prerequisites
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To run this tutorial, install the extras for deep learning and tutorials:
  pip install imml[deep]
We also use the Hugging Face Datasets library to load Oxford‑IIIT Pets:
  pip install datasets

.. GENERATED FROM PYTHON SOURCE LINES 39-41

Step 1: Import required libraries
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 41-61

.. code-block:: Python


    import shutil
    from PIL import Image
    from lightning import Trainer
    import lightning as L
    from matplotlib import pyplot as plt
    from sklearn.model_selection import train_test_split
    from torch.utils.data import DataLoader
    import torch
    import os
    import pandas as pd
    from sklearn.metrics import matthews_corrcoef, ConfusionMatrixDisplay
    import numpy as np
    from sklearn.preprocessing import LabelEncoder
    from datasets import load_dataset

    from imml.classify import RAGPT
    from imml.load import RAGPTDataset, RAGPTCollator
    from imml.retrieve import MCR





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
      warn(




.. GENERATED FROM PYTHON SOURCE LINES 62-68

Step 2: Prepare the dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We use the oxford-iiit-pet-vl-enriched dataset, a public vision–language dataset with images and captions
available on `Hugging Face Datasets
<https://huggingface.co/datasets>`__ as visual-layer/oxford-iiit-pet-vl-enriched. For retrieval, we will use
the ``MCR`` class from the retrieve module.

.. GENERATED FROM PYTHON SOURCE LINES 68-101

.. code-block:: Python


    random_state = 42
    L.seed_everything(random_state)

    # Local working directory (images will be saved here so ``MCR`` can read paths)
    data_folder = "oxford_iiit_pet"
    folder_images = os.path.join(data_folder, "imgs")
    os.makedirs(folder_images, exist_ok=True)

    # Load the dataset
    ds = load_dataset("visual-layer/oxford-iiit-pet-vl-enriched", split="train[:50]")

    # Build a DataFrame with image paths and captions. We persist images to disk because
    # the retriever expects paths.
    n_total = len(ds)
    rows = []
    for i in range(n_total):
        ex = ds[i]
        img = ex.get("image", None)
        caption = ex.get("caption_enriched", None)
        label = ex.get("label_cat_dog", None)
        img_path = os.path.join(folder_images, f"{i:06d}.jpg")
        try:
            img.save(img_path)
        except Exception:
            img.convert("RGB").save(img_path)
        rows.append({"img": img_path, "text": caption, "label": label})

    df = pd.DataFrame(rows)
    le = LabelEncoder()
    df["class"] = le.fit_transform(df["label"])
    df["class"].value_counts()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Seed set to 42

    class
    1    37
    0    13
    Name: count, dtype: int64



.. GENERATED FROM PYTHON SOURCE LINES 102-103

Split into 40% bank memory, 40% train and 20% test sets

.. GENERATED FROM PYTHON SOURCE LINES 103-111

.. code-block:: Python

    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, stratify=df["class"])
    train_df, bank_df = train_test_split(train_df, test_size=0.5, shuffle=True, stratify=train_df["class"])
    print("train_df", train_df.shape)
    print("test_df", test_df.shape)
    print("bank_df", bank_df.shape)
    train_df.head()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    train_df (20, 4)
    test_df (10, 4)
    bank_df (20, 4)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>img</th>
          <th>text</th>
          <th>label</th>
          <th>class</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>27</th>
          <td>oxford_iiit_pet/imgs/000027.jpg</td>
          <td>a large white dog standing on a patio near a b...</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>24</th>
          <td>oxford_iiit_pet/imgs/000024.jpg</td>
          <td>a white cat laying on a floor</td>
          <td>cat</td>
          <td>0</td>
        </tr>
        <tr>
          <th>21</th>
          <td>oxford_iiit_pet/imgs/000021.jpg</td>
          <td>a black and tan dog with a blue collar</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>44</th>
          <td>oxford_iiit_pet/imgs/000044.jpg</td>
          <td>a cat yawning on the floor</td>
          <td>cat</td>
          <td>0</td>
        </tr>
        <tr>
          <th>19</th>
          <td>oxford_iiit_pet/imgs/000019.jpg</td>
          <td>a dog laying on a red pillow</td>
          <td>dog</td>
          <td>1</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 112-116

Step 3: Simulate missing modalities
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To reflect realistic scenarios, we randomly introduce missing data. In this case, 30% of training and test samples
will have either text or image missing. You can change this parameter for more or less amount of incompleteness.

.. GENERATED FROM PYTHON SOURCE LINES 116-135

.. code-block:: Python


    p = 0.3
    missing_mask = train_df.sample(frac=p/2, random_state=random_state).index
    train_df.loc[missing_mask, "img"] = np.nan
    missing_mask = train_df. \
        drop(labels=missing_mask). \
        sample(n=len(missing_mask), random_state=random_state). \
        index
    train_df.loc[missing_mask, "text"] = np.nan

    missing_mask = test_df.sample(frac=p/2, random_state=random_state).index
    test_df.loc[missing_mask, "img"] = np.nan
    missing_mask = test_df. \
        drop(labels=missing_mask). \
        sample(n=len(missing_mask), random_state=random_state). \
        index
    test_df.loc[missing_mask, "text"] = np.nan









.. GENERATED FROM PYTHON SOURCE LINES 136-140

Step 4: Generate the prompts using a retriever
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
``RAGPT`` needs prompts, which are created from a memory bank with a retriever.
We use ``MCR`` (Multi-Channel Retriever) to construct a memory bank and generate prompts.

.. GENERATED FROM PYTHON SOURCE LINES 140-157

.. code-block:: Python


    modalities = ["image", "text"]
    batch_size = 64
    estimator = MCR(batch_size=batch_size, modalities=modalities, save_memory_bank=True,
                    prompt_path=data_folder, n_neighbors=2, generate_cap=True)

    Xs_bank = [
        bank_df["img"].to_list(),
        bank_df["text"].to_list()
    ]
    y_bank = bank_df["class"]

    estimator.fit(Xs=Xs_bank, y=y_bank)
    memory_bank = estimator.memory_bank_
    print("memory_bank", memory_bank.shape)
    memory_bank.head()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
    /home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.
      return bound(*args, **kwds)
    memory_bank (20, 8)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>item_id</th>
          <th>img_path</th>
          <th>text</th>
          <th>q_i</th>
          <th>q_t</th>
          <th>label</th>
          <th>prompt_image_path</th>
          <th>prompt_text_path</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>18</th>
          <td>18</td>
          <td>oxford_iiit_pet/imgs/000018.jpg</td>
          <td>a gray cat laying on the floor</td>
          <td>[-0.288349986076355, 0.6082454919815063, 0.257...</td>
          <td>[-0.5205259919166565, -0.275832861661911, 0.22...</td>
          <td>0</td>
          <td>oxford_iiit_pet/image/000018.npy</td>
          <td>oxford_iiit_pet/text/000018.npy</td>
        </tr>
        <tr>
          <th>7</th>
          <td>7</td>
          <td>oxford_iiit_pet/imgs/000007.jpg</td>
          <td>a man holding a black dog</td>
          <td>[-0.3650640547275543, 0.2776173949241638, -0.4...</td>
          <td>[-0.25834596157073975, 0.549543023109436, 0.35...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000007.npy</td>
          <td>oxford_iiit_pet/text/000007.npy</td>
        </tr>
        <tr>
          <th>20</th>
          <td>20</td>
          <td>oxford_iiit_pet/imgs/000020.jpg</td>
          <td>a cat is sitting on a branch</td>
          <td>[-0.3221859633922577, -0.1820007562637329, 0.2...</td>
          <td>[-0.8176321387290955, 0.08956006169319153, 0.7...</td>
          <td>0</td>
          <td>oxford_iiit_pet/image/000020.npy</td>
          <td>oxford_iiit_pet/text/000020.npy</td>
        </tr>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>oxford_iiit_pet/imgs/000000.jpg</td>
          <td>a cat walking on grass</td>
          <td>[0.04112936556339264, 0.2862536907196045, 0.22...</td>
          <td>[0.3640563488006592, 0.47397851943969727, 0.63...</td>
          <td>0</td>
          <td>oxford_iiit_pet/image/000000.npy</td>
          <td>oxford_iiit_pet/text/000000.npy</td>
        </tr>
        <tr>
          <th>46</th>
          <td>46</td>
          <td>oxford_iiit_pet/imgs/000046.jpg</td>
          <td>a dog laying in the grass</td>
          <td>[0.13979414105415344, 0.36746183037757874, -0....</td>
          <td>[0.5348194241523743, 0.22137261927127838, 0.30...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000046.npy</td>
          <td>oxford_iiit_pet/text/000046.npy</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 158-159

Load generated training and testing prompts.

.. GENERATED FROM PYTHON SOURCE LINES 159-178

.. code-block:: Python


    Xs_train = [
        train_df["img"].to_list(),
        train_df["text"].to_list()
    ]
    y_train = train_df["class"]
    train_db = estimator.transform(Xs=Xs_train, y=y_train)
    print("train_db", train_db.shape)
    train_db.head()

    Xs_test = [
        test_df["img"].to_list(),
        test_df["text"].to_list()
    ]
    y_test = test_df["class"]
    test_db = estimator.transform(Xs=Xs_test, y=y_test)
    print("test_db", test_db.shape)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    train_db (20, 14)
    test_db (10, 14)




.. GENERATED FROM PYTHON SOURCE LINES 179-182

Step 5: Training the model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Create the loaders.

.. GENERATED FROM PYTHON SOURCE LINES 182-190

.. code-block:: Python

    train_data = RAGPTDataset(database=train_db)
    train_dataloader = DataLoader(dataset= train_data, batch_size=batch_size,
                                  collate_fn= RAGPTCollator(), shuffle=True)

    test_data = RAGPTDataset(database=test_db)
    test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,
                                 collate_fn=RAGPTCollator(), shuffle=False)








.. GENERATED FROM PYTHON SOURCE LINES 191-193

Train the ``RAGPT`` model using the generated prompts. For speed in this demo we train for only 2 epochs using
the `Lightning <https://lightning.ai/docs/pytorch/stable/starter/introduction.html>`_ library.

.. GENERATED FROM PYTHON SOURCE LINES 193-197

.. code-block:: Python

    trainer = Trainer(max_epochs=2, logger=False, enable_checkpointing=False)
    estimator = RAGPT(cls_num=len(le.classes_))
    trainer.fit(estimator, train_dataloader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    GPU available: False, used: False
    TPU available: False, using: 0 TPU cores
    HPU available: False, using: 0 HPUs
    /home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.

      | Name  | Type        | Params | Mode 
    ----------------------------------------------
    0 | model | RAGPTModule | 118 M  | train
    ----------------------------------------------
    7.3 M     Trainable params
    111 M     Non-trainable params
    118 M     Total params
    473.226   Total estimated model params size (MB)
    19        Modules in train mode
    232       Modules in eval mode
    /home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
    Training: |          | 0/? [00:00<?, ?it/s]    Training:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] /home/alberto/Work/imml/imml/load/ragpt_dataset.py:229: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
      "input_ids": torch.tensor(input_ids,dtype=torch.int64),
    /home/alberto/Work/imml/imml/classify/ragpt.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
      t_observed_mask = torch.tensor(observed_text).to(pixel_values.device)
    /home/alberto/Work/imml/imml/classify/ragpt.py:269: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
      i_observed_mask = torch.tensor(observed_image).to(pixel_values.device)
    Epoch 0: 100%|██████████| 1/1 [00:26<00:00,  0.04it/s]    Epoch 0: 100%|██████████| 1/1 [00:26<00:00,  0.04it/s]    Epoch 0: 100%|██████████| 1/1 [00:26<00:00,  0.04it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]            Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 1: 100%|██████████| 1/1 [00:23<00:00,  0.04it/s]    Epoch 1: 100%|██████████| 1/1 [00:23<00:00,  0.04it/s]    Epoch 1: 100%|██████████| 1/1 [00:23<00:00,  0.04it/s]`Trainer.fit` stopped: `max_epochs=2` reached.
    Epoch 1: 100%|██████████| 1/1 [00:23<00:00,  0.04it/s]




.. GENERATED FROM PYTHON SOURCE LINES 198-202

Step 6: Advanced Usage: Track Metrics During Training
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
As any other model in `Lightning <https://lightning.ai/docs/pytorch/stable/starter/introduction.html>`_, we can
modify the internal functions. For instance, we can track loss and compute evaluation metrics during training.

.. GENERATED FROM PYTHON SOURCE LINES 202-222

.. code-block:: Python


    trainer = Trainer(max_epochs=2, logger=False, enable_checkpointing=False)
    estimator = RAGPT(cls_num=len(le.classes_))
    estimator.loss_list = []
    estimator.agg_loss_list = []
    validation_step = estimator.validation_step

    def compute_metric(*args):
        loss = validation_step(*args)
        estimator.loss_list.append(loss)
        return loss
    estimator.validation_step = compute_metric

    def agg_metric(*args):
        estimator.agg_loss_list.append(torch.stack(estimator.loss_list).mean())
        estimator.loss_list = []
    estimator.on_validation_epoch_end = agg_metric

    trainer.fit(estimator, train_dataloader, test_dataloader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    GPU available: False, used: False
    TPU available: False, using: 0 TPU cores
    HPU available: False, using: 0 HPUs

      | Name  | Type        | Params | Mode 
    ----------------------------------------------
    0 | model | RAGPTModule | 118 M  | train
    ----------------------------------------------
    7.3 M     Trainable params
    111 M     Non-trainable params
    118 M     Total params
    473.226   Total estimated model params size (MB)
    19        Modules in train mode
    232       Modules in eval mode
    Sanity Checking: |          | 0/? [00:00<?, ?it/s]/home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
    Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]    Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]    Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:06<00:00,  0.15it/s]                                                                               Training: |          | 0/? [00:00<?, ?it/s]    Training:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]     Epoch 0: 100%|██████████| 1/1 [00:24<00:00,  0.04it/s]    Epoch 0: 100%|██████████| 1/1 [00:24<00:00,  0.04it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0: 100%|██████████| 1/1 [00:05<00:00,  0.17it/s]
                                                                              Epoch 0: 100%|██████████| 1/1 [00:30<00:00,  0.03it/s]    Epoch 0: 100%|██████████| 1/1 [00:30<00:00,  0.03it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]            Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 1: 100%|██████████| 1/1 [00:21<00:00,  0.05it/s]    Epoch 1: 100%|██████████| 1/1 [00:21<00:00,  0.05it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0: 100%|██████████| 1/1 [00:06<00:00,  0.16it/s]
                                                                              Epoch 1: 100%|██████████| 1/1 [00:28<00:00,  0.03it/s]    Epoch 1: 100%|██████████| 1/1 [00:28<00:00,  0.03it/s]`Trainer.fit` stopped: `max_epochs=2` reached.
    Epoch 1: 100%|██████████| 1/1 [00:28<00:00,  0.03it/s]




.. GENERATED FROM PYTHON SOURCE LINES 223-226

Step 7: Evaluation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
After training, we can evaluate predictions and visualize the results.

.. GENERATED FROM PYTHON SOURCE LINES 226-258

.. code-block:: Python

    preds = trainer.predict(estimator, test_dataloader)
    preds = [batch.softmax(dim=1) for batch in preds]
    preds = [pred for batch in preds for pred in batch]
    preds = torch.stack(preds).argmax(1).cpu()
    losses = [i.item() for i in estimator.agg_loss_list]

    nrows, ncols = 2,3
    test_df = test_df.reset_index()
    preds = preds[test_df.index]
    fig, axes = plt.subplots(nrows, ncols, constrained_layout=True)
    for i, (i_row, row) in enumerate(test_df.sample(n=nrows*ncols, random_state=random_state).iterrows()):
        pred = preds[i_row]
        image_to_show = row["img"]
        caption = row["text"]
        real_class = row["label"]
        ax = axes[i//ncols, i%ncols]
        ax.axis("off")
        if isinstance(image_to_show, str):
            image_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)
            ax.imshow(image_to_show)
        pred_class = le.classes_[pred]
        c = "green" if pred_class == real_class else "red"
        ax.set_title(f"Pred:{pred_class}; Real:{real_class}", **{"color":c})
        if isinstance(caption, str):
            caption = caption.split(" ")
            if len(caption) >=6:
                caption = caption[:len(caption)//2] + ["\n"] + caption[len(caption)//2:]
                caption = " ".join(caption)
            ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')

    shutil.rmtree(data_folder, ignore_errors=True)




.. image-sg:: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_001.png
   :alt: Pred:cat; Real:cat, Pred:dog; Real:dog, Pred:dog; Real:dog, Pred:cat; Real:cat, Pred:dog; Real:dog, Pred:cat; Real:cat
   :srcset: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/alberto/anaconda3/envs/imc/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting:   0%|          | 0/1 [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]    Predicting DataLoader 0: 100%|██████████| 1/1 [00:05<00:00,  0.17it/s]    Predicting DataLoader 0: 100%|██████████| 1/1 [00:05<00:00,  0.17it/s]




.. GENERATED FROM PYTHON SOURCE LINES 259-263

.. code-block:: Python


    ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=preds)
    print("Testing metric:", matthews_corrcoef(y_true=y_test, y_pred=preds))




.. image-sg:: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_002.png
   :alt: classify incomplete vision language
   :srcset: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Testing metric: 1.0




.. GENERATED FROM PYTHON SOURCE LINES 264-265

Despite using only 50 instances and minimal training, the performance was excellent thanks to the pretrained models.

.. GENERATED FROM PYTHON SOURCE LINES 267-276

Summary of results
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We first built a memory bank with 40% independent vision-language samples using the `iMML` retrieve module to
generate retrieval-augmented prompts with a multi-channel retriever (``MCR``). Subsequently, we trained a model
using the ``RAGPT`` algorithm available in `iMML` under 25% randomly missing text and image modalities. The model
demonstrated strong robustness on the test set.

This example is intentionally simplified, using only 50 instances for demonstration.
For stronger performance and more reliable results, the full dataset and longer training should be used.

.. GENERATED FROM PYTHON SOURCE LINES 278-282

Conclusion
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This example illustrates how `iMML` enables state-of-the-art performance in classification, even in the presence
of significant modality incompleteness in vision-language datasets.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (5 minutes 11.724 seconds)


.. _sphx_glr_download_auto_tutorials_classify_incomplete_vision_language.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: classify_incomplete_vision_language.ipynb <classify_incomplete_vision_language.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: classify_incomplete_vision_language.py <classify_incomplete_vision_language.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: classify_incomplete_vision_language.zip <classify_incomplete_vision_language.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
