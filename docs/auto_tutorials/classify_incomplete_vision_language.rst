
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/classify_incomplete_vision_language.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_classify_incomplete_vision_language.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_classify_incomplete_vision_language.py:


===========================================================================================================
Classify an incomplete vision–language dataset (Oxford‑IIIT Pets) with deep learning
===========================================================================================================

This tutorial demonstrates how to classify samples from an incomplete vision–language dataset using the `iMML`
library. `iMML` supports robust classification even when some modalities (e.g., text or image) are missing, making it
suitable for real‑world multi‑modal data where missingness is common.

We will use the ``RAGPT`` algorithm from the `iMML` classify module on the Oxford‑IIIT Pets dataset and evaluate performance.

What you will learn:

- How to load a public vision–language dataset (Oxford‑IIIT Pets via Hugging Face Datasets).
- How to adapt this workflow to your own vision–language data.
- How to build a retrieval‑augmented memory bank and prompts with ``MCR``.
- How to train the ``RAGPT`` classifier when image or text may be missing.
- How to track metrics during training and evaluate with MCC and a confusion matrix.

.. GENERATED FROM PYTHON SOURCE LINES 20-25

.. code-block:: Python


    # sphinx_gallery_thumbnail_number = 1

    # License: BSD 3-Clause License








.. GENERATED FROM PYTHON SOURCE LINES 26-32

Step 0: Prerequisites
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To run this tutorial, install the extras for deep learning and tutorials:
  pip install imml[deep]
We also use the Hugging Face Datasets library to load Oxford‑IIIT Pets:
  pip install datasets

.. GENERATED FROM PYTHON SOURCE LINES 35-37

Step 1: Import required libraries
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 37-57

.. code-block:: Python


    import shutil
    from PIL import Image
    from lightning import Trainer
    import lightning as L
    from matplotlib import pyplot as plt
    from sklearn.model_selection import train_test_split
    from torch.utils.data import DataLoader
    import torch
    import os
    import pandas as pd
    from sklearn.metrics import matthews_corrcoef, ConfusionMatrixDisplay
    import numpy as np
    from sklearn.preprocessing import LabelEncoder
    from datasets import load_dataset

    from imml.classify import RAGPT
    from imml.load import RAGPTDataset, RAGPTCollator
    from imml.retrieve import MCR








.. GENERATED FROM PYTHON SOURCE LINES 58-63

Step 2: Prepare the dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We use the oxford-iiit-pet-vl-enriched dataset, a public vision–language dataset with images and captions
available on Hugging Face Datasets as visual-layer/oxford-iiit-pet-vl-enriched. For retrieval, we will use
the ``MCR`` class from the retrieve module.

.. GENERATED FROM PYTHON SOURCE LINES 63-96

.. code-block:: Python


    random_state = 42
    L.seed_everything(random_state)

    # Local working directory (images will be saved here so ``MCR`` can read paths)
    data_folder = "oxford_iiit_pet"
    folder_images = os.path.join(data_folder, "imgs")
    os.makedirs(folder_images, exist_ok=True)

    # Load the dataset
    ds = load_dataset("visual-layer/oxford-iiit-pet-vl-enriched", split="train[:100]")

    # Build a DataFrame with image paths and captions. We persist images to disk because
    # the retriever expects paths.
    n_total = len(ds)
    rows = []
    for i in range(n_total):
        ex = ds[i]
        img = ex.get("image", None)
        caption = ex.get("caption_enriched", None)
        label = ex.get("label_cat_dog", None)
        img_path = os.path.join(folder_images, f"{i:06d}.jpg")
        try:
            img.save(img_path)
        except Exception:
            img.convert("RGB").save(img_path)
        rows.append({"img": img_path, "text": caption, "label": label})

    df = pd.DataFrame(rows)
    le = LabelEncoder()
    df["class"] = le.fit_transform(df["label"])
    df["class"].value_counts()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    class
    1    72
    0    28
    Name: count, dtype: int64



.. GENERATED FROM PYTHON SOURCE LINES 97-98

Split into 40% bank memory, 40% train and 20% test sets

.. GENERATED FROM PYTHON SOURCE LINES 98-106

.. code-block:: Python

    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, stratify=df["class"])
    train_df, bank_df = train_test_split(train_df, test_size=0.5, shuffle=True, stratify=train_df["class"])
    print("train_df", train_df.shape)
    print("test_df", test_df.shape)
    print("bank_df", bank_df.shape)
    train_df.head()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    train_df (40, 4)
    test_df (20, 4)
    bank_df (40, 4)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>img</th>
          <th>text</th>
          <th>label</th>
          <th>class</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>48</th>
          <td>oxford_iiit_pet/imgs/000048.jpg</td>
          <td>a black dog with a long hair</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>65</th>
          <td>oxford_iiit_pet/imgs/000065.jpg</td>
          <td>a white dog with a blue collar</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>52</th>
          <td>oxford_iiit_pet/imgs/000052.jpg</td>
          <td>a small dog sitting on a bed</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>6</th>
          <td>oxford_iiit_pet/imgs/000006.jpg</td>
          <td>a dog laying on a brick sidewalk</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>13</th>
          <td>oxford_iiit_pet/imgs/000013.jpg</td>
          <td>a cat sitting on a desk next to a laptop</td>
          <td>cat</td>
          <td>0</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 107-111

Step 3: Simulate missing modalities
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To reflect realistic scenarios, we randomly introduce missing data. In this case, 30% of training and test samples
will have either text or image missing. You can change this parameter for more or less amount of incompleteness.

.. GENERATED FROM PYTHON SOURCE LINES 111-130

.. code-block:: Python


    p = 0.3
    missing_mask = train_df.sample(frac=p/2, random_state=random_state).index
    train_df.loc[missing_mask, "img"] = np.nan
    missing_mask = train_df. \
        drop(labels=missing_mask). \
        sample(n=len(missing_mask), random_state=random_state). \
        index
    train_df.loc[missing_mask, "text"] = np.nan

    missing_mask = test_df.sample(frac=p/2, random_state=random_state).index
    test_df.loc[missing_mask, "img"] = np.nan
    missing_mask = test_df. \
        drop(labels=missing_mask). \
        sample(n=len(missing_mask), random_state=random_state). \
        index
    test_df.loc[missing_mask, "text"] = np.nan









.. GENERATED FROM PYTHON SOURCE LINES 131-134

Step 4: Generate the prompts using a retriever
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We use the ``MCR`` (Multi-Channel Retriever) to construct a memory bank and generate prompts for the ``RAGPT`` model.

.. GENERATED FROM PYTHON SOURCE LINES 134-151

.. code-block:: Python


    modalities = ["image", "text"]
    batch_size = 64
    estimator = MCR(batch_size=batch_size, modalities=modalities, save_memory_bank=True,
                    prompt_path=data_folder, n_neighbors=2, generate_cap=True)

    Xs_bank = [
        bank_df["img"].to_list(),
        bank_df["text"].to_list()
    ]
    y_bank = bank_df["class"]

    estimator.fit(Xs=Xs_bank, y=y_bank)
    memory_bank = estimator.memory_bank_
    print("memory_bank", memory_bank.shape)
    memory_bank.head()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    memory_bank (40, 8)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>item_id</th>
          <th>img_path</th>
          <th>text</th>
          <th>q_i</th>
          <th>q_t</th>
          <th>label</th>
          <th>prompt_image_path</th>
          <th>prompt_text_path</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>26</th>
          <td>26</td>
          <td>oxford_iiit_pet/imgs/000026.jpg</td>
          <td>a fluffy dog standing on top of a green field</td>
          <td>[0.032110460102558136, 0.3297426402568817, -0....</td>
          <td>[-0.015881717205047607, -0.3460213243961334, 0...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000026.npy</td>
          <td>oxford_iiit_pet/text/000026.npy</td>
        </tr>
        <tr>
          <th>62</th>
          <td>62</td>
          <td>oxford_iiit_pet/imgs/000062.jpg</td>
          <td>a small white dog sitting on the floor with toys</td>
          <td>[0.0885244756937027, 0.14477987587451935, -0.2...</td>
          <td>[-0.16208945214748383, -0.44643452763557434, 0...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000062.npy</td>
          <td>oxford_iiit_pet/text/000062.npy</td>
        </tr>
        <tr>
          <th>82</th>
          <td>82</td>
          <td>oxford_iiit_pet/imgs/000082.jpg</td>
          <td>a fluffy orange cat</td>
          <td>[-0.3348943889141083, 1.2845616340637207, 0.23...</td>
          <td>[-0.28176644444465637, 0.6349888443946838, 0.1...</td>
          <td>0</td>
          <td>oxford_iiit_pet/image/000082.npy</td>
          <td>oxford_iiit_pet/text/000082.npy</td>
        </tr>
        <tr>
          <th>63</th>
          <td>63</td>
          <td>oxford_iiit_pet/imgs/000063.jpg</td>
          <td>a gray and white cat sitting on a blue background</td>
          <td>[-0.2473592758178711, 0.42799001932144165, -0....</td>
          <td>[0.4105989336967468, 0.2259322553873062, 0.436...</td>
          <td>0</td>
          <td>oxford_iiit_pet/image/000063.npy</td>
          <td>oxford_iiit_pet/text/000063.npy</td>
        </tr>
        <tr>
          <th>38</th>
          <td>38</td>
          <td>oxford_iiit_pet/imgs/000038.jpg</td>
          <td>a dog looking up at the sky</td>
          <td>[0.39031967520713806, 0.3217568099498749, -0.4...</td>
          <td>[-0.8532043695449829, -0.4827764630317688, 0.7...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000038.npy</td>
          <td>oxford_iiit_pet/text/000038.npy</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 152-153

Load generated training and testing prompts.

.. GENERATED FROM PYTHON SOURCE LINES 153-172

.. code-block:: Python


    Xs_train = [
        train_df["img"].to_list(),
        train_df["text"].to_list()
    ]
    y_train = train_df["class"]
    train_db = estimator.transform(Xs=Xs_train, y=y_train)
    print("train_db", train_db.shape)
    train_db.head()

    Xs_test = [
        test_df["img"].to_list(),
        test_df["text"].to_list()
    ]
    y_test = test_df["class"]
    test_db = estimator.transform(Xs=Xs_test, y=y_test)
    print("test_db", test_db.shape)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    train_db (40, 14)
    test_db (20, 14)




.. GENERATED FROM PYTHON SOURCE LINES 173-176

Step 5: Training the model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Create the loaders.

.. GENERATED FROM PYTHON SOURCE LINES 176-184

.. code-block:: Python

    train_data = RAGPTDataset(database=train_db)
    train_dataloader = DataLoader(dataset= train_data, batch_size=batch_size,
                                  collate_fn= RAGPTCollator(), shuffle=True)

    test_data = RAGPTDataset(database=test_db)
    test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,
                                 collate_fn=RAGPTCollator(), shuffle=False)








.. GENERATED FROM PYTHON SOURCE LINES 185-187

Train the ``RAGPT`` model using the generated prompts. For speed in this demo we train for only 2 epochs using
Lightning.

.. GENERATED FROM PYTHON SOURCE LINES 187-191

.. code-block:: Python

    trainer = Trainer(max_epochs=2, logger=False, enable_checkpointing=False)
    estimator = RAGPT(cls_num=len(le.classes_))
    trainer.fit(estimator, train_dataloader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training: |          | 0/? [00:00<?, ?it/s]    Training:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]     Epoch 0: 100%|██████████| 1/1 [01:01<00:00,  0.02it/s]    Epoch 0: 100%|██████████| 1/1 [01:01<00:00,  0.02it/s]    Epoch 0: 100%|██████████| 1/1 [01:01<00:00,  0.02it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]            Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 1: 100%|██████████| 1/1 [00:58<00:00,  0.02it/s]    Epoch 1: 100%|██████████| 1/1 [00:58<00:00,  0.02it/s]    Epoch 1: 100%|██████████| 1/1 [00:58<00:00,  0.02it/s]    Epoch 1: 100%|██████████| 1/1 [00:58<00:00,  0.02it/s]




.. GENERATED FROM PYTHON SOURCE LINES 192-196

Step 6: Advanced Usage: Track Metrics During Training
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We can modify the internal functions. For instance, we can track loss and compute evaluation metrics during
training.

.. GENERATED FROM PYTHON SOURCE LINES 196-216

.. code-block:: Python


    trainer = Trainer(max_epochs=2, logger=False, enable_checkpointing=False)
    estimator = RAGPT(cls_num=len(le.classes_))
    estimator.loss_list = []
    estimator.agg_loss_list = []
    validation_step = estimator.validation_step

    def compute_metric(*args):
        loss = validation_step(*args)
        estimator.loss_list.append(loss)
        return loss
    estimator.validation_step = compute_metric

    def agg_metric(*args):
        estimator.agg_loss_list.append(torch.stack(estimator.loss_list).mean())
        estimator.loss_list = []
    estimator.on_validation_epoch_end = agg_metric

    trainer.fit(estimator, train_dataloader, test_dataloader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Sanity Checking: |          | 0/? [00:00<?, ?it/s]    Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]    Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]    Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:14<00:00,  0.07it/s]                                                                               Training: |          | 0/? [00:00<?, ?it/s]    Training:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]     Epoch 0: 100%|██████████| 1/1 [01:01<00:00,  0.02it/s]    Epoch 0: 100%|██████████| 1/1 [01:01<00:00,  0.02it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0: 100%|██████████| 1/1 [00:13<00:00,  0.07it/s]
                                                                              Epoch 0: 100%|██████████| 1/1 [01:15<00:00,  0.01it/s]    Epoch 0: 100%|██████████| 1/1 [01:15<00:00,  0.01it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]            Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 1: 100%|██████████| 1/1 [00:59<00:00,  0.02it/s]    Epoch 1: 100%|██████████| 1/1 [00:59<00:00,  0.02it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]
    Validation DataLoader 0: 100%|██████████| 1/1 [00:15<00:00,  0.06it/s]
                                                                              Epoch 1: 100%|██████████| 1/1 [01:15<00:00,  0.01it/s]    Epoch 1: 100%|██████████| 1/1 [01:15<00:00,  0.01it/s]    Epoch 1: 100%|██████████| 1/1 [01:15<00:00,  0.01it/s]




.. GENERATED FROM PYTHON SOURCE LINES 217-220

Step 7: Evaluation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
After training, we can evaluate predictions and visualize the results.

.. GENERATED FROM PYTHON SOURCE LINES 220-252

.. code-block:: Python

    preds = trainer.predict(estimator, test_dataloader)
    preds = [batch.softmax(dim=1) for batch in preds]
    preds = [pred for batch in preds for pred in batch]
    preds = torch.stack(preds).argmax(1).cpu()
    losses = [i.item() for i in estimator.agg_loss_list]

    nrows, ncols = 2,3
    test_df = test_df.reset_index()
    preds = preds[test_df.index]
    fig, axes = plt.subplots(nrows, ncols, constrained_layout=True)
    for i, (i_row, row) in enumerate(test_df.sample(n=nrows*ncols, random_state=random_state).iterrows()):
        pred = preds[i_row]
        image_to_show = row["img"]
        caption = row["text"]
        real_class = row["label"]
        ax = axes[i//ncols, i%ncols]
        ax.axis("off")
        if isinstance(image_to_show, str):
            image_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)
            ax.imshow(image_to_show)
        pred_class = le.classes_[pred]
        c = "green" if pred_class == real_class else "red"
        ax.set_title(f"Pred:{pred_class}; Real:{real_class}", **{"color":c})
        if isinstance(caption, str):
            caption = caption.split(" ")
            if len(caption) >=6:
                caption = caption[:len(caption)//2] + ["\n"] + caption[len(caption)//2:]
                caption = " ".join(caption)
            ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')

    shutil.rmtree(data_folder, ignore_errors=True)




.. image-sg:: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_001.png
   :alt: Pred:dog; Real:dog, Pred:dog; Real:dog, Pred:cat; Real:dog, Pred:dog; Real:dog, Pred:cat; Real:cat, Pred:dog; Real:dog
   :srcset: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting:   0%|          | 0/1 [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]    Predicting DataLoader 0: 100%|██████████| 1/1 [00:15<00:00,  0.06it/s]    Predicting DataLoader 0: 100%|██████████| 1/1 [00:15<00:00,  0.06it/s]




.. GENERATED FROM PYTHON SOURCE LINES 253-257

.. code-block:: Python


    ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=preds)
    print("Testing metric:", matthews_corrcoef(y_true=y_test, y_pred=preds))




.. image-sg:: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_002.png
   :alt: classify incomplete vision language
   :srcset: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Testing metric: 0.8921425711997711




.. GENERATED FROM PYTHON SOURCE LINES 258-267

Summary of results
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We first built a memory bank with 40% independent vision-language samples using the `iMML` ``retrieve`` module to
generate retrieval-augmented prompts with a multi-channel retriever (``MCR``). Subsequently, we trained a model
using the ``RAGPT`` algorithm available in `iMML` under 25% randomly missing text and image modalities. The model
demonstrated strong robustness on the test set.

This example is intentionally simplified, using only 50 instances for demonstration.
For stronger performance and more reliable results, the full dataset and longer training should be used.

.. GENERATED FROM PYTHON SOURCE LINES 269-273

Conclusion
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This example illustrates how `iMML` enables state-of-the-art performance in classification, even in the presence
of significant modality incompleteness in vision-language datasets.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (19 minutes 26.977 seconds)


.. _sphx_glr_download_auto_tutorials_classify_incomplete_vision_language.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: classify_incomplete_vision_language.ipynb <classify_incomplete_vision_language.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: classify_incomplete_vision_language.py <classify_incomplete_vision_language.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: classify_incomplete_vision_language.zip <classify_incomplete_vision_language.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
