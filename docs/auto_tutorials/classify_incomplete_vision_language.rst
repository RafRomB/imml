
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/classify_incomplete_vision_language.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_classify_incomplete_vision_language.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_classify_incomplete_vision_language.py:


===========================================================================================================
Classify an incomplete vision–language dataset (Oxford‑IIIT Pets) with deep learning
===========================================================================================================

This tutorial demonstrates how to classify samples from an incomplete vision–language dataset using the `iMML`
library. `iMML` supports robust classification even when some modalities (e.g., text or image) are missing, making it
suitable for real‑world multi‑modal data where missingness is common.

We will use the ``RAGPT`` algorithm from the `iMML` classify module on the Oxford‑IIIT Pets dataset and evaluate performance.

What you will learn:

- How to load a public vision–language dataset (Oxford‑IIIT Pets via Hugging Face Datasets).
- How to adapt this workflow to your own vision–language data.
- How to build a retrieval‑augmented memory bank and prompts with ``MCR``.
- How to train the ``RAGPT`` classifier when image or text may be missing.
- How to track metrics during training and evaluate with MCC and a confusion matrix.

.. GENERATED FROM PYTHON SOURCE LINES 20-25

.. code-block:: Python


    # sphinx_gallery_thumbnail_number = 1

    # License: BSD 3-Clause License








.. GENERATED FROM PYTHON SOURCE LINES 26-32

Step 0: Prerequisites
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To run this tutorial, install the extras for deep learning and tutorials:
  pip install imml[deep]
We also use the Hugging Face Datasets library to load Oxford‑IIIT Pets:
  pip install datasets

.. GENERATED FROM PYTHON SOURCE LINES 35-37

Step 1: Import required libraries
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 37-57

.. code-block:: Python


    import shutil
    from PIL import Image
    from lightning import Trainer
    import lightning as L
    from matplotlib import pyplot as plt
    from sklearn.model_selection import train_test_split
    from torch.utils.data import DataLoader
    import torch
    import os
    import pandas as pd
    from sklearn.metrics import matthews_corrcoef, ConfusionMatrixDisplay
    import numpy as np
    from sklearn.preprocessing import LabelEncoder
    from datasets import load_dataset

    from imml.classify import RAGPT
    from imml.load import RAGPTDataset, RAGPTCollator
    from imml.retrieve import MCR








.. GENERATED FROM PYTHON SOURCE LINES 58-63

Step 2: Prepare the dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We use the oxford-iiit-pet-vl-enriched dataset, a public vision–language dataset with images and captions
available on Hugging Face Datasets as visual-layer/oxford-iiit-pet-vl-enriched. For retrieval, we will use
the ``MCR`` class from the retrieve module.

.. GENERATED FROM PYTHON SOURCE LINES 63-96

.. code-block:: Python


    random_state = 42
    L.seed_everything(random_state)

    # Local working directory (images will be saved here so ``MCR`` can read paths)
    data_folder = "oxford_iiit_pet"
    folder_images = os.path.join(data_folder, "imgs")
    os.makedirs(folder_images, exist_ok=True)

    # Load the dataset. For this case, we will make sure that enough instances of each class are downloaded.
    ds = load_dataset("visual-layer/oxford-iiit-pet-vl-enriched", split="train[-14:-3]")

    # Build a DataFrame with image paths and captions. We persist images to disk because
    # the retriever expects paths.
    n_total = len(ds)
    rows = []
    for i in range(n_total):
        ex = ds[i]
        img = ex.get("image", None)
        caption = ex.get("caption_enriched", None)
        label = ex.get("label_cat_dog", None)
        img_path = os.path.join(folder_images, f"{i:06d}.jpg")
        try:
            img.save(img_path)
        except Exception:
            img.convert("RGB").save(img_path)
        rows.append({"img": img_path, "text": caption, "label": label})

    df = pd.DataFrame(rows)
    le = LabelEncoder()
    df["class"] = le.fit_transform(df["label"])
    df["class"].value_counts()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    class
    1    7
    0    4
    Name: count, dtype: int64



.. GENERATED FROM PYTHON SOURCE LINES 97-98

Split into 40% bank memory, 40% train and 20% test sets

.. GENERATED FROM PYTHON SOURCE LINES 98-106

.. code-block:: Python

    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, stratify=df["class"])
    train_df, bank_df = train_test_split(train_df, test_size=0.5, shuffle=True, stratify=train_df["class"])
    print("train_df", train_df.shape)
    print("test_df", test_df.shape)
    print("bank_df", bank_df.shape)
    train_df.head()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    train_df (4, 4)
    test_df (3, 4)
    bank_df (4, 4)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>img</th>
          <th>text</th>
          <th>label</th>
          <th>class</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>3</th>
          <td>oxford_iiit_pet/imgs/000003.jpg</td>
          <td>a cat walking in the snow</td>
          <td>cat</td>
          <td>0</td>
        </tr>
        <tr>
          <th>9</th>
          <td>oxford_iiit_pet/imgs/000009.jpg</td>
          <td>a dog standing on a concrete floor</td>
          <td>dog</td>
          <td>1</td>
        </tr>
        <tr>
          <th>0</th>
          <td>oxford_iiit_pet/imgs/000000.jpg</td>
          <td>a black cat is standing on a towel</td>
          <td>cat</td>
          <td>0</td>
        </tr>
        <tr>
          <th>1</th>
          <td>oxford_iiit_pet/imgs/000001.jpg</td>
          <td>a dog standing in the grass</td>
          <td>dog</td>
          <td>1</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 107-111

Step 3: Simulate missing modalities
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To reflect realistic scenarios, we randomly introduce missing data. In this case, 30% of training and test samples
will have either text or image missing. You can change this parameter for more or less amount of incompleteness.

.. GENERATED FROM PYTHON SOURCE LINES 111-130

.. code-block:: Python


    p = 0.3
    missing_mask = train_df.sample(frac=p/2, random_state=random_state).index
    train_df.loc[missing_mask, "img"] = np.nan
    missing_mask = train_df. \
        drop(labels=missing_mask). \
        sample(n=len(missing_mask), random_state=random_state). \
        index
    train_df.loc[missing_mask, "text"] = np.nan

    missing_mask = test_df.sample(frac=p/2, random_state=random_state).index
    test_df.loc[missing_mask, "img"] = np.nan
    missing_mask = test_df. \
        drop(labels=missing_mask). \
        sample(n=len(missing_mask), random_state=random_state). \
        index
    test_df.loc[missing_mask, "text"] = np.nan









.. GENERATED FROM PYTHON SOURCE LINES 131-134

Step 4: Generate the prompts using a retriever
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We use the ``MCR`` (Multi-Channel Retriever) to construct a memory bank and generate prompts for the ``RAGPT`` model.

.. GENERATED FROM PYTHON SOURCE LINES 134-152

.. code-block:: Python


    modalities = ["image", "text"]
    batch_size = 64
    estimator = MCR(batch_size=batch_size, modalities=modalities, save_memory_bank=True,
                    prompt_path=data_folder, n_neighbors=1, generate_cap=True)

    Xs_bank = [
        bank_df["img"].to_list(),
        bank_df["text"].to_list()
    ]
    y_bank = bank_df["class"]

    # As the training takes some time, we will save the results in this tutorial. Uncomment this line for training
    estimator.fit(Xs=Xs_bank, y=y_bank)
    memory_bank = estimator.memory_bank_
    print("memory_bank", memory_bank.shape)
    memory_bank.head()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    memory_bank (4, 8)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>item_id</th>
          <th>img_path</th>
          <th>text</th>
          <th>q_i</th>
          <th>q_t</th>
          <th>label</th>
          <th>prompt_image_path</th>
          <th>prompt_text_path</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>2</th>
          <td>2</td>
          <td>oxford_iiit_pet/imgs/000002.jpg</td>
          <td>a brown and white dog</td>
          <td>[-0.36566221714019775, -0.20091623067855835, -...</td>
          <td>[0.02590126544237137, 0.5221644639968872, 0.51...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000002.npy</td>
          <td>oxford_iiit_pet/text/000002.npy</td>
        </tr>
        <tr>
          <th>8</th>
          <td>8</td>
          <td>oxford_iiit_pet/imgs/000008.jpg</td>
          <td>a dog wearing a green vest</td>
          <td>[0.39330407977104187, 0.6766937375068665, -0.4...</td>
          <td>[0.04553622007369995, -0.043125078082084656, -...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000008.npy</td>
          <td>oxford_iiit_pet/text/000008.npy</td>
        </tr>
        <tr>
          <th>6</th>
          <td>6</td>
          <td>oxford_iiit_pet/imgs/000006.jpg</td>
          <td>a dog with a curly coat sitting on a porch</td>
          <td>[-0.4398946762084961, 0.7886341214179993, -0.3...</td>
          <td>[0.1656709909439087, -0.4372251033782959, 0.16...</td>
          <td>1</td>
          <td>oxford_iiit_pet/image/000006.npy</td>
          <td>oxford_iiit_pet/text/000006.npy</td>
        </tr>
        <tr>
          <th>10</th>
          <td>10</td>
          <td>oxford_iiit_pet/imgs/000010.jpg</td>
          <td>a cat laying on a bed</td>
          <td>[0.37429460883140564, 0.6862649917602539, 0.11...</td>
          <td>[-0.5021401643753052, -0.17627793550491333, 0....</td>
          <td>0</td>
          <td>oxford_iiit_pet/image/000010.npy</td>
          <td>oxford_iiit_pet/text/000010.npy</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 153-154

Load generated training and testing prompts.

.. GENERATED FROM PYTHON SOURCE LINES 154-174

.. code-block:: Python


    Xs_train = [
        train_df["img"].to_list(),
        train_df["text"].to_list()
    ]
    y_train = train_df["class"]

    train_db = estimator.transform(Xs=Xs_train, y=y_train)
    print("train_db", train_db.shape)
    train_db.head()

    Xs_test = [
        test_df["img"].to_list(),
        test_df["text"].to_list()
    ]
    y_test = test_df["class"]
    test_db = estimator.transform(Xs=Xs_test, y=y_test)
    print("test_db", test_db.shape)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    train_db (4, 14)
    test_db (3, 14)




.. GENERATED FROM PYTHON SOURCE LINES 175-178

Step 5: Training the model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Create the loaders.

.. GENERATED FROM PYTHON SOURCE LINES 178-186

.. code-block:: Python

    train_data = RAGPTDataset(database=train_db)
    train_dataloader = DataLoader(dataset= train_data, batch_size=batch_size,
                                  collate_fn= RAGPTCollator(), shuffle=True)

    test_data = RAGPTDataset(database=test_db)
    test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,
                                 collate_fn=RAGPTCollator(), shuffle=False)








.. GENERATED FROM PYTHON SOURCE LINES 187-189

Train the ``RAGPT`` model using the generated prompts. For speed in this demo we train for only 2 epochs using
Lightning.

.. GENERATED FROM PYTHON SOURCE LINES 189-193

.. code-block:: Python

    trainer = Trainer(max_epochs=1, logger=False, enable_checkpointing=False)
    estimator = RAGPT(cls_num=len(le.classes_))
    trainer.fit(estimator, train_dataloader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training: |          | 0/? [00:00<?, ?it/s]    Training:   0%|          | 0/1 [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]     Epoch 0: 100%|██████████| 1/1 [00:09<00:00,  0.11it/s]    Epoch 0: 100%|██████████| 1/1 [00:09<00:00,  0.11it/s]    Epoch 0: 100%|██████████| 1/1 [00:09<00:00,  0.11it/s]    Epoch 0: 100%|██████████| 1/1 [00:09<00:00,  0.11it/s]




.. GENERATED FROM PYTHON SOURCE LINES 194-198

Step 6: Evaluation
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
######################################################
 After training, we can evaluate predictions and visualize the results.

.. GENERATED FROM PYTHON SOURCE LINES 198-228

.. code-block:: Python

    preds = trainer.predict(estimator, test_dataloader)
    preds = [batch.softmax(dim=1) for batch in preds]
    preds = [pred for batch in preds for pred in batch]
    preds = torch.stack(preds).argmax(1).cpu()

    images_to_show = [Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)
                      if isinstance(image_to_show, str) else image_to_show
                      for image_to_show in test_df["img"].to_list()]

    nrows, ncols = 1,3
    fig, axes = plt.subplots(nrows, ncols, constrained_layout=True)
    for i, (ax, image_to_show, caption, pred, real_class) in enumerate(zip(
            axes, images_to_show, test_df["text"].to_list(), preds, test_df["label"].to_list())):
        ax.axis("off")
        try:
            ax.imshow(image_to_show)
        except TypeError:
            pass
        pred_class = le.classes_[pred]
        c = "green" if pred_class == real_class else "red"
        ax.set_title(f"Pred:{pred_class}; Real:{real_class}", **{"color":c})
        if isinstance(caption, str):
            caption = caption.split(" ")
            if len(caption) >=6:
                caption = caption[:len(caption)//2] + ["\n"] + caption[len(caption)//2:]
                caption = " ".join(caption)
            ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')

    shutil.rmtree(data_folder, ignore_errors=True)




.. image-sg:: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_001.png
   :alt: Pred:dog; Real:dog, Pred:dog; Real:dog, Pred:dog; Real:cat
   :srcset: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting:   0%|          | 0/1 [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]    Predicting DataLoader 0: 100%|██████████| 1/1 [00:04<00:00,  0.22it/s]    Predicting DataLoader 0: 100%|██████████| 1/1 [00:04<00:00,  0.22it/s]




.. GENERATED FROM PYTHON SOURCE LINES 229-233

.. code-block:: Python


    ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=preds)
    print("Testing metric:", matthews_corrcoef(y_true=y_test, y_pred=preds))




.. image-sg:: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_002.png
   :alt: classify incomplete vision language
   :srcset: /auto_tutorials/images/sphx_glr_classify_incomplete_vision_language_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Testing metric: 0.0




.. GENERATED FROM PYTHON SOURCE LINES 234-243

Summary of results
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We first built a memory bank with 40% independent vision-language samples using the `iMML` ``retrieve`` module to
generate retrieval-augmented prompts with a multi-channel retriever (``MCR``). Subsequently, we trained a model
using the ``RAGPT`` algorithm available in `iMML` under 25% randomly missing text and image modalities. The model
demonstrated strong robustness on the test set.

This example is intentionally simplified, using only 50 instances for demonstration.
For stronger performance and more reliable results, the full dataset and longer training should be used.

.. GENERATED FROM PYTHON SOURCE LINES 245-249

Conclusion
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This example illustrates how `iMML` enables state-of-the-art performance in classification, even in the presence
of significant modality incompleteness in vision-language datasets.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 42.619 seconds)


.. _sphx_glr_download_auto_tutorials_classify_incomplete_vision_language.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: classify_incomplete_vision_language.ipynb <classify_incomplete_vision_language.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: classify_incomplete_vision_language.py <classify_incomplete_vision_language.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: classify_incomplete_vision_language.zip <classify_incomplete_vision_language.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
