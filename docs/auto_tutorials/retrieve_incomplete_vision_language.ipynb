{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Retrieval on a vision\u2013language dataset (flickr30k)\n\nThis tutorial demonstrates how to retrieve samples from an incomplete vision\u2013language dataset using `iMML`.\nWe will use the ``MCR`` retriever to find similar items across modalities (image/text) even when one modality\nis missing. The example uses the public nlphuji/flickr8k dataset from Hugging Face Datasets, so you don't\nneed to prepare files manually.\n\nWhat you will learn:\n\n- How to load a vision\u2013language dataset.\n- How to build a memory bank with MCR for cross-modal retrieval.\n- How to retrieve relevant items with missing modalities.\n- How to visualize top retrieved examples for qualitative inspection.\n\nThis tutorial is fully reproducible. You can swap the loading section with your own data by constructing two\nparallel lists: image paths and texts for each sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 1\n\n# License: BSD 3-Clause License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Prerequisites\nTo run this tutorial, install the extras for deep learning and tutorials:\n  pip install imml[deep]\nAdditionally, we will use the Hugging Face Datasets library to load Flickr30k:\n  pip install datasets\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import required libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import lightning as L\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport shutil\n\nfrom imml.retrieve import MCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare the dataset\nWe use the Flickr30k dataset, a public vision\u2013language dataset with images and captions available on\nHugging Face Datasets as nlphuji/flickr30k. For retrieval, we will use the MCR method from the retrieve module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_state = 42\nL.seed_everything(random_state)\n\n# Local working directory (images will be saved here so MCR can read paths)\ndata_folder = \"flickr30k\"\nfolder_images = os.path.join(data_folder, \"imgs\")\nos.makedirs(folder_images, exist_ok=True)\n\n# Load the dataset\nds = load_dataset(\"nlphuji/flickr30k\", split=\"test[:5]\")\n\n# Build a DataFrame with image paths and captions. We persist images to disk because\n# the retriever expects paths.\nn_total = len(ds)\nrows = []\nfor i in range(n_total):\n    ex = ds[i]\n    img = ex.get(\"image\", None)\n    caption = ex.get(\"caption\", None)[0]\n    img_path = os.path.join(folder_images, f\"{i:06d}.jpg\")\n    img.save(img_path)\n    rows.append({\"img\": img_path, \"text\": caption})\n\ndf = pd.DataFrame(rows)\n\n# Split into 60% train and 40% test sets\ntrain_df = df.sample(frac=0.6, random_state=random_state)\ntest_df = df.drop(index=train_df.index)\nprint(\"train_df\", train_df.shape)\ntrain_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Simulate missing modalities\nTo reflect realistic scenarios, we randomly introduce missing data. In this case, 70% of test samples\nwill have either text or image missing. You can change this parameter for more or less amount of incompleteness.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "missing_img_id = 0\nmissing_text_id = 1\ntest_df.loc[test_df.index[missing_img_id], \"img\"] = np.nan\ntest_df.loc[test_df.index[missing_text_id], \"text\"] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate the memory bank\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "modalities = [\"image\", \"text\"]\nestimator = MCR(batch_size=64, modalities=modalities, save_memory_bank=True)\n\nXs_train = [\n    train_df[\"img\"].to_list(),\n    train_df[\"text\"].to_list()\n]\n# Use dummy labels for API compatibility (labels are not provided in Flickr30k)\ny_train = pd.Series(np.zeros(len(train_df)), index=train_df.index)\n\nestimator.fit(Xs=Xs_train, y=y_train)\nmemory_bank = estimator.memory_bank_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Retrieve\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xs_test = [\n    test_df[\"img\"].to_list(),\n    test_df[\"text\"].to_list()\n]\n# Use dummy labels for API compatibility\ny_test = pd.Series(np.zeros(len(test_df)), index=test_df.index)\n\npreds = estimator.predict(Xs=Xs_test, n_neighbors=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualize the retrieved instances\nWe can visualize the top-2 retrieved instances for a given target sample.\nHere we focus on qualitative inspection: looking at the images and reading the captions\nof the retrieved items to assess whether they are semantically similar to the target.\n\nLet's begin by visualizing the top-2 retrieved instances for a target sample that is missing its text modality.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nrows, ncols = 1,3\nfig, axes = plt.subplots(nrows, ncols, figsize=(6, 2), constrained_layout=True)\nax = axes[0]\nax.axis(\"off\")\nimage_to_show = Xs_test[0][missing_text_id]\nimage_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)\nax.imshow(image_to_show)\nax.set_title(\"Target instance\")\n\nretrieved_instances = preds[\"image\"][\"id\"][missing_text_id]\nretrieved_instances = memory_bank.loc[retrieved_instances]\nfor i,retrieved_instance in retrieved_instances.reset_index(drop=True).iterrows():\n    ax = axes[i+1%ncols]\n    ax.axis(\"off\")\n    image_to_show = retrieved_instance[\"img_path\"]\n    image_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)\n    try:\n        ax.imshow(image_to_show)\n    except TypeError:\n        pass\n    ax.set_title(f\"Top-{i+1}\")\n    caption = retrieved_instance[\"text\"]\n    caption = caption.split(\" \")\n    if len(caption) >= 6:\n        caption = caption[:len(caption) // 4] + [\"\\n\"] + caption[len(caption) // 4:len(caption) // 4*2] + \\\n                  [\"\\n\"] + caption[len(caption) // 4*2:len(caption) // 4*3] + [\"\\n\"] + caption[len(caption) // 4*3:]\n        caption = \" \".join(caption)\n    ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let\u2019s consider a target instance that is missing its image modality.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nrows, ncols = 1,3\nfig, axes = plt.subplots(nrows, ncols, figsize=(6, 2), constrained_layout=True)\nax = axes[0]\nax.axis(\"off\")\nax.set_title(\"Target instance\")\ncaption = Xs_test[1][missing_img_id]\ncaption = caption.split(\" \")\nif len(caption) >= 6:\n    caption = caption[:len(caption) // 4] + [\"\\n\"] + caption[len(caption) // 4:len(caption) // 4 * 2] + \\\n              [\"\\n\"] + caption[len(caption) // 4 * 2:len(caption) // 4 * 3] + [\"\\n\"] + caption[len(caption) // 4 * 3:]\n    caption = \" \".join(caption)\nax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')\n\nretrieved_instances = preds[\"text\"][\"id\"][missing_img_id]\nretrieved_instances = memory_bank.loc[retrieved_instances]\nfor i,retrieved_instance in retrieved_instances.reset_index(drop=True).iterrows():\n    ax = axes[i+1%ncols]\n    ax.axis(\"off\")\n    image_to_show = retrieved_instance[\"img_path\"]\n    image_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)\n    try:\n        ax.imshow(image_to_show)\n    except TypeError:\n        pass\n    ax.set_title(f\"Top-{i+1}\")\n    caption = retrieved_instance[\"text\"]\n    caption = caption.split(\" \")\n    if len(caption) >= 6:\n        caption = caption[:len(caption) // 4] + [\"\\n\"] + caption[len(caption) // 4:len(caption) // 4*2] + \\\n                  [\"\\n\"] + caption[len(caption) // 4*2:len(caption) // 4*3] + [\"\\n\"] + caption[len(caption) // 4*3:]\n        caption = \" \".join(caption)\n    ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')\n\nshutil.rmtree(data_folder, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of results\nWe used the ``MCR`` retriever from `iMML` to identify the most relevant instances from a\nmemory bank, even when one of the modalities (image or text) was missing.\n\nThis example is intentionally simplified, using only a few instances for demonstration.\nFor stronger performance and more reliable results, the full dataset should be used.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nThis example illustrates how `iMML` enables robust retrieval and similarity search in vision-language datasets,\neven in the presence of missing modalities.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}