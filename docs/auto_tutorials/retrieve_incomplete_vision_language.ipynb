{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Retrieval on a vision\u2013language dataset (flickr30k)\n\nThis tutorial demonstrates how to retrieve samples from an incomplete vision\u2013language dataset using `iMML`.\nWe will use the ``MCR`` retriever to find similar items across modalities (image/text) even when one modality\nis missing. The example uses the public [nlphuji/flickr30k](https://huggingface.co/datasets/nlphuji/flickr30k)_ dataset from [Hugging Face Datasets](https://huggingface.co/datasets)_, so you don't need to prepare files manually.\n\nWhat you will learn:\n\n- How to load a vision\u2013language dataset.\n- How to build a memory bank with ``MCR`` for cross-modal retrieval.\n- How to retrieve relevant items with missing modalities.\n- How to visualize top retrieved examples for qualitative inspection.\n\nThis tutorial is fully reproducible. You can swap the loading section with your own data by constructing two\nparallel lists: image paths and texts for each sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 1\n\n# License: BSD 3-Clause License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Prerequisites\nTo run this tutorial, install the extras for deep learning and tutorials:\n  pip install imml[deep]\nAdditionally, we will use the Hugging Face Datasets library to load Flickr30k:\n  pip install datasets\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import required libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import lightning as L\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport shutil\n\nfrom imml.retrieve import MCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare the dataset\nWe use the Flickr30k dataset, a public vision\u2013language dataset with images and captions available on\n[Hugging Face Datasets](https://huggingface.co/datasets)_ as [nlphuji/flickr30k](https://huggingface.co/datasets/nlphuji/flickr30k)_. For retrieval, we will use the ``MCR`` method from the\nretrieve module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_state = 42\nL.seed_everything(random_state)\n\n# Local working directory (images will be saved here so MCR can read paths)\ndata_folder = \"flickr30k\"\nfolder_images = os.path.join(data_folder, \"imgs\")\nos.makedirs(folder_images, exist_ok=True)\n\n# Load the dataset\nds = load_dataset(\"nlphuji/flickr30k\", split=\"test[:100]\")\n\n# Build a DataFrame with image paths and captions. We persist images to disk because\n# the retriever expects paths.\nn_total = len(ds)\nrows = []\nfor i in range(n_total):\n    ex = ds[i]\n    img = ex.get(\"image\", None)\n    caption = ex.get(\"caption\", None)[0]\n    img_path = os.path.join(folder_images, f\"{i:06d}.jpg\")\n    img.save(img_path)\n    rows.append({\"img\": img_path, \"text\": caption})\n\ndf = pd.DataFrame(rows)\n\n# Split into 80% train and 20% test sets\ntrain_df = df.sample(frac=0.8, random_state=random_state)\ntest_df = df.drop(index=train_df.index)\nprint(\"train_df\", train_df.shape)\ntrain_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Simulate missing modalities\nTo reflect realistic scenarios, we randomly introduce missing data. In this case, 70% of test samples\nwill have either text or image missing. You can change this parameter for more or less amount of incompleteness.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = 0.7\nmissing_mask = test_df.sample(frac=p/2, random_state=random_state).index\ntest_df.loc[missing_mask, \"img\"] = np.nan\nmissing_mask = test_df. \\\n    drop(labels=missing_mask). \\\n    sample(n=len(missing_mask), random_state=random_state). \\\n    index\ntest_df.loc[missing_mask, \"text\"] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate the memory bank\nWe build the retriever with ``MCR``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "modalities = [\"image\", \"text\"]\nestimator = MCR(batch_size=64, modalities=modalities, save_memory_bank=True)\n\nXs_train = [\n    train_df[\"img\"].to_list(),\n    train_df[\"text\"].to_list()\n]\n# Use dummy labels for API compatibility (labels are not provided in Flickr30k)\ny_train = pd.Series(np.zeros(len(train_df)), index=train_df.index)\n\nestimator.fit(Xs=Xs_train, y=y_train)\nmemory_bank = estimator.memory_bank_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Retrieve\nWe retrieved the most similar items for the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xs_test = [\n    test_df[\"img\"].to_list(),\n    test_df[\"text\"].to_list()\n]\n\npreds = estimator.predict(Xs=Xs_test, n_neighbors=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the content of the prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds[\"image\"].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds[\"text\"].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualize the retrieved instances\nWe can visualize the top-2 retrieved instances for a given target sample.\nHere we focus on qualitative inspection: looking at the images and reading the captions\nof the retrieved items to assess whether they are semantically similar to the target.\n\nThe target instance is displayed in the leftmost column, followed by the most similar instances in descending\norder of similarity. Note that some instances have missing modalities, which will not appear in the plot. In this\nexample, the first two instances are missing the image modality, while the last one is missing the text modality.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nrows, ncols = 3,3\nfig, axes = plt.subplots(nrows, ncols, constrained_layout=True)\nfor i in range(nrows*ncols):\n    row, col = i//ncols, i%ncols\n    ax = axes[i//ncols, col]\n    ax.axis(\"off\")\n    if col == 0:\n        image_to_show = Xs_test[0][row]\n        caption = Xs_test[1][row]\n        ax.set_title(\"Target instance\")\n    else:\n        col -= 1\n        try:\n            retrieved_instance = preds[\"image\"][\"id\"][row][col]\n        except IndexError:\n            retrieved_instance = preds[\"text\"][\"id\"][row][col]\n        retrieved_instance = memory_bank.loc[retrieved_instance]\n        image_to_show = retrieved_instance[\"img_path\"]\n        caption = retrieved_instance[\"text\"]\n        ax.set_title(f\"Top-{col+1}\")\n\n    if isinstance(image_to_show, str):\n        image_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)\n        ax.imshow(image_to_show)\n    if isinstance(caption, str):\n        caption = caption.split(\" \")\n        if len(caption) >= 6:\n            caption = caption[:len(caption) // 4] + [\"\\n\"] + caption[len(caption) // 4:len(caption) // 4 * 2] + \\\n                      [\"\\n\"] + caption[len(caption) // 4 * 2:len(caption) // 4 * 3] + [\"\\n\"] + caption[\n                          len(caption) // 4 * 3:]\n            caption = \" \".join(caption)\n        ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')\n\nshutil.rmtree(data_folder, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of results\nWe used the ``MCR`` retriever from `iMML` to identify the most relevant instances from a\nmemory bank, even when one of the modalities (image or text) was missing.\n\nThis example is intentionally simplified, using only a few instances for demonstration.\nFor stronger performance and more reliable results, the full dataset should be used.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nThis example illustrates how `iMML` enables robust retrieval and similarity search in vision-language datasets,\neven in the presence of missing modalities.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}