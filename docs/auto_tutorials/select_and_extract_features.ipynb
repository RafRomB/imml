{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Dimensionality reduction: Feature extraction and feature selection\n\nHigh-dimensional datasets can severely impact machine learning projects, by increasing computational demands,\ndata-adquisition costs and reducing model interpretability. It can also degrade performance due to the curse\nof dimensionality, as well as the presence of correlated, noisy, or irrelevant features. Consequently, reducing the\nnumber of features is often critical. Dimensionality reduction addresses these challenges by enhancing\ncomputational efficiency, highlighting key features, reducing noise, and enabling better data visualization.\n\nDimensionality reduction refers to two main approaches: feature selection and feature extraction.\n- Feature selection identifies the most relevant features from the dataset.\n- Feature extraction creates new features by transforming the original ones to capture essential information.\n\nIn this tutorial, you will learn how to use ``JNMF`` for both feature selection and feature extraction. We will also\ncover how to work with missing data, infer modality importance, and visualize the contributions of the top features.\n\nWhat you will learn:\n\n- How to represent your multi-modal dataset as Xs (a list of data matrices).\n- How to apply ``JNMF`` for multi-modal feature extraction.\n- How to apply ``JNMFFeatureSelector`` for multi-modal feature selection.\n- How to handle missing values.\n- How to assess modality importance and inspect the selected top features.\n- How to benchmark different dimensionality-reduction strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Prerequisites\nTo run this tutorial, install the extra dependencies:\n  pip install imml[r]\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import required libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.patches as mpatches\n\nfrom imml.decomposition import JNMF\nfrom imml.preprocessing import MultiModTransformer, ConcatenateMods\nfrom imml.ampute import Amputer\nfrom imml.feature_selection import JNMFFeatureSelector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define plotting functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_modality_importance(Xs, selected_features, weights, names):\n    selected_features = {\"Feature\": selected_features, \"Feature Importance\": weights}\n    selected_features = pd.DataFrame(selected_features)\n    selected_features = selected_features.sort_values(by=\"Feature Importance\",\n                                                      ascending=False)\n    selected_features[\"Modality\"] = selected_features[\"Feature\"].apply(\n        lambda x: [name for X,name in zip(Xs, names) if x in X.columns][0])\n    selected_features = selected_features.groupby(\"Modality\")[\"Feature Importance\"].sum()\n    selected_features = selected_features.div(selected_features.sum()).mul(100)\n    selected_features = selected_features.sort_values(ascending=False)\n    return selected_features\n\ndef get_top_features(Xs, selected_features, weights, components, names):\n    selected_features = {\"Feature\": selected_features, \"Feature Importance\": weights,\n                         \"Component\": components}\n    selected_features = pd.DataFrame(selected_features)\n    selected_features = selected_features.sort_values(by=\"Feature Importance\",\n                                                      ascending=False)\n    selected_features[\"Modality\"] = selected_features[\"Feature\"].apply(\n        lambda x: [name for X,name in zip(Xs, names) if x in X.columns][0])\n    selected_features[\"Component\"] += 1\n    return selected_features\n\ndef get_contributions(Xs, selected_features, weights, components, names):\n    selected_features = {\"Feature\": selected_features, \"Feature Importance\": weights,\n                         \"Component\": components}\n    selected_features = pd.DataFrame(selected_features)\n    selected_features = selected_features.sort_values(by=[\"Component\", \"Feature Importance\"],\n                                                      ascending=[True, False])\n    selected_features[\"Modality\"] = selected_features[\"Feature\"].apply(\n        lambda x: [name for X,name in zip(Xs, names) if x in X.columns][0])\n    selected_features[\"Component\"] += 1\n    return selected_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load the dataset\nFor reproducibility, we generate a small synthetic classification dataset and split the features into two\nmodalities (Xs[0], Xs[1]).\nOptional: set a random_state for reproducibility (we do below).\n\nUsing your own data:\n\n- Represent your dataset as a Python list Xs, one entry per modality.\n- Each Xs[i] should be a 2D array-like (pandas DataFrame or NumPy array) of shape (n_samples, n_features_i).\n- All modalities must refer to the same samples and be aligned by row order or index.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_state = 42\nX, y = make_classification(n_samples=50, random_state=random_state, n_clusters_per_class=1, n_classes=3)\nX, y = pd.DataFrame(X), pd.Series(y)\nX.columns = X.columns.astype(str)\n# Two modalities: first 10 features and last 10 features\nXs = [X.iloc[:, :10], X.iloc[:, 10:]]\nnames= [\"Modality A\", \"Modality B\"]\nprint(\"Samples:\", len(Xs[0]), \"\\t\", \"Modalities:\", len(Xs), \"\\t\", \"Features:\", [X.shape[1] for X in Xs])\nn_clusters = len(np.unique(y))\ny.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply feature selection and feature extraction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_components = 4\n# Feature extraction\npipeline = make_pipeline(MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                         JNMF(n_components=n_components, random_state=random_state))\npipeline.fit(Xs)\n\n# Feature selection\npipeline = make_pipeline(MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                         JNMFFeatureSelector(n_components=n_components, random_state=random_state))\npipeline.fit(Xs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can identify and visualize the selected features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "selected_features = get_top_features(Xs=Xs, selected_features= pipeline[-1].selected_features_,\n                                     weights= pipeline[-1].weights_, components= pipeline[-1].component_,\n                                     names=names)\nselected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "palette = {mod:col for mod, col in zip(names, [\"#2ca25f\", \"#99d8c9\"])}\npalette_list = [palette[mod] for mod in selected_features[\"Modality\"]]\nselected_features = selected_features.assign(color= palette_list).sort_values(\"Feature Importance\")\nax = selected_features.plot(\n    kind=\"barh\", x=\"Feature\", y=\"Feature Importance\", legend=False,\n    color=selected_features[\"color\"], xlabel=\"Feature Importance\",\n    xlim=(0,selected_features[\"Feature Importance\"].max() + .8)\n)\nax = ax.legend(handles=[mpatches.Patch(color=color, label=modality) for modality, color in palette.items()],\n               loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The top features include attributes from both modalities, but Modality A appears to be more important overall.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the modality relative importance with a barplot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "selected_features = get_modality_importance(\n    Xs=Xs, selected_features= pipeline[-1].selected_features_,\n    weights= pipeline[-1].weights_, names=names)\nselected_features.to_frame()\n\nax = selected_features.plot(kind= \"bar\", color= list(palette.values()), ylabel= \"Modality Importance (\\%)\", rot=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yes, in fact Modality A is the most important modality in this example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also extract features and visualize the original features with the largest contribution to the components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                         JNMFFeatureSelector(n_components = n_components, select_by=\"component\",\n                                             random_state=42, f_per_component=2))\npipeline.fit(Xs)\nselected_features = get_contributions(Xs=Xs, selected_features= pipeline[-1].selected_features_,\n                                     weights= pipeline[-1].weights_, components= pipeline[-1].component_,\n                                     names= names)\nselected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "palette_list = [palette[mod] for mod in selected_features[\"Modality\"]]\nselected_features = selected_features.assign(color= palette_list).sort_values(\"Feature Importance\")\nax = selected_features.plot(\n    kind=\"barh\", x=\"Component\", y=\"Feature Importance\", legend=False,\n    color=selected_features[\"color\"], xlabel=\"Feature Importance\", width=0.9,\n    xlim=(0,selected_features[\"Feature Importance\"].max() + .8)\n)\nax.legend(handles=[mpatches.Patch(color=color, label=modality) for modality, color in palette.items()],\n          loc=\"lower right\")\nax.bar_label(ax.containers[0], labels=selected_features[\"Feature\"], padding = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Analyzing an incomplete multi-modal dataset\nWe simulated block- and feature-wise missing data. To provide comparative benchmarks, we included baselines using\nrandomly selected features and all available features. The outputs from these methods were then used as inputs for a\nsupport vector machine to predict the ground-truth labels. As the feature selection process does not replace missing\nvalues, an imputation step was applied prior the classification. We repeat the analysis 5 times with different\nseeds to have robust results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ps = np.arange(0., 1., 0.2)\nn_times = 5\nmethods = [\"No prior imputation\", \"Baseline imputation\"]\nalgorithms = [\"Feature extraction\", \"Feature selection\", \"Randomly selected features\", \"All features\"]\nall_metrics = []\n\nfor algorithm in algorithms:\n    for p in ps:\n        for i in range(n_times):\n            ampute = True\n            while ampute: # avoid those iterations where a sample has no available data\n                Xs_train = Amputer(p=p, random_state=i).fit_transform(Xs)\n                for X in Xs_train:\n                    mask = np.random.default_rng(i).choice([True, False], p= [p,1-p], size = X.shape)\n                    X.iloc[mask] = np.nan\n                if pd.concat(Xs_train, axis=1).isna().all(axis=1).any():\n                    i += n_times\n                else:\n                    ampute = False\n            if algorithm == \"Feature extraction\":\n                pipeline = make_pipeline(\n                    MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                    JNMF(n_components = n_components, random_state=i),\n                )\n            elif algorithm == \"Feature selection\":\n                pipeline = make_pipeline(\n                    MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                    JNMFFeatureSelector(n_components = n_components, random_state=i),\n                    ConcatenateMods(),\n                    SimpleImputer(),\n                )\n            elif algorithm == \"Randomly selected features\":\n                pipeline = make_pipeline(\n                    MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                    ConcatenateMods(),\n                    SimpleImputer().set_output(transform=\"pandas\"),\n                    FunctionTransformer(lambda x:\n                                        x.iloc[:,np.random.default_rng(i).integers(\n                                            0, sum([X.shape[1] for X in Xs_train]), size= n_components)]),\n                 )\n            elif algorithm == \"All features\":\n                pipeline = make_pipeline(\n                    MultiModTransformer(MinMaxScaler().set_output(transform=\"pandas\")),\n                    ConcatenateMods(),\n                    SimpleImputer().set_output(transform=\"pandas\"),\n                 )\n            transformed_X = pipeline.fit_transform(Xs_train)\n            preds = SVC(random_state=i).fit(transformed_X, y).predict(transformed_X)\n            metric = accuracy_score(y_pred=preds, y_true=y)\n            result = {\n                \"Method\": algorithm,\n                'Missing rate (%)': int(p*100),\n                \"Iteration\": i,\n                \"Accuracy\": metric,\n            }\n            all_metrics.append(result)\n\ndf = pd.DataFrame(all_metrics)\ndf['Method'] = pd.Categorical(\n    df['Method'],\n    categories=[\"Feature extraction\", \"Feature selection\", \"All features\", \"Randomly selected features\"],\n    ordered=True\n)\ndf = df.sort_values([\"Method\", \"Missing rate (%)\", \"Iteration\"], ascending=[True, True, True])\nprint(df.shape)\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now visualize the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = df.groupby([\"Method\", \"Missing rate (%)\"])[\"Accuracy\"]\nstats = g.agg(mean=\"mean\", sem=lambda x: x.std(ddof=1) / np.sqrt(len(x))).reset_index()\nmean_wide = stats.pivot(index=\"Missing rate (%)\", columns=\"Method\", values=\"mean\")\nsem_wide  = stats.pivot(index=\"Missing rate (%)\", columns=\"Method\", values=\"sem\")\nax = mean_wide.plot(yerr=sem_wide, marker=\"o\", capsize=3, ylabel=\"Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of results\nAccuracy degrades as the missing\u2011data rate increases, as a natural consequence of losing information. Using all\nfeatures achieved always the best performance, a result that was expected. Both dimensionality\u2011reduction strategies\n(extraction and selection) perform well when the amount of missing data is not high. Feature extraction with\ntends to be more robust than feature selection as missingness increases, often yielding the highest accuracy\namong the reduced representations at moderate-to-high missing rates.\n\nWhy feature extraction can be more resilient here:\n\n- ``JNMF`` learns low\u2011rank, shared latent components across modalities, which can attenuate noise introduced by\n  missing values.\n- The selection pipeline requires imputation after selecting features; simple imputers can inject bias,\n  slightly hurting downstream classification in settings with substantial missingness.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nIn this tutorial we showed how to build compact, informative representations from multi\u2011modal data and how missingness\naffects downstream performance.\n\nOverall, `iMML` provides flexible pipelines to extract or select features across modalities and to benchmark\nrobustness under missing data, helping you choose the right trade\u2011off between accuracy, efficiency, and interpretability\nfor your application.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}