{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classify an incomplete vision\u2013language dataset (Oxford\u2011IIIT Pets) with deep learning\n\nThis tutorial demonstrates how to classify samples from an incomplete vision\u2013language dataset using the `iMML`\nlibrary. `iMML` supports robust classification even when some modalities (e.g., text or image) are missing, making it\nsuitable for real\u2011world multi\u2011modal data where missingness is common.\n\nWe will use the ``RAGPT`` algorithm from the `iMML` classify module on the [Oxford\u2011IIIT Pets](https://huggingface.co/datasets/visual-layer/oxford-iiit-pet-vl-enriched?library=datasets)_  dataset\nand evaluate its performance.\n\nWhat you will learn:\n\n- How to load a public vision\u2013language dataset\n  ([Oxford\u2011IIIT Pets](https://huggingface.co/datasets/visual-layer/oxford-iiit-pet-vl-enriched?library=datasets)\n  via [Hugging Face Datasets](https://huggingface.co/datasets)_).\n- How to adapt this workflow to your own vision\u2013language data.\n- How to build a retrieval\u2011augmented memory bank and prompts with ``MCR``.\n- How to train the ``RAGPT`` classifier when image or text may be missing.\n- How to track metrics during training and evaluate with MCC and a confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 1\n\n# License: BSD 3-Clause License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Prerequisites\nTo run this tutorial, install the extras for deep learning and tutorials:\n  pip install imml[deep]\nWe also use the Hugging Face Datasets library to load Oxford\u2011IIIT Pets:\n  pip install datasets\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import required libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\nfrom PIL import Image\nfrom lightning import Trainer\nimport lightning as L\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport torch\nimport os\nimport pandas as pd\nfrom sklearn.metrics import matthews_corrcoef, ConfusionMatrixDisplay\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import load_dataset\n\nfrom imml.ampute import Amputer\nfrom imml.classify import RAGPT\nfrom imml.load import RAGPTDataset, RAGPTCollator\nfrom imml.retrieve import MCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare the dataset\nWe use the oxford-iiit-pet-vl-enriched dataset, a public vision\u2013language dataset with images and captions\navailable on [Hugging Face Datasets](https://huggingface.co/datasets)_ as visual-layer/oxford-iiit-pet-vl-enriched. For retrieval, we will use\nthe ``MCR`` class from the retrieve module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "random_state = 42\nL.seed_everything(random_state)\n\n# Local working directory (images will be saved here so MCR can read paths)\ndata_folder = \"oxford_iiit_pet\"\nfolder_images = os.path.join(data_folder, \"imgs\")\nos.makedirs(folder_images, exist_ok=True)\n\n# Load the dataset\nds = load_dataset(\"visual-layer/oxford-iiit-pet-vl-enriched\", split=\"train[:50]\")\n\n# Build a DataFrame with image paths and captions. We persist images to disk because\n# the retriever expects paths.\nn_total = len(ds)\nrows = []\nfor i in range(n_total):\n    ex = ds[i]\n    img = ex.get(\"image\", None)\n    caption = ex.get(\"caption_enriched\", None)\n    label = ex.get(\"label_cat_dog\", None)\n    img_path = os.path.join(folder_images, f\"{i:06d}.jpg\")\n    try:\n        img.save(img_path)\n    except Exception:\n        img.convert(\"RGB\").save(img_path)\n    rows.append({\"img\": img_path, \"text\": caption, \"label\": label})\n\ndf = pd.DataFrame(rows)\nle = LabelEncoder()\ndf[\"class\"] = le.fit_transform(df[\"label\"])\ndf[\"class\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split into 40% bank memory, 40% train and 20% test sets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, stratify=df[\"class\"])\ntrain_df, bank_df = train_test_split(train_df, test_size=0.5, shuffle=True, stratify=train_df[\"class\"])\nprint(\"train_df\", train_df.shape)\nprint(\"test_df\", test_df.shape)\nprint(\"bank_df\", bank_df.shape)\ntrain_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Simulate missing modalities\nTo reflect realistic scenarios, we randomly introduce missing data using ``Amputer``. In this case, 30% of training\nand test samples will have either text or image missing. You can change this parameter for more or less\namount of incompleteness.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xs_train = [train_df[[\"img\"]], train_df[[\"text\"]]]\nXs_test = [test_df[[\"img\"]], test_df[[\"text\"]]]\namputer = Amputer(p=0.3, random_state=random_state)\nXs_train = amputer.fit_transform(Xs_train)\nXs_test = amputer.fit_transform(Xs_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate the prompts using a retriever\n``RAGPT`` needs prompts, which are created from a memory bank with a retriever.\nWe use ``MCR`` (Multi-Channel Retriever) to construct a memory bank and generate prompts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "modalities = [\"image\", \"text\"]\nbatch_size = 64\nestimator = MCR(batch_size=batch_size, modalities=modalities, save_memory_bank=True,\n                prompt_path=data_folder, n_neighbors=2, generate_cap=True)\n\nXs_bank = [bank_df[[\"img\"]], bank_df[[\"text\"]]]\ny_bank = bank_df[\"class\"]\n\nestimator.fit(Xs=Xs_bank, y=y_bank)\nmemory_bank = estimator.memory_bank_\nprint(\"memory_bank\", memory_bank.shape)\nmemory_bank.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load generated training and testing prompts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_train = train_df[\"class\"]\ntrain_db = estimator.transform(Xs=Xs_train, y=y_train)\nprint(\"train_db\", train_db.shape)\ntrain_db.head()\n\ny_test = test_df[\"class\"]\ntest_db = estimator.transform(Xs=Xs_test, y=y_test)\nprint(\"test_db\", test_db.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training the model\nCreate the loaders.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data = RAGPTDataset(database=train_db)\ntrain_dataloader = DataLoader(dataset= train_data, batch_size=batch_size,\n                              collate_fn= RAGPTCollator(), shuffle=True)\n\ntest_data = RAGPTDataset(database=test_db)\ntest_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,\n                             collate_fn=RAGPTCollator(), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the ``RAGPT`` model using the generated prompts. For speed in this demo we train for only 2 epochs using\nthe [Lightning](https://lightning.ai/docs/pytorch/stable/starter/introduction.html) library.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(max_epochs=2, logger=False, enable_checkpointing=False)\nestimator = RAGPT(cls_num=len(le.classes_))\ntrainer.fit(estimator, train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Advanced Usage: Track Metrics During Training\nAs any other model in [Lightning](https://lightning.ai/docs/pytorch/stable/starter/introduction.html), we can\nmodify the internal functions. For instance, we can track loss and compute evaluation metrics during training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(max_epochs=2, logger=False, enable_checkpointing=False)\nestimator = RAGPT(cls_num=len(le.classes_))\nestimator.loss_list = []\nestimator.agg_loss_list = []\nvalidation_step = estimator.validation_step\n\ndef compute_metric(*args):\n    loss = validation_step(*args)\n    estimator.loss_list.append(loss)\n    return loss\nestimator.validation_step = compute_metric\n\ndef agg_metric(*args):\n    estimator.agg_loss_list.append(torch.stack(estimator.loss_list).mean())\n    estimator.loss_list = []\nestimator.on_validation_epoch_end = agg_metric\n\ntrainer.fit(estimator, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluation\nAfter training, we can evaluate predictions and visualize the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds = trainer.predict(estimator, test_dataloader)\npreds = [batch.softmax(dim=1) for batch in preds]\npreds = [pred for batch in preds for pred in batch]\npreds = torch.stack(preds).argmax(1).cpu()\nlosses = [i.item() for i in estimator.agg_loss_list]\n\nnrows, ncols = 2,3\ntest_df = test_df.reset_index()\npreds = preds[test_df.index]\nfig, axes = plt.subplots(nrows, ncols, constrained_layout=True)\nfor i, (i_row, row) in enumerate(test_df.sample(n=nrows*ncols, random_state=random_state).iterrows()):\n    pred = preds[i_row]\n    image_to_show = row[\"img\"]\n    caption = row[\"text\"]\n    real_class = row[\"label\"]\n    ax = axes[i//ncols, i%ncols]\n    ax.axis(\"off\")\n    if isinstance(image_to_show, str):\n        image_to_show = Image.open(image_to_show).resize((512, 512), Image.Resampling.LANCZOS)\n        ax.imshow(image_to_show)\n    pred_class = le.classes_[pred]\n    c = \"green\" if pred_class == real_class else \"red\"\n    ax.set_title(f\"Pred:{pred_class}; Real:{real_class}\", **{\"color\":c})\n    if isinstance(caption, str):\n        caption = caption.split(\" \")\n        if len(caption) >=6:\n            caption = caption[:len(caption)//2] + [\"\\n\"] + caption[len(caption)//2:]\n            caption = \" \".join(caption)\n        ax.annotate(caption, xy=(0.5, -0.08), xycoords='axes fraction', ha='center', va='top')\n\nshutil.rmtree(data_folder, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=preds)\nprint(\"Testing metric:\", matthews_corrcoef(y_true=y_test, y_pred=preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Despite using only 50 instances and minimal training, the performance was excellent thanks to the pretrained models.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of results\nWe first built a memory bank with 40% independent vision-language samples using the `iMML` retrieve module to\ngenerate retrieval-augmented prompts with a multi-channel retriever (``MCR``). Subsequently, we trained a model\nusing the ``RAGPT`` algorithm available in `iMML` under 25% randomly missing text and image modalities. The model\ndemonstrated strong robustness on the test set.\n\nThis example is intentionally simplified, using only 50 instances for demonstration.\nFor stronger performance and more reliable results, the full dataset and longer training should be used.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nThis example illustrates how `iMML` enables state-of-the-art performance in classification, even in the presence\nof significant modality incompleteness in vision-language datasets.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}